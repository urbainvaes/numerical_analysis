\documentclass[11pt]{article}
\usepackage{setspace}
\onehalfspacing
\usepackage{fontawesome}
\usepackage[newfloat]{minted}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage[margin=1.2in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{mathtools}
\usepackage[colorlinks=true]{hyperref}
\theoremstyle{definition}
\newtheorem{question}{{\normalfont \faGears}~Question}
\newtheorem{compexercise}{{\normalfont \faLaptop}~Computer exercise}
\input{../macros.tex}
\renewcommand{\mymarks}[1]{\textbf{#1 marks}}
\begin{document}

\title{Numerical Analysis: Midterm {\small (\mymarks{60})}}
\author{Urbain Vaes}
\maketitle

\begin{question}
    [Floating point arithmetic, \mymarks{12}]
    True or false? (+1/0/-1)
    \begin{enumerate}
        \item Let $(\placeholder)_2$ denote binary representation.
            It holds that
            \(
                (0.1111)_2 + (0.0001)_2 = 1.
            \)
        \item It holds that
            \(
                (1000)_{16} \times (0.001)_{16} = 1.
            \)
        \item It holds that
            \[
                (0.\overline{1})_3 = \frac{1}{2}.
            \]

        \item
            In base 16, all the natural numbers from 1 to 300 can be represented using 2 digits.

        \item
            If $x$ is finite \julia{Float64} number,
            then $x$ is a rational number.

        \item
            In Julia, \julia{eps(2.0)} returns the machine epsilon for the Float64 format.

        \item
            Storing the matrix obtained with the Julia command \julia{zeros(10^5, 10^5)} would require more than 50GB of memory.

        \item
            The spacing (in absolute value) between successive double-precision (\julia{Float64}) floating point numbers is constant.

        \item
            It holds that $(0.\overline{10101})_2 = (1.2345)_{10}$.

        \item Machine addition~$\madd$ is an associative operation.
            More precisely, given any three double-precision floating point numbers $x$, $y$ and $z$,
            the following equality holds:
            \[
                (x \madd y) \madd z = x \madd (y \madd z).
            \]
        \item
            The machine epsilon is the smallest strictly positive number that can be represented in a floating point format.

        \item
            In Julia, the command \julia{nextfloat(2.)} returns
            the next \julia{Float32} number after $2$.
    \end{enumerate}
\end{question}

\newpage
\begin{question}
    [Interpolation and approximation, \mymarks{10}]
    Throughout this exercise, we use the notation
    $x^n_i = i/n$ and assume that $u \colon \real \to \real$ is a continuous function.
    The notation~$\poly(n)$ denotes the set of polynomials of degree less than or equal to $n$.
    We proved in class that,
    for all $n \geq 0$,
    there exists a unique polynomial~$p_n \in \poly(n)$ such that
    \begin{equation}
        \label{eq:interpolation}
        \forall i \in \{0, \dotsc, n\}, \qquad
        p_n(x^n_i) = u (x^n_i).
    \end{equation}
    \begin{enumerate}
        \item
            The degree of $p_n$ is exactly~$n$ or $n - 1$.

        \item
            Suppose that $u \in \poly(m)$.
            Then~$p_n = u$ if~$n \geq m$.

        \item
            Fix $u(x) = \sin(3\pi x)$. Then $p_3(x) = 0$.

        \item
            Fix $u(x) = \cos(\pi x)$.
            Then $p_2(x) = (2x - 1)^2$.

        \item
            For all $u$ that is smooth,
            it holds that
            \[
                \max_{x \in [0, 1]} \bigl\lvert u(x) - p_n(x) \bigr\rvert \xrightarrow[n \to \infty]{} 0.
            \]

        \item
            Fix $u(x) = \cos(2x)$.
            Then
            \[
                \max_{x \in [0, 1]} \bigl\lvert u(x) - p_n(x) \bigr\rvert \xrightarrow[n \to \infty]{} 0.
            \]

        \item
            Fix $u(x) = \sin(x)$.
            Then
            \[
                \max_{x \in \real} \bigl\lvert u(x) - p_n(x) \bigr\rvert \xrightarrow[n \to \infty]{} 0.
            \]

        \item
            Suppose that $p(x) \in \poly(n)$ and let $q(x) = p(x+1) - p(x)$. Then $q \in \poly(n-1)$.

        \item
            Let $(f_0, f_1, f_2, \dotsc) = (1, 1, 2, \dotsc)$ denote the Fibonacci sequence.
            There exists a polynomial~$p$ such that
            \begin{equation*}
                \label{eq:fibonacci_polynomial}
                \forall n \in \nat, \qquad
                f_n = p(n).
            \end{equation*}

        \item
            For any matrix $\mat A \in \real^{20 \times 10}$,
            the linear system
            \[
                \mat A^\t \mat A \vect \alpha = \mat A^\t \vect \alpha
            \]
            admits a unique solution.
    \end{enumerate}
\end{question}

\newpage
\begin{question}
    [Numerical integration, \mymarks{9}]
    True or false? (+1/0/-1)
    \begin{enumerate}
        \item
            The degree of precision of the following rule is equal to 2:
            \[
                \int_{-1}^{1} u(x) \, \d x \approx 2u(0).
            \]

        \item
            The degree of precision of the following rule is equal to 3:
            \[
                \int_{-1}^{1} u(x) \, \d x \approx u \left(-\frac{1}{3}\right) + u \left(\frac{1}{3}\right).
            \]

        \item
            For any natural number $N > 0$,
            there exists a quadrature rule with a degree of precision equal to $2N + 1$ of the form
            \[
                \int_{-1}^{1} u(x) \, \d x \approx
                \sum_{n=0}^{N} w_n u(x_n).
            \]

        \item
            Legendre polynomials are orthogonal for the inner product
            \[
                \ip{f, g} := \int_{-1}^{1} f(x) \, g(x) \,  \d x.
            \]

        \item
            Fix $u(x) = \cos(x)$ and let
            \begin{equation}
                \label{eq:mc}
                I^{MC}_N = \frac{1}{N} \sum_{n=1}^{N} u(X_n), \qquad X_n \stackrel{\rm i.i.d.}{\sim} \mathcal U([0, 1]).
            \end{equation}
            The expectation of~$\widehat I_N$ is independent of~$N$.

        \item
            The variance of $I^{MC}_N$ in~\eqref{eq:mc} tends to 0 in the limit $N \to \infty$.

        \item
            Let $x^N_i = i/N$ and consider the following approximation of $\int_{0}^{1} u(x) \, \d x$:
            \begin{equation}
                \label{eq:trapezium}
                I^T_N = \frac{1}{2N} \Bigl( u\left(x^N_0\right) + 2 u\left(x^N_1\right) + 2 u\left(x^N_2\right) + \dotsc + u\left(x^N_{N-2}\right) + 2 u\left(x^N_{N-1}\right) + u\left(x^N_N\right) \Bigr).
            \end{equation}
            Fix $u(x) = (1 + 25 x^2)^{-1}$.
            Then $I^T_N$ diverges in the limit $N \to \infty$.

        \item
            Fix $u(x) = \cos(x)$ and let $I^T_N$ be as in~\eqref{eq:trapezium}.
            Then there exists $C \in (0, \infty)$ such that
            \[
                \forall N \geq 2, \qquad
                \left\lvert I^T_N - \int_{0}^1 u(x) \, \d x \right\rvert \leq \frac{C}{N}.
            \]

        \item
            Fix $u(x) = 2x - 1$ and let $I^T_N$ be as in~\eqref{eq:trapezium}.
            Then $I^T_N = 0$ for $N \geq 2$.
    \end{enumerate}
\end{question}

\newpage

\begin{compexercise}
    [Floating point arithmetic, \mymarks{10}]
    Read the documentation of the \julia{nextfloat} function.
    Using this function,
    plot on the same graph the spacing between successive \julia{Float16}, \julia{Float32} and \julia{Float64} numbers in the range $[1, 10^4]$.
    Use a logarithmic scale for the $x$ and $y$ axes.
    You may find it useful to use \julia{LinRange{Type}(a, b, n)}
    to create a vector of $n$ equidistant numbers of type \julia{Type} between $a$ and $b$.
\end{compexercise}

\begin{compexercise}
    [Interpolation, \mymarks{10}]
    Consider the data
    \[
        x =
        \begin{pmatrix}
            1 \\
            2 \\
            3 \\
            4 \\
        \end{pmatrix},
        \qquad
        y =
        \begin{pmatrix}
            12 \\
            7 \\
            2 \\
            4 \\
        \end{pmatrix}.
    \]
    Find $\alpha_1, \alpha_2, \alpha_3, \alpha_4$ such that
    \[
        \widehat u(x) := \alpha_1 \, \cos(\pi x) + \frac{\alpha_2}{x} + \alpha_3 \, 2^x + \alpha_4 \, x^2.
    \]
    satisfies
    \[
        \forall i \in \{1, 2, 3, 4\}, \qquad
        \widehat u(x_i) = y_i.
    \]
    Plot on the same graph the function $\widehat u$ and the data points.
\end{compexercise}

\newpage
\begin{compexercise}
    [Approximation, \mymarks{10}]
    Write, without using the \julia{Polynomials} library,
    a function \julia{approx(x, y, d, X)} to obtain,
    given data points
    \[
        (x_0, y_0), \dotsc, (x_N, y_N)
    \]
    and a nonnegative integer $0 \leq d \leq N$,
    the polynomial $p \in \poly(d)$ minimizing the total error
    \[
        E := \frac{1}{2} \sum_{n=0}^{N} \bigl\lvert p(x_n) - y_n \bigr\rvert^2.
    \]
    Your function should a vector containing the values $p(X_0), \dotsc, p(X_M)$,
    where $X_0, \dotsc, X_M$ are the elements of the vector \julia{X}.
    Within the function, proceed in 3 steps:
    \begin{itemize}
        \item
            First create the following matrix and vector:
            \[
                \mat A
                \begin{pmatrix}
                    1 & x_0 & \hdots & x_0^d \\
                    \vdots & \vdots & & \vdots \\
                    1 & x_{N} & \hdots & x_N^d
                \end{pmatrix},
                \qquad
                \vect b :=
                \begin{pmatrix}
                    y_0 \\
                    \vdots \\
                    y_N
                \end{pmatrix}.
            \]
            \item
                Then solve the normal equations using the backslash operator:
                \[
                    \mat A^\t \mat A \vect \alpha = \mat A^\t \vect b.
                \]

            \item
                Finally, evaluate the polynomial
                \[
                    p(x) = \alpha_0 + \alpha_1 x + \dotsc + \alpha_d x^d.
                \]
                at all the points in \julia{X} and return the result in a vector.
    \end{itemize}
    Use the data given in the notebook,
    of the altitude of a marble in free fall as a function of time,
    to test your code.
    The experiment was performed on a different planet.
    Can you find which one?
    See \url{https://en.wikipedia.org/wiki/Gravitational_acceleration}.
\end{compexercise}

\newpage
\begin{compexercise}
    [Numerical integration, \mymarks{10}]
    Milne's integration rule reads
    \[
        \int_{-1}^{1} u(x) \, \d x
        \approx \frac{2}{3} \left( 2 f\left(-\frac{1}{2}\right) - f(0) + 2 f\left(\frac{1}{2}\right) \right)
    \]
    \begin{itemize}
        % \item
        %     Write a function \julia{get_milne_weights} which calculates and returns a vector containing the weights~$w_1, w_2, w_3$.

        \item
            Write a function \julia{composite_milne(u, a, b, N)},
            which returns an approximation of the integral
            \[
                \int_{a}^{b} u(x) \, \d x
            \]
            obtained by partitioning the integration interval $[a, b]$ into $N$ cells,
            and applying Milne's rule within each cell.

        \item
            Take $u(x) = \cos(x)$, $a = -1$ and $b = 1$.
            Plot the evolution of the error for $N$ varying from 1 to 1000.

        \item
            Estimate the order of convergence with respect to $N$, i.e.\ find~$\alpha$ such that
            \[
                \lvert \widehat I_{N} - I \rvert \propto C N^{-\alpha},
            \]
            where $I$ denotes the exact value of the integral
            and $\widehat I_{N}$ denotes its approximation.
            In order to find~$\alpha$,
            use the function \julia{fit} from the \julia{Polynomials} package to find a linear approximation
            of the form
            \[
                \log \lvert \widehat I_{N} - I \rvert \approx \log (C) - \alpha \log(N).
            \]
    \end{itemize}
\end{compexercise}

\end{document}
