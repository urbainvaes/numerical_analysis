\chapter*{List of examinable proofs}%
This list collects the examinable results for this course.
It will grow and may be modified as the course progresses.
You don't have to remember the statements of the results,
but should be able to prove them given the statements.

\subsection*{\cref{cha:interpolation_and_approximation}}%

\begin{itemize}
    \item \Cref{theorem:interpolation_error} (Interpolation error).
    \item \Cref{corollary:interpolation_error} (Corollary following from \cref{theorem:interpolation_error}).
    \item \Cref{theorem:minimum_infty_norm} (Monic polynomial with minimum~$\infty$ norm).
    \item \Cref{corollary:chebyshev_nodes} (Derivation of Chebyshev nodes).
    \item Derivation of normal equations (in text).
\end{itemize}

\subsection*{\cref{cha:quadrature}}%

\begin{itemize}
    \item Derivation of the Newton--Cotes integration rules (in text).
    \item \Cref{theorem:integration_error_trapeze,theorem:integration_error_simpson} 
        (Error estimates for the composite trapezoidal and Simpson rules).
    \item
        \Cref{theorem:gauss_legendre_connection_polynomials}
        (Connection between Gaussian quadrature and Legendre polynomials).
\end{itemize}

\subsection*{\cref{cha:solution_of_linear_systems}}%

\begin{itemize}
    \item \Cref{proposition:linear_perturbation_rhs} (upper bound on the condition number with respect to perturbations of the right-hand side);
    \item \Cref{proposition:linear_perturbation_matrix} (upper bound the on condition number with respect to perturbations of the matrix),
        if you are given the preceding \cref{lemma:linear_inverse_neumann};
    \item \Cref{lemma:linear_inverse_product_gaussian_transformations} (explicit expression of the matrix $\mat L$ given the parameters of the Gaussian transformations).
    \item \Cref{proposition:matrices_convergence_power_of_matrix},
        in the case where $\mat A$ is diagonalizable (equivalence between $\rho(\mat A) < 1$ and the convergence $\norm{\mat A^k} \to 0$ as $k \to \infty$).

    \item \Cref{proposition:linear_convergence} when given Gelfand's formula (convergence of the general splitting method).
    \item Derivation of the optimal $\omega$ in Richardson's method.
    \item \Cref{proposition:linear_convergence_jacobi} (convergence of Jacobi's method in the case of a strictly diagonally dominant matrix).
    \item \Cref{proposition:criterion_convergence} and \cref{corollary:convergence_relaxation} (convergence of the relaxation method for Hermitian and positive definite $\mat A$).
    \item \Cref{theorem:linear_convergenec_steepest_descent} given the Kantorovich inequality (convergence of the steepest descent method).
\end{itemize}

\subsection*{\cref{cha:solution_of_nonlinear_systems}}%

\begin{itemize}
    \item \Cref{theorem:exponenital_convergence_fixed_point} (global exponential convergence of the fixed point iteration).
    \item \Cref{proposition:local_convergence_fixed_point} (local exponential convergence of the fixed point iteration under a local Lipschitz condition).
    \item \Cref{proposition:local_convergence}  (local exponential convergence given bound on the Jacobian matrix).
    \item \Cref{proposition:superlinear_convergence} (superlinear convergence of fixed point iteration when the Jacobian is zero at the fixed point).
\end{itemize}


\subsection*{\cref{cha:numerical_computation_of_eigenvalues}}%

\begin{itemize}
    \item \Cref{proposition:convergence_of_the_power_iteration} (Convergence of the power iteration).
\end{itemize}

