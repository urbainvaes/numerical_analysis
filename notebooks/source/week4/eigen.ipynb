{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d8b67aaa306ad4bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Week 4: Eigenproblems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff\n",
    "using LinearAlgebra\n",
    "using Polynomials\n",
    "using SpecialFunctions\n",
    "using TestImages\n",
    "using LaTeXStrings\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6c26ba24c814e8fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### <font color='green'> Single eigenvalues</font>\n",
    "### <font color='orange'>[Exercise 1]</font> Inverse and Rayleigh iteration\n",
    "1. Given a $N\\times N$ matrix $A$ and a complex number $\\mu \\notin \\sigma(A)$, implement the inverse iteration to approximate the eigenvalue of $A$ which is closest to $\\mu$\n",
    "   and an associated eigenvector, using as stopping criterion that $$\\|A\\hat v - \\hat\\lambda\\hat v\\| \\leq \\varepsilon\\|\\hat v\\|.$$\n",
    "\n",
    " The method will take as arguments the matrix $A$, the number $\\mu$, the starting vector $x_0$, and as keyword arguments the tolerance $\\varepsilon$ and a maximum number of iterations before aborting.\n",
    " Return a triplet $(\\lambda_n,v_n,n)$ with the approximate (normalized) eigenpair and the number $n$ of iterations taken.\n",
    "   <details>\n",
    "       <summary>\n",
    "           <em><font color='gray'>Hint (click to display)</font></em>\n",
    "       </summary>\n",
    "\n",
    "   - By functional calculus, $\\lambda_j = \\underset{\\lambda\\in \\sigma(A)}{\\mathrm{argmin}} |\\lambda_j-\\mu|$ if and only if $(\\lambda_j-\\mu)^{-1}$ is the dominant eigenvalue of $(A-\\mu I_N)^{-1}$\n",
    "   - It is <b>not</b> necessary (or efficient) to compute the inverse of $A-\\mu I_N$ (read the documentation for `LinearAlgebra.factorize`)\n",
    "   - The identity matrix need not be created explicitly;\n",
    "     To construct $A - \\mu I_N$, simply write `A - μ*I` (assuming that `LinearAlgebra` has been imported with `using`)\n",
    "   </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1590f5d6d58133cf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "function inverse_iteration(A, μ, x₀; tol=1e-12, maxiter=100)\n",
    "    x = copy(x₀)\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    M = factorize(A-μ*I)\n",
    "    niter = 0\n",
    "    while niter < maxiter\n",
    "        niter += 1\n",
    "        x .= M\\x\n",
    "        norm2_x = x'x\n",
    "        norm_x = √norm2_x\n",
    "        x /= norm_x\n",
    "        λ = x'A*x\n",
    "        (norm(A*x - λ*x) ≤ tol) && return (λ,x,niter)\n",
    "    end\n",
    "    ### END SOLUTION\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Implement the Rayleigh quotient iteration, taking as arguments the matrix $A$, the initial $\\mu_0$, the initial vector $x_0$, and as keywords the tolerance $\\varepsilon$ and a maximum number of iterations. As above, return a triple with the eigenpair and the number of iterations taken, using the same stopping criterion.\n",
    "    <details>\n",
    "       <summary>\n",
    "           <em><font color='gray'>Hint (click to display)</font></em>\n",
    "       </summary>\n",
    "\n",
    "   - This method amounts to setting $\\mu_k$ to the current estimate of the target eigenvalue, which is the Rayleigh quotient $\\frac{v_k^* A v_k}{v_k^*v_k}$.\n",
    "   - Again, no need to invert $A-\\mu_k I_N$. Should you use `LinearAlgebra.factorize`?\n",
    "   </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-65a77defb2af88bd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "function rayleigh_iteration(A, μ₀, x₀; tol=1e-12, maxiter=100)\n",
    "\n",
    "    x = copy(x₀)\n",
    "    μ = μ₀\n",
    "    niter = 0\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    while niter < maxiter\n",
    "        niter += 1\n",
    "        x .= (A - μ*I) \\ x\n",
    "        norm2_x = x'x\n",
    "        norm_x = √norm2_x\n",
    "        x /= norm_x\n",
    "        μ = x'A*x\n",
    "        (norm(A*x - μ*x) ≤ tol) && return (μ,x,niter)\n",
    "\n",
    "    end\n",
    "    ### END SOLUTION\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with the value of $N$ in the code below (keep $N$ below 1000).\n",
    "There is no guarantee that the Rayleigh iteration method will converge to the eigenvalue closest to $\\mu_0$, so you may have to run the cell a few times for this to happen.\n",
    " Is the Rayleigh quotient method faster in number of iterations? In wall-clock time? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d9266a75a702108d",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test code\n",
    "N = 1000\n",
    "A = randn(N,N); A = A*A'/N\n",
    "x₀ = randn(N); x₀ /= √(x₀'x₀)\n",
    "μ = 2.0\n",
    "ε = 1e-12\n",
    "\n",
    "@time λ,x,n = inverse_iteration(A,μ,x₀;tol=ε)\n",
    "println(\"Inverse iteration: eigenvalue $(λ) found in $n iterations.\")\n",
    "@time λ_r,x_r,n_r = rayleigh_iteration(A,μ,x₀;tol=ε)\n",
    "println(\"Rayleigh quotient iteration: eigenvalue $(λ_r) found in $n_r iterations.\")\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "if abs(λ-λ_r) < 1/N^2 # A has about N eigenvalues in [0,4]\n",
    "    @assert (abs(λ-λ_r) ≤ 2ε)\n",
    "    @assert all(abs.((x'A*x_r)/(x'x_r) .- λ) .≤ ε)\n",
    "end\n",
    "\n",
    "@assert n_r ≤ n\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'> Multiple eigenvalues</font>\n",
    "From now on, $A$ will be a Hermitian matrix.\n",
    "### <font color='orange'>[Exercise 2]</font> Subspace iteration and SVD\n",
    "The subspace iteration method is defined, given an initial condition $X_0\\in\\mathbb C^{n\\times p}$, by the recursion:\n",
    "$$ X_{n+1} R_{n+1} = A X_n,$$\n",
    "where $X_{n+1} R_{n+1}$ is the partial QR decomposition of $AX_n \\in \\mathbb C^{n\\times p}$. One can show, under suitable conditions, that the columns of $X_n$ converge to the $p$ dominant eigenvectors of $A$ (see lecture notes).\n",
    "\n",
    "\n",
    "1. Implement the partial QR decomposition, i.e. a function `myQR` taking as argument a $n\\times p$ matrix $M$ and returning $Q,R$,\n",
    "   where $Q$ is a $n\\times p$ orthogonal matrix and $R$ is a $p\\times p$ upper triangular matrix with positive diagonal entries.\n",
    "    <details>\n",
    "       <summary>\n",
    "           <em><font color='gray'>Hint (click to display)</font></em>\n",
    "       </summary>\n",
    "\n",
    "    - Use induction on $p$, writing $$ Q = \\begin{pmatrix} \\widetilde Q &\\mathbf{q}\\end{pmatrix},\\qquad R = \\begin{pmatrix} \\widetilde R & \\mathbf{r} \\\\ 0 & \\alpha\\end{pmatrix}, \\qquad M=\\begin{pmatrix} \\widetilde{M} & \\mathbf m\\end{pmatrix},$$ with $\\widetilde Q,\\widetilde M \\in \\mathbb C^{n\\times (p-1)}$, $\\widetilde R\\in\\mathbb C^{(p-1)\\times (p-1)}$, $\\mathbf r \\in \\mathbb C^{p-1}$, $\\alpha\\in \\R_+$ et $\\mathbf m\\in \\mathbb C^n$, assuming you know the decomposition $\\widetilde Q \\widetilde R = \\widetilde M$ of the matrix consisting of the $(p-1)$ first columns of $M$ (the case $p=1$ is trivial).\n",
    "   </details>\n",
    "\n",
    "- To avoid unecessary memory allocations, it is often useful in Julia to use `view(A,ix...)` instead of `A[ix...]` when performing array operations.\n",
    "To understand why, recall from Week 1 that Julia will by default allocate memory when slicing an array as an r-value `B = A[ix...]` (`B` is a <b>copy</b> of the slice of `A`) vs slicing as a l-value.\n",
    "As a result, the following happens in Julia:\n",
    "```julia\n",
    "    A = randn(20000,20000)\n",
    "    A[1:20000,1] === A[1:20000,1]\n",
    "    # false\n",
    "```\n",
    "To avoid this, the method `view(A,ix...)` returns <b>a reference</b> to the slice of `A`, which behaves in other regards as an array. For example:\n",
    "```julia\n",
    "    @time A[1:20000,1]'A[1,1:20000]\n",
    "    # 0.000655 seconds (6 allocations: 312.625 KiB)\n",
    "    @time view(A,1:20000,1)'view(A,1,1:20000)\n",
    "    #0.000369 seconds (4 allocations: 208 bytes)\n",
    "```\n",
    "For convenience, Julia also provides the [macro](https://docs.julialang.org/en/v1/manual/metaprogramming/#man-macros) `@views` which transforms every expression of the form `A[ix...]` into one of the form `view(A,ix...)` in the code on which it operates:\n",
    "```julia\n",
    "    @views A[1:20000,1] === A[1:20000,1] # transforms a line of code\n",
    "    # true\n",
    "\n",
    "    @time @views A[1:20000,1]'A[1,1:20000]\n",
    "    #0.000289 seconds (4 allocations: 208 bytes)\n",
    "\n",
    "    @views function f(M) # or a full block of code\n",
    "            return M[1:20000,1]'M[1,1:20000]\n",
    "        end\n",
    "    @time f(A)\n",
    "    # 0.000314 seconds (1 allocation: 16 bytes)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-91162ed9ba1c801c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@views function myQR(M)\n",
    "    n,p = size(M)\n",
    "\n",
    "    @assert p <= n \"Error: p > n\"\n",
    "\n",
    "    Q = zero(M)\n",
    "    R = zeros(eltype(M),p,p)\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "\n",
    "    for k=1:p\n",
    "        a = view(M,:,k)\n",
    "        Q̃ = view(Q,:, 1:k-1)\n",
    "        r = view(R,1:k-1,k)\n",
    "        q = view(Q,:,k)\n",
    "        \n",
    "        r .= Q̃'a\n",
    "        q .= a - Q̃*r\n",
    "        α = sqrt(q'q)\n",
    "        q ./= α\n",
    "        R[k,k] = α\n",
    "    end\n",
    "\n",
    "    ### END SOLUTION\n",
    "\n",
    "    return Q,R\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-48703f7eb64a8594",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test code\n",
    "M = randn(ComplexF64,100,50)\n",
    "M = big.(M)\n",
    "Q,R = myQR(M)\n",
    "@show norm(Q*R-M)\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "@assert norm(Q*R-M) < 1e-50\n",
    "@assert norm(Q'Q-I) < 1e-50\n",
    "@assert all(isreal.(diag(R)))\n",
    "@assert all(real.(diag(R)) .>= 0)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Implement the subspace iteration, taking as arguments the matrix $B$, the number of eigenvalues $p$, and a fixed number of iterations $n = $`niter`.\n",
    "   Return the approximate eigenpairs $(\\boldsymbol{\\lambda}_n,X_n)$ after $n$ iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e4e09644cd068383",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "function myEigen(B, p, niter)\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    n = size(B, 1)\n",
    "    X = randn(n, p)\n",
    "    R = zeros(p,p)\n",
    "    for i in 1:niter\n",
    "        X, R = myQR(B*X)\n",
    "    end\n",
    "    λs = diag(X'*B*X)\n",
    "    ### END SOLUTION\n",
    "\n",
    "    return λs, X\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-29664d8bc6aceb60",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@show myEigen([1. 2.; 2. 1.], 2, 100)[1]\n",
    "@show myEigen([1. 2.; 2. 1.], 2, 100)[2]\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "@assert begin n = 2; A = randn(n, n); myEigen(A'A, n, 100)[1] ≈ reverse(eigen(A'A).values) end\n",
    "@assert begin n = 4; A = randn(n, n); myEigen(A'A, n, 100)[1] ≈ reverse(eigen(A'A).values) end\n",
    "@assert begin A = randn(5, 5); q, r = qr(A); B = q*Diagonal(1:5)*q'; myEigen(B, 3, 100)[1] ≈ [5; 4; 3] end\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the singular value decomposition for a $B \\in \\mathbb C^{m\\times n}$\n",
    "$$B = U \\Sigma V^*, \\quad U\\in \\mathbb C^{m\\times m},\\quad \\Sigma \\in \\mathbb R^{m\\times n},\\quad V\\in \\mathbb C^{n\\times n},$$\n",
    "where $U U^* = U U^* = I_m$, $V V^* = V^* V = I_n$,\n",
    "and $\\Sigma \\in \\mathbb R^{m \\times n}$ is a rectangular diagonal matrix with non-negative real entries on the diagonal.\n",
    "The columns of $U$ (resp. $V$) are called the left (resp. right) singular vectors of $B$, and the diagonal entries of $\\Sigma$ are the (non-negative) singular values.\n",
    "\n",
    "3. Write a function `mySVD(B, p, niter)`\n",
    "   that returns the `p` dominant singular values of square matrix `B` (in a vector `σs`),\n",
    "   together with the associated left and right singular vectors (in matrices `Up` and `Vp`).\n",
    "\n",
    "    <details>\n",
    "       <summary>\n",
    "           <em><font color='gray'>Hint (click to display)</font></em>\n",
    "       </summary>\n",
    "\n",
    "    - Notice that\n",
    "      $$\n",
    "      A A^* = U \\Sigma^2 U^*, \\qquad\n",
    "      A^* A = V \\Sigma^2 V^*.\n",
    "      $$\n",
    "      Therefore, the left singular vectors of $A$\n",
    "      are the eigenvectors of $A A^*$,\n",
    "      while the right singular vectors of $A$\n",
    "      are the eigenvectors of $A^* A$.\n",
    "\n",
    "   - Once you have calculated the left and right singular vectors\n",
    "     associated with the `p` dominant singular values,\n",
    "     the singular values themselves can be obtained by extracting the diagonal from the matrix\n",
    "     $$\n",
    "     \\Sigma_p = U_p^* B V_p.\n",
    "     $$\n",
    "     Here $U_p$ and $V_p$ are the matrices containing as columns the left and right singular vectors associated with the `p` dominant singular values,\n",
    "     respectively.\n",
    "\n",
    "   </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-67643e1e3025dd6a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "function mySVD(B, p, niter)\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    n = size(B, 1)\n",
    "    λ₁, U = myEigen(B*B', p, niter)\n",
    "    λ₂, V = myEigen(B'B, p, niter)\n",
    "    σs = U'B*V\n",
    "    ### END SOLUTION\n",
    "    return diag(σs), U, V\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4cf89ce094ac0043",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "B = randn(n, n)\n",
    "σs, U, V = mySVD(B, n, 1000)\n",
    "@assert norm(U'U - I(n)) < 1e-10\n",
    "@assert norm(V'V - I(n)) < 1e-10\n",
    "@assert norm(U*Diagonal(σs)*V' - B) < 1e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. The singular value decomposition is very useful for compressing matrices.\n",
    "   The idea of matrix compression based on SVD is the following:\n",
    "   given $p \\leqslant n$, a matrix $B \\in \\mathbb C^{n\\times n}$\n",
    "   (we consider square matrices for simplicty)\n",
    "   can be approximated by\n",
    "   $$\n",
    "   \\widetilde {B} := U_p \\Sigma_p V_p^*,\n",
    "   $$\n",
    "   where $\\Sigma_p \\in \\mathbb R^{p \\times p}$ is a diagonal matrix containing the $p$ dominant singular values of $B$ on its diagonal,\n",
    "   and where $U_p \\in \\mathbb C^{n \\times p}$ and $V_p \\in \\mathbb C^{n \\times p}$ are rectangular matrices containing the associated left and right singular vectors,\n",
    "   respectively.\n",
    "\n",
    "   Since a grayscale image can be represented by a matrix containing the intensity values of all the pixels,\n",
    "   this approach for compressing matrices can be used for compressing grayscale images.\n",
    "   Use this method, i.e. calculate $\\widetilde {B}$, for $p \\in \\{5, 10, 20, 30\\}$,\n",
    "   in order to compress the image `woman_darkhair` given below (other available test images are listed [here](https://testimages.juliaimages.org/stable/imagelist/)),\n",
    "   and plot the compressed image for these values of `p`.\n",
    "\n",
    "   **Remarks**:\n",
    "   - (For information only) In practice, instead of storing the full matrix $\\widetilde {B}$,\n",
    "     which contains $n^2$ entries,\n",
    "     we can store only the matrices $U_p$, $\\Sigma_p$ and $V_p$,\n",
    "     which together contain only $(2n+1)p$ entries (we only count the $p$ diagonal entries of $\\Sigma_p$).\n",
    "     If $p \\ll n$,\n",
    "     then the memory required to store these matrices is much smaller than the memory required to store the initial matrix $B$.\n",
    "\n",
    "   - A function for drawing images based on the matrix of pixel intensity values is provided below, and serves as a test for your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = testimage(\"woman_darkhair\")\n",
    "\n",
    "# Convert image to matrix of Float64\n",
    "M = Float64.(A)\n",
    "n = size(M,1)\n",
    "\n",
    "# Function to plot a grayscale image from the matrix\n",
    "# containing the intensity values of all the pixels\n",
    "function plot_matrix(B, p)\n",
    "    plot(Gray.(B), ticks=false, showaxis=false, title=\"p=$p\")\n",
    "end\n",
    "\n",
    "plots = typeof(plot())[]\n",
    "\n",
    "for p in [5,10,20,30]\n",
    "    niter = 100\n",
    "    σs, U, V = mySVD(M, p, niter)\n",
    "    println(\"p = $p, compression ratio = \", (2n*(p+1))/(n^2))\n",
    "    push!(plots,plot_matrix(U*Diagonal(σs)*V', p))\n",
    "end\n",
    "\n",
    "push!(plots, plot_matrix(M, \"n\"))\n",
    "plot(plots...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>Using sparse matrices for high-dimensional problems</font>\n",
    "### <font color='orange'>[Exercise 3]</font> PageRank algorithm\n",
    "\n",
    "[PageRank](https://en.wikipedia.org/wiki/PageRank) is an algorithm that assigns a score to the vertices of a directed graph.\n",
    "It used to be used by major search engines to rank search results. In this context, the directed graph encodes the links between pages on the World Wide Web: the vertices of the directed graph represent web pages, and the edges represent connections between the pages: there is an edge from page $i$ to page $j$ if page $i$ contains a hyperlink to page $j$.\n",
    "\n",
    "Let us consider a directed graph $G(V, E)$ with vertices $V = \\{1, \\dotsc, n\\}$ and edges $E$. The graph can be represented by its adjacency matrix $A \\in \\{0, 1\\}^{n \\times n}$, where the entries are given by\n",
    "$$\n",
    "a_{ij} =\n",
    "\\begin{cases}\n",
    "    1 & \\text{if there is an edge from $i$ to $j$,} \\\\\n",
    "    0 & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "The idea of the PageRank algorithm, in its simplest form, is to assign scores $r_i$ to the vertices by solving the following system of equations:\n",
    "$$ \\tag{PageRank}\n",
    "    \\forall i \\in  V, \\qquad\n",
    "    r_i\n",
    "    = \\sum_{j \\in \\mathcal N(i)} \\frac{r_j}{o_j}.\n",
    "$$\n",
    "<span id=\"pagerank\"></span>\n",
    "where $o_j$ is the out-degree of vertex $j$, i.e., the number of edges with origin $j$. Here, the sum applies to the set of nodes in $\\mathcal N(i)$, which represents the set of incoming neighbors of vertex $i$, i.e., those that have an edge pointing to vertex $i$.\n",
    "\n",
    "Let $\\mathbf r = \\begin{pmatrix} r_1 & \\dots & r_n \\end{pmatrix}^T$. It is straightforward to show that solving the system <a href=\"#pagerank\">(PageRank)</a> is equivalent to solving the following problem:\n",
    "$$  \\tag{PageRank-vector}\n",
    "    \\mathbf r =\n",
    "    A^T\n",
    "    \\begin{pmatrix}\n",
    "        \\frac{1}{o_1} & &  \\\\\n",
    "                      & \\ddots & \\\\\n",
    "                      & & \\frac{1}{o_n}\n",
    "    \\end{pmatrix}\n",
    "    \\mathbf r =:  A^T O^{-1} \\mathbf r.\n",
    "$$\n",
    "<span id=\"pagerank\"></span>\n",
    "In other words, the problem boils down to finding an eigenvector with eigenvalue $1$ of the matrix $M = A^T O^{-1}$. Note that at this stage, we have neither proved the existence nor the uniqueness of a solution to this equation. The question of the uniqueness of a solution is related to the connectivity of the graph and will not be addressed here. However, we will demonstrate that a solution to the problem exists.\n",
    "\n",
    "**Remark.** The matrix $O^{-1} A$ is the transition matrix of a random walk on the directed graph, where at each step a move is made to an outgoing neighbor, with equal probability for each of them. Solving <a href=\"#pagerank\">(PageRank-vector)</a> is equivalent to finding a stationary distribution of this random walk.\n",
    "\n",
    "1. - Note that $M$ is a left stochastic matrix, i.e., the sum of the elements in each column is equal to 1.\n",
    "\n",
    "    - Prove that the eigenvalues of any matrix $B \\in \\mathbb R^{n \\times n}$ coincide with those of $B^T$. You can use the fact that $\\det(B) = \\det(B^T)$.\n",
    "\n",
    "    - Using the previous points, show that $1$ is an eigenvalue and that $\\rho(M) = 1$. For the second part, find a subordinate matrix norm such that $\\lVert M\\rVert= 1$. This demonstrates the existence of a solution to <a href=\"#pagerank\">(PageRank-vector)</a>, and also proves that $1$ is the dominant eigenvalue of $M$.\n",
    "\n",
    "We will apply PageRank to sort Wikipedia pages according to their importance. The two following cells download and parse the data into arrays. To limit computation time, only 5% of the best scored articles have been selected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Downloads\n",
    "import Tar\n",
    "\n",
    "# URL where data can be downloaded\n",
    "url = \"https://urbain.vaes.uk/static/wikidata.tar\"\n",
    "\n",
    "# Download the data\n",
    "filename = \"wikidata.tar\"\n",
    "isfile(filename) || Downloads.download(url, filename)\n",
    "\n",
    "# Extract data into directory `wikidata`\n",
    "directoryname = \"wikidata\"\n",
    "isdir(directoryname) || Tar.extract(filename, directoryname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CSV\n",
    "import DataFrames\n",
    "\n",
    "# Read nodes and edges into data frames\n",
    "nodes_dataframe = CSV.read(\"$directoryname/names.csv\", DataFrames.DataFrame)\n",
    "edges_dataframe = CSV.read(\"$directoryname/edges.csv\", DataFrames.DataFrame)\n",
    "\n",
    "# Convert data to matrices\n",
    "nodes = Matrix(nodes_dataframe)\n",
    "edges = Matrix(edges_dataframe)\n",
    "\n",
    "# The data structures should be self-explanatory\n",
    "edges_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many nodes are there? Compute the memory required to store every entry of $M$ using the `Float64` format.\n",
    "\n",
    "2. Implement a structure `struct MySparseMatrix` to represent a sparse matrix with `Float64` entries (in the COO format), and a method `*(M::MySparseMatrix,X::Vector{Float64})` to compute the product of a sparse matrix `M` by a vector `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-25fdfda7e1a65ea0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import Base.*\n",
    "\n",
    "struct MySparseMatrix\n",
    "    rows::Vector{Int}\n",
    "    cols::Vector{Int}\n",
    "    vals::Vector{Float64}\n",
    "    m::Int\n",
    "    n::Int\n",
    "end\n",
    "\n",
    "MySparseMatrix(R::Vector{Int},C::Vector{Int},V::Vector{Float64}) = MySparseMatrix(R,C,V,maximum(R),maximum(C))\n",
    "\n",
    "@inbounds function *(M::MySparseMatrix, X::Vector{Float64})\n",
    "    @assert size(X, 1) == M.n \"Incompatible dimensions: M has $(M.n) columns but X has $(length(X)) rows.\"\n",
    "    ### BEGIN SOLUTION\n",
    "    Y = zeros(M.m)\n",
    "    for (i,j,v) = zip(M.rows, M.cols, M.vals)\n",
    "        Y[i] += v*X[j]\n",
    "    end\n",
    "    return Y\n",
    "    ### END SOLUTION\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code with the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-969a9f1e55c7cf13",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "m, n = 4, 3\n",
    "R = [2, 2, 2, 3, 3]\n",
    "C = [1, 2, 3, 1, 3]\n",
    "V = [5., 6., 7., 8., 9.]\n",
    "A = MySparseMatrix(R, C, V, m, n)\n",
    "b = [1.; 1.; 1.]\n",
    "@assert A*b == [0.; 18.; 17.; 0.] \"Multiplication does not work!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Construct the left-stochastic matrix $M$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0e3e63a076fe4d8c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "nn, ne = length(nodes), size(edges, 1)\n",
    "\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "# Count the number of outbound edges for each node\n",
    "n_outbound = zeros(Int, nn)\n",
    "for e in eachrow(edges)\n",
    "    n_outbound[e[1]] += 1\n",
    "end\n",
    "\n",
    "# Build matrix\n",
    "R, C = edges[:, 1], edges[:, 2]\n",
    "V = 1 ./ n_outbound[R]\n",
    "### END SOLUTION\n",
    "\n",
    "M = MySparseMatrix(C, R, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Implement the power iteration to compute the eigenvector associated with the principal eigenvalue of $M$.\n",
    "   Since the eigenvalue $\\lambda=1$ is known, you can use $\\|Mr-r\\| \\leq \\varepsilon\\|r\\|$ as a stopping criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f32c638e9907f43e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "function power_iteration(H, x; ε=1e-12, maxiter=1000)\n",
    "    ### BEGIN SOLUTION\n",
    "    niter = 0\n",
    "    while niter < maxiter\n",
    "        niter += 1\n",
    "        x = x / √(x'x)\n",
    "        Hx = H*x\n",
    "        λ = x'Hx\n",
    "        e = norm(Hx - λ*x)\n",
    "        x = Hx\n",
    "        e ≤ ε && return x\n",
    "    end\n",
    "    return nothing\n",
    "    ### END SOLUTION\n",
    "    # Return only the eigenvector, not the eigenvalue\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1ca5c885bb313a20",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@assert [1, -1]'power_iteration([1. 2.; 2. 1.], [1., 0.]) |> abs < 1e-9\n",
    "@assert [1, 0]'power_iteration([0. 0.; 0. 1.], [1., 1.]) |> abs < 1e-9\n",
    "@assert [0, 1]'power_iteration([1. 0.; 0. .5], [1., 1.]) |> abs < 1e-9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell runs PageRank:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3f21d7c9c4d9083b",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x = ones(nn) / nn\n",
    "x = @time power_iteration(M, x)\n",
    "\n",
    "p = sortperm(x, rev=true)\n",
    "sorted_nodes = view(nodes, p)\n",
    "\n",
    "print(join(sorted_nodes[1:20], \"\\n\"))\n",
    "@assert sorted_nodes[1] == \"United States\"\n",
    "@assert sorted_nodes[2] == \"United Kingdom\"\n",
    "@assert sorted_nodes[3] == \"World War II\"\n",
    "@assert sorted_nodes[4] == \"Latin\"\n",
    "@assert sorted_nodes[5] == \"France\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Write a function `search(keyword)` to perform a search in the database. Here's an example of what this function could return:\n",
    "\n",
    "```\n",
    "   julia> search(\"Newton\")\n",
    "   47-element Vector{String}:\n",
    "    \"Isaac Newton\"\n",
    "    \"Newton (unit)\"\n",
    "    \"Newton's laws of motion\"\n",
    "    …\n",
    "   ```\n",
    "<details>\n",
    "      <summary>\n",
    "      <em><font color='gray'>Hint (click to display)</font></em>\n",
    "   </summary>\n",
    "\n",
    "   - The method `filter(condition, itr)` returns a filtered version of the iterable `itr` with only the elements `x` satisfying `condition(x) == true` remaining.\n",
    "   - The method `occursin(needle::String, haystack::String)` returns `true` if and only if `needle` is a substring of `haystack`.\n",
    "\n",
    "   Of course, the best way to understand these methods is to <u> read the documentation </u>.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8569725918020197",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "function search(keyword)\n",
    "    ### BEGIN SOLUTION\n",
    "    filter(s -> occursin(keyword, s), sorted_nodes)\n",
    "    ### END SOLUTION\n",
    "end\n",
    "\n",
    "search(\"Newton\")"
   ]
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Julia 1.10.3",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
