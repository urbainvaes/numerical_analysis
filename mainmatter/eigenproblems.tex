\chapter{Numerical computation of eigenvalues}%
\label{cha:numerical_computation_of_eigenvalues}
\minitoc

\section*{Introduction}

Calculating the eigenvalues and eigenvectors of a matrix is a task often encountered in scientific and engineering applications.
Eigenvalue problems naturally arise in quantum physics,
solid mechanics, structural engineering and molecular dynamics,
to name just a few applications.
The aim of this chapter is to present an overview of the standard methods for calculating eigenvalues and eigenvectors numerically.
We focus predominantly on the case of a Hermitian matrix $\mat A \in \complex^{n \times n}$,
which is technically simpler and arises in many applications.
The reader is invited to go through the background material in~\cref{sec:diagonalization} before reading this chapter.
The rest of this chapter is organized as follows
\begin{itemize}
    \item
        In \cref{sec:general_remarks},
        we make general remarks concerning the calculation of eigenvalues.

    \item
        In \cref{sec:simple_vector_iterations},
        we present standard methods based on a simple vector iteration.

    \item
        In \cref{sec:subspace_iteration},
        we present a method for calculating several eigenvectors simultaneously,
        based on iterating a subspace.

    \item
        In \cref{sec:projection_methods},
        we present method for constructing an approximation of the eigenvectors in a given subspace of~$\complex^{n}$.
\end{itemize}

\section{Numerical methods for eigenvalue problems: general remarks}
\label{sec:general_remarks}

As mentioned in~\cref{sec:diagonalization},
a complex number $\lambda \in \complex$ is an eigenvalue of $\mat A \in \complex^{n \times n}$
if and only if $\lambda$ is a root of the characteristic polynomial $p_A \colon \complex \to \complex$ of $\mat A$,
which is given by
\[
    p_A(\lambda) = \det (\mat A - \lambda \mat I).
\]
One may,
therefore,
calculate the eigenvalues of $\mat A$ by calculating the roots of the polynomial~$p_A$ using,
for example, one of the methods presented in~\cref{cha:solution_of_nonlinear_systems}.
While feasible for small matrices,
this approach is not viable for large matrices,
because the number of floating point operations required for calculating calculating the coefficients of the characteristic polynomial scales as the factorial of $n$.

In view of the prohibitive computational cost required for calculating the characteristic polynomial,
other methods are required for solving large eigenvalue problems numerically.
All the methods that we study in this chapter are of iterative nature.
While some of them are aimed at calculating all the eigenpairs of the matrix~$\mat A$,
other methods enable to calculate only a small number of eigenpairs at a lower computational cost,
which is often desirable.
Indeed, calculating all the eigenvalues of a large matrix is computationally expensive;
on a personal computer, the following Julia code takes well over a second to terminate:
\begin{minted}{julia}
    import LinearAlgebra
    A = rand(2000, 2000)
    LinearAlgebra.eigen(A)
\end{minted}

In many applications,
the matrix $\mat A$ is sparse,
and in this case it is important to use algorithms for eigenvalue problems that do not destroy the sparsity structure.
Note that the eigenvectors of a sparse matrix are generally not sparse.

To conclude this section,
we introduce some notation used throughout this chapter.
For a diagonalizable matrix $\mat A$,
we denote the eigenvalues by $\lambda_1, \dotsc, \lambda_n$,
with $\abs{\lambda_1} \geq \abs{\lambda_2} \geq \dotsc \geq \abs{\lambda_n}$.
The associated normalized eigenvectors are denoted by $\vect v_1, \dotsc, \vect v_n$.
Therefore, it holds that
\[
    \mat V^{-1} \mat A \mat V = \mat D = \diag(\lambda_1, \dotsc, \lambda_n),
    \qquad \text{where } \mat V = \begin{pmatrix} \vect v_1 & \hdots & \vect v_n \end{pmatrix}.
\]

\section{Simple vector iterations}
\label{sec:simple_vector_iterations}

In this section,
we present simple iterative methods aimed at calculating just one eigenvector of the matrix~$\mat A$,
which we assume to be diagonalizable for simplicity.

\subsection{The power iteration}
The power iteration is the simplest method for calculating the eigenpair associated with the eigenvalue of~$\mat A$ with largest modulus.
Since the eigenvectors of $\mat A$ span $\complex^n$,
any vector $\vect x_0$ may be decomposed as
\begin{align}
    \label{eq:eigen_decomposition_x0}
    \vect x_0 = \alpha_1 \vect v_1 + \dotsb + \alpha_n \vect v_n.
\end{align}
The idea of the power iteration is to repeatedly left-multiply this vector by the matrix $\mat A$,
in order to amplify the coefficient of $\vect v_0$ relative to the other ones.
Indeed, notice that
\[
    \mat A^k \vect x_0 = \lambda_1^k \alpha_1 \vect v_1 + \dotsb + \lambda_n^k \alpha_n \vect v_n.
\]
If $\lambda_1$ is strictly greater in modulus than the other eigenvalues,
and if $\alpha_1 \neq 0$,
then for large $k$ the vector $\mat A^k \vect x_0$ is approximately aligned,
in a sense made precise below,
with the eigenvector~$\vect v_1$.
In order to avoid overflow errors at the numerical level,
the iterates are normalized at each iteration.
The power iteration is presented in~\cref{algo:power_iteration}.
\begin{algorithm}
\caption{Power iteration}%
\label{algo:power_iteration}%
\begin{algorithmic}
\State $\vect x \gets \vect x_0$
\For{$i \in \{1, 2, \dotsc\}$}
    \State $\vect x \gets \mat A \vect x$
    \State $\vect x \gets \vect x / \norm{\vect x}$
\EndFor
\end{algorithmic}
\end{algorithm}

To precisely quantify the convergence of the power method,
we introduce the notion of \emph{acute angle} between vectors of $\complex^n$.
\begin{align*}
    \angle(\vect x, \vect y)
    &= \arccos\left( \frac{\abs{\vect x^* \vect y}}{\sqrt{\vect x^* \vect x} \sqrt{\vect y^* \vect y}}\right) \\
    &= \arcsin\left( \frac{\norm{(\mat I - \mat P_{\vect y}) \vect x}} {\norm{\vect x}} \right),
    \qquad \mat P_{\vect y} := \frac{\vect y \vect y^*}{\vect y^* \vect y}.
\end{align*}
This definition generalizes the familiar notion of angle for vectors in $\real^2$ or $\real^3$,
and we note that the angle function satisfies $\angle(\e^{\i \theta_1} \vect x, \e^{\i \theta_2} \vect y) = \angle(\vect x, \vect y)$ as well as $\angle(\vect x, \vect y) \in [0, \pi/2]$.
We can then prove the following convergence result.
\begin{proposition}
    [Convergence of the power iteration]
    \label{proposition:convergence_of_the_power_iteration}
    Suppose that $\mat A$ is diagonalizable and that $\abs{\lambda_1} > \abs{\lambda_2}$.
    Then, for every initial guess with $\alpha_1 \neq 0$,
    the sequence $(\vect x_k)_{k \geq 0}$ generated by the power iteration satisfies
    \[
        \lim_{k \to \infty} \angle(\vect x_k, \vect v_1) = 0.
    \]
\end{proposition}
\begin{proof}
    By construction,
    it holds that
    \begin{equation}
        \label{eq:power_iteration_iterate}
        \vect x_k
        = \frac{\lambda_1^k \alpha_1 \vect v_1 + \dotsb + \lambda_n^k \alpha_n \vect v_n}{\norm{\lambda_1^k \alpha_1 \vect v_1 + \dotsb + \lambda_n^k \alpha_n \vect v_n}}
        = \e^{\i \theta_k} \frac{\vect v_1 + \frac{\lambda_2^k \alpha_2}{\lambda_1^k \alpha_1} \vect v_2 +  \dotsb + \frac{\lambda_n^k \alpha_2}{\lambda_n^k \alpha_1} \vect v_n }{\norm*{\vect v_1 + \frac{\lambda_2^k \alpha_2}{\lambda_1^k \alpha_1} \vect v_2 +  \dotsb + \frac{\lambda_n^k \alpha_n}{\lambda_1^k \alpha_1} \vect v_n}},
    \end{equation}
    where
    \[
        \e^{\i \theta_k} := \frac{\lambda_1^k \alpha_1}{\abs{\lambda_1^k \alpha_1}}.
    \]
    It follows from~\eqref{eq:power_iteration_iterate} that $\e^{-\i \theta_k} \vect x_k \to \vect v_1/ \norm{\vect v_1} = \vect v_1$ in the limit as $k \to \infty$,
    where we employed the fact that $\norm{\vect v_1} = 1$.
    Using the definition of the angle between two vectors in $\complex^n$,
    and the continuity with respect to either argument of the $\complex^n$ Euclidean inner product and of the $\arccos$ function,
    we obtain that
    \begin{align*}
        \angle(\vect x_k, \vect v_1)
        &= \arccos\left( \frac{\abs{\vect v_1^* \vect x_k}}{\sqrt{\vect v_1^* \vect v_1} \sqrt{\vect x_k^* \vect x_k}}\right) = \arccos\left(\abs{\vect v_1^* \vect x_k} \right) \\
        &= \arccos\left( \abs*{\vect v_1^*\left(\e^{\i \theta_k}\vect x_k\right)} \right)
        \xrightarrow[k \to \infty]{} \arccos(1) = 0,
    \end{align*}
    which concludes the proof.
\end{proof}

An inspection of the proof also reveals that the dominant term in the error,
asymptotically in the limit as $k \to \infty$,
is the one with coefficient $\frac{\lambda_2^k \alpha_2}{\lambda_1^k \alpha_1}$.
Therefore, we deduce that
\[
    \angle(\vect x_k, \vect v_1) = \mathcal O \left( \abs*{\frac{\lambda_2}{\lambda_1}}^k \right).
\]
The convergence is slow if $\abs{\lambda_2/\lambda_1}$ is close to one,
and fast if $\abs{\lambda_2} \ll \abs{\lambda_1}$.
Once an approximation of the eigenvector $\vect v_1$ has been calculated,
the corresponding eigenvalue $\lambda_1$ can be estimated from the \emph{Rayleigh quotient:}
\begin{equation}
    \label{eq:rayleigh_quotient}
    \rho_{\mat A}\colon \mat \complex^n_* \to \complex\colon \vect x \mapsto \frac{\vect x^* \mat A \vect x}{\vect x^* \vect x}.
\end{equation}
For any eigenvector~$\vect v$ of $\mat A$,
the corresponding eigenvalue is equal to~$\rho_{\mat A}(\vect v)$.
In order to study the error on the eigenvalue~$\lambda_1$ for the power iteration,
we assume for simplicity that $\mat A$ is Hermitian
and that the eigenvectors $\vect v_1, \dotsc, \vect v_n$ are orthonormal.
Substituting~\eqref{eq:power_iteration_iterate} in the Rayleigh quotient~\eqref{eq:rayleigh_quotient},
we obtain
\[
    \rho_{\mat A}(\vect x_k)
    = \frac{\lambda_1 + \abs*{ \frac{\lambda_2^k \alpha_2}{\lambda_1^k \alpha_1} }^2 \lambda_2 +  \dotsb + \abs*{ \frac{\lambda_n^k \alpha_n}{\lambda_1^k \alpha_1} }^2 \lambda_n}
    {1 + \abs*{ \frac{\lambda_2^k \alpha_2}{\lambda_1^k \alpha_1} }^2 +  \dotsb +  \abs*{ \frac{\lambda_n^k \alpha_n}{\lambda_1^k \alpha_1} }^2}.
\]
Therefore,
by reducing to a common denominator we deduce
\begin{align*}
    \abs{\rho_{\mat A}(\vect x_k) - \lambda_1}
    &= \left\lvert \frac{\lambda_1 + \abs*{ \frac{\lambda_2^k \alpha_2}{\lambda_1^k \alpha_1} }^2 \lambda_2 +  \dotsb + \abs*{ \frac{\lambda_n^k \alpha_n}{\lambda_1^k \alpha_1} }^2 \lambda_n}
    {1 + \abs*{ \frac{\lambda_2^k \alpha_2}{\lambda_1^k \alpha_1} }^2 +  \dotsb +  \abs*{ \frac{\lambda_n^k \alpha_n}{\lambda_1^k \alpha_1} }^2} - \lambda_1 \right\rvert \\
    &\leq  \abs*{ \frac{\lambda_2^k \alpha_2}{\lambda_1^k \alpha_1} }^2 \abs{\lambda_2 - \lambda_1} +  \dotsb + \abs*{ \frac{\lambda_n^k \alpha_n}{\lambda_1^k \alpha_1} }^2 \abs{\lambda_n- \lambda_1}
    = \mathcal O\left(\abs*{\frac{\lambda_2}{\lambda_1}}^{2k}\right).
\end{align*}

The convergence of the eigenvalue in the particular case of a Hermitian matrix is faster than for a general matrix in~$\complex^{n \times n}$.
For general matrices,
it is possible to show using a similar argument
that the error is of order~$\mathcal O\bigl(\abs{\lambda_2/\lambda_1}^k\bigr)$
in the limit as $k \to \infty$.

\paragraph{Essential convergence.}
It is useful at this point to introduce the concept of \emph{essential convergence}.
A sequence~$(\vect x_k)$ in $\complex^n$ is said to \emph{converge essentially} to a vector~$\vect x_{\infty}$ if
there exists a sequence of complex numbers~$\left(\e^{\i \phi_k}\right)$ such that
the sequence $\left( \e^{\i \phi_k} \vect x_k \right)$ converges to $\vect x_{\infty}$.
Equivalently, the sequence~$(\vect x_k)$ converges essentially to $\vect x_{\infty}$ if
$\angle(\vect x_k, \vect x_{\infty})$ converges to 0.
Proving this equivalence is the goal of~\cref{exercise:essential_convergence}.
Reformulated in this new terminology,
\cref{proposition:convergence_of_the_power_iteration} states that the sequence~$(\vect x_k)$ obtained from the power iteration converges essentially to $\vect v_1$.

\subsection{Inverse iteration}
The power iteration is simple but enables to calculate only the dominant eigenvalue of the matrix~$\mat A$,
i.e.\ the eigenvalue of largest modulus.
In addition, the convergence of the method is slow when $\abs{\lambda_2} \approx \abs{\lambda_1}$.

The inverse iteration enables a more efficient calculation of not only the dominant eigenvalue
but also the other eigenvalues of~$\mat A$.
It is based on applying the power iteration to $(\mat A - \mu \mat I)^{-1}$,
where $\mu \in \complex$ is a shift.
The eigenvalues of $(\mat A - \mu \mat I)^{-1}$ are given by $(\lambda_1 - \mu)^{-1}, \dotsc, (\lambda_n - \mu)^{-1}$,
with associated eigenvectors $\vect v_1, \dotsc, \vect v_n$.
If $0 < \abs{\lambda_J - \mu} < \abs{\lambda_j - \mu}$ for all $j \neq J$,
then the dominant eigenvalue of the matrix $(\mat A - \mu \mat I)^{-1}$ is $(\lambda_J - \mu)^{-1}$,
and so the power iteration applied to this matrix yields an approximation of the eigenvector $\vect v_J$.
In other words,
the inverse iteration with shift~$\mu$ enables to calculate an approximation of the eigenvector of~$\mat A$ corresponding to the eigenvalue nearest~$\mu$.
The inverse iteration is presented in~\cref{algo:inverse_iteration}.
In practice, the inverse matrix $(\mat A - \mu \mat I)^{-1}$ need not be calculated,
and it is often preferable to solve a linear system at each iteration.
\begin{algorithm}
\caption{Inverse iteration}%
\label{algo:inverse_iteration}%
\begin{algorithmic}
\State $\vect x \gets \vect x_0$
\For{$i \in \{1, 2, \dotsc\}$}
    \State Solve $(\mat A - \mu \mat I) \vect y = \vect x$
    \State $\vect x \gets \vect y / \norm{\vect y}$
\EndFor
\State $\lambda \gets \vect x^* \mat A \vect x / \vect x^* \vect x$
\State \Return $\vect x, \lambda$
\end{algorithmic}
\end{algorithm}

An application of~\cref{proposition:convergence_of_the_power_iteration} immediately gives the following convergence result for the inverse iteration.
\begin{proposition}
    [Convergence of the inverse iteration]
    \label{proposition:inverse_iteration}
    Assume that $\mat A \in \complex^n$ is diagonalizable
    and that there exist $J$ and $K$ such that
    \[
        0 < \abs{\lambda_J - \mu} < \abs{\lambda_K - \mu} \leq \abs{\lambda_j - \mu} \qquad \forall j \neq J.
    \]
    Assume also that $\alpha_J \neq 0$,
    where $\alpha_J$ is the coefficient of $\vect v_J$ in the expansion of $\vect x_0$ given in~\eqref{eq:eigen_decomposition_x0}.
    Then the iterates of the inverse iteration satisfy
    \[
        \lim_{k \to \infty} \angle(\vect x_k, \vect v_J) = 0.
    \]
    More precisely,
    \[
        \angle(\vect x_k, \vect v_J) = \mathcal O \left( \abs*{\frac{\lambda_J - \mu}{\lambda_K - \mu}}^k \right).
    \]
\end{proposition}
\Cref{proposition:inverse_iteration} states that $\vect x_k$ converges essentially to~$\vect v_J$.
Notice that the closer $\mu$ is to~$\lambda_J$,
the faster the inverse iteration converges.
Note also that with $\mu = 0$,
the inverse iteration enables to calculate the eigenvalue of~$\mat A$ of smallest modulus.

\subsection{Rayleigh quotient iteration}
Since the inverse iteration is fast when $\mu$ is close to an eigenvalue $\lambda_J$,
it is natural to wonder whether the method can be improved by progressively updating $\mu$ as the simulation progresses.
Specifically, an approximation of the eigenvalue associated with the current vector may be employed in place of~$\mu$.
This leads to the Rayleigh quotient iteration,
presented in~\cref{algo:rayleigh_quotient}.
\begin{algorithm}
\caption{Inverse iteration}%
\label{algo:rayleigh_quotient}%
\begin{algorithmic}
\State $\vect x \gets \vect x_0$
\For{$i \in \{1, 2, \dotsc\}$}
    \State $\mu \gets \vect x^* \mat A \vect x / \vect x^* \vect x$
    \State Solve $(\mat A - \mu \mat I) \vect y = \vect x$
    \State $\vect x \gets \vect y / \norm{\vect y}$
\EndFor
\State $\lambda \gets \vect x^* \mat A \vect x / \vect x^* \vect x$
\State \Return $\vect x, \lambda$
\end{algorithmic}
\end{algorithm}

It is possible to show that, when~$\mat A$ is Hermitian,
the Rayleigh quotient iteration converges to an eigenvector for almost every initial guess~$\vect x_0$.
Furthermore, if convergence to an eigenvector occurs,
then $\mu$ converges cubically to the corresponding eigenvalue.
See~\cite{MR3396212} and the references therein for more details.

\section{Methods based on a subspace iteration}
\label{sec:subspace_iteration}
The subspace iteration resembles the power iteration but it is more general:
not just one but several vectors are updated at each iteration.

\subsection{Simultaneous iteration}
\label{sub:simultaneous_iteration}

Let $\mat X_0 = \begin{pmatrix} \vect x_1 & \dotsc & \vect x_p \end{pmatrix}$ denote an initial set of linearly independent vectors.
Before we present the simultaneous iteration,
we recall a statement concerning the $\mat Q \mat R$ decomposition of a matrix,
which is related to the Gram--Schmidt orthonormalization process.
We recall that the Gram--Schmidt method enables to construct,
starting form an ordered set of vectors $\{\vect x_1, \dotsc, \vect x_p \}$ in~$\complex^n$,
a new set of vectors $\{\vect q_1, \dotsc, \vect q_p \}$
which are \emph{orthonormal} and span the same subspace of $\complex^n$ as the original vectors.
\begin{proposition}
    [Reduced QR decomposition]
    Assume that $\mat X \in \complex^{n \times p}$ has linearly independent columns.
    Then there exist a matrix $\mat Q \in \complex^{n \times p}$ with orthonormal columns and
    an upper triangular matrix~$\mat R \in \complex^{p \times p}$ such that the following factorization holds:
    \[
        \mat X = \mat Q \mat R.
    \]
    This decomposition is known as a \emph{reduced $\mat Q \mat R$ decomposition} if $p < n$,
    or simply \emph{$\mat Q \mat R$ decomposition} if $p = n$,
    in which case $\mat X$ is a square matrix and $\mat Q$ is a unitary matrix.
    The decomposition is unique if we require that the diagonal elements of~$\mat R$ are real and positive.
\end{proposition}
\begin{proof}
    The statement is clear when $p=1$.
    Reasoning by induction,
    we assume that the result is true up to $p-1$ and prove that it then also holds true for~$p$.
    Let $\vect x_p$ denote the $p$-th column of $\mat X$,
    By the induction hypothesis,
    it holds that
    \begin{equation}
        \label{eq:qr_decomposition}
        \mat X =
        \begin{pmatrix}
            \mat Q_{p-1} \mat R_{p-1} &
            \vect x_p
        \end{pmatrix},
    \end{equation}
    for a unique matrix $\mat Q \in \complex^{n \times (p-1)}$ with orthonormal columns and
    a unique upper triangular matrix~$\mat R \in \complex^{(p-1) \times (p-1)}$ with positive real diagonal elements.
    We wish to show that there is a unique normalized vector $\vect q \in \complex^n$ orthogonal to the columns of~$\mat Q_{p-1}$,
    a unique vector $\vect r \in \complex^{n-1}$ and a unique scalar $r \in \real_{> 0}$ such that
    \begin{equation}
        \label{eq:qr_decomposition_second_expression}
        \mat X =
        \begin{pmatrix}
            \mat Q_{p-1} & \vect q
        \end{pmatrix}
        \begin{pmatrix}
            \mat R_{p-1} & \vect r \\
            \vect 0_{p-1}^\t & r
        \end{pmatrix}.
    \end{equation}
    Comparing the right-hand side of this equation with that of~\eqref{eq:qr_decomposition},
    we obtain that
    \begin{equation}
        \label{eq:qr_decomposition_intermediate}
        \vect x_p = \mat Q_{p-1} \vect r + \vect q r.
    \end{equation}
    Left-multiplying both sides by~$\mat Q_{p-1}^*$ and employing the orthogonality between~$\vect q$ and the columns of~$\mat Q_{p-1}$,
    we deduce that necessarily
    \(
        \vect r = \mat Q_{p-1}^* \vect x_p.
    \)
    It then follows from~\eqref{eq:qr_decomposition_intermediate} that
    \[
        r = \norm*{\vect x_p -  \mat Q_{p-1} \mat Q_{p-1}^* \vect x_p}, \qquad
        \vect q = \frac{1}{r} \left(\vect x_p - \mat Q_{p-1} \mat Q_{p-1}^* \vect x_p\right).
    \]
    It is simple to check that $\vect q$ is indeed orthogonal to the columns of~$\mat Q$,
    which concludes the proof.
\end{proof}
Note that the columns of the matrix~$\mat Q$ of the decomposition coincide with the vectors that
would be obtained by applying the Gram--Schmidt method to the columns of the matrix~$\mat X$.
In fact, the Gram--Schmidt process is one of several methods by which the $\mat Q \mat R$ decomposition can be calculated in practice.

\begin{algorithm}[ht!]
\caption{Simultaneous iteration}%
\label{algo:subspace_iteration}%
\begin{algorithmic}
\State $\mat X \gets \mat X_0$
\For{$k \in \{1, 2, \dotsc\}$}
    \State $\mat Q_k \mat R_k = \mat A \mat X_{k-1}$ (QR decomposition).
    \State $\mat X_k \gets \mat Q_k$.
\EndFor
\end{algorithmic}
\end{algorithm}
The simultaneous iteration method is presented in~\cref{algo:subspace_iteration}.
Like the normalization in the power iteration~\cref{algo:power_iteration},
the $\mat Q \mat R$ decomposition performed at each step in~\cref{algo:subspace_iteration} enables to avoid overflow errors.
Notice that when $p = 1$,
the simultaneous iteration reduces to the power iteration.
We emphasize that
the factorization step at each iteration does not influence the subspace spanned by the columns of~$\mat X$.
Therefore, this subspace after~$k$ iterations
coincides with that spanned by the columns of the matrix $\mat A^k \mat X_0$.
In fact, in exact arithmetic, it would be equivalent to perform the $\mat Q \mat R$ decomposition only once as a final step,
after the \julia{for} loop.
Indeed, denoting by $\mat Q_k \mat R_k$ the $\mat Q \mat R$ decomposition of $\mat A \mat X_{k-1}$,
we have
\begin{align*}
    &\mat X_k = \mat A \mat X_{k-1} \mat R_k^{-1} = \mat A^2 \mat X_{k-2} \mat R_{k-1}^{-1} \mat R_k^{-1} = \dotsb
    = \mat A^k \mat X_0 \mat R_1^{-1} \dotsc \mat R_k^{-1} \\
    &\Leftrightarrow \quad \mat X_k (\mat R_k \dotsc \mat R_1) = \mat A^k \mat X_0.
\end{align*}
Since $\mat X_k$ has orthonormal columns and $\mat R_k \dotsc \mat R_1$ is an upper triangular matrix (see~\cref{exercise:linear_product_of_lower_triangular})
with real positive elements on the diagonal (check this!),
it follows that~$\mat X_k$ can be obtained by~$\mat Q \mat R$ factorization of $\mat A^k \mat X_0$.
In order to show the convergence of the simultaneous iteration,
we begin by proving the following preparatory lemma.
\begin{lemma}
    [Continuity of the QR decomposition]
    \label{lemma:continuity_qr}
    If $\mat Q_k \mat R_k \to \mat Q \mat R$,
    where $\mat Q$ is orthogonal and~$\mat R$ is upper triangular with positive real entries on the diagonal,
    then $\mat Q_k \to \mat Q$.
\end{lemma}
\begin{proof}
    We reason by contradiction and assume
    there is $\varepsilon > 0$ and a subsequence $(\mat Q_{k_n})_{n\geq 0}$ such that $\norm{\mat Q_{k_n} - \mat Q} \geq \varepsilon$ for all~$n$.
    Since the set of unitary matrices is a compact subset of $\mat C^{n \times n}$,
    there exists a further subsequence $(\mat Q_{k_{n_m}})_{m\geq 0}$ that converges to a limit $\mat Q_{\infty}$
    which is also a unitary matrix and at least $\varepsilon$ away in norm from $\mat Q$.
    But then
    \[
        \mat R_{k_{n_m}}
        = \mat Q_{k_{n_m}}^{-1} (\mat Q_{k_{n_m}} \mat R_{k_{n_m}})
        = \mat Q_{k_{n_m}}^* ( \mat Q_{k_{n_m}} \mat R_{k_{n_m}})
        \xrightarrow[m \to \infty]{} \mat Q_{\infty}^* (\mat Q \mat R) =: \mat R_{\infty}.
    \]
    Since $\mat R_k$ is upper triangular with positive diagonal elements for all $k$,
    clearly $\mat R_{\infty}$ is also upper triangular with positive diagonal elements.
    But then $\mat Q_{\infty} \mat R_{\infty} = \mat Q \mat R$,
    and by uniqueness of the decomposition we deduce that $\mat Q = \mat Q_{\infty}$,
    which is a contradiction.
\end{proof}

Before presenting the convergence theorem,
we introduce the following terminology: we say that $\mat X_k \in \complex^{n \times p}$ converges essentially to a matrix $\mat X_{\infty}$
if each column of $\mat X_k$ converges essentially to the corresponding column of $\mat X_{\infty}$.
% We recall that a vector sequence $(\vect x_k)_{k \geq 1}$ converges essentially to $\vect x_{\infty}$ if there exists $(\theta_k)_{k \geq 1}$
% such that $(\e^{i \theta_k} \vect x_k)_{k \geq 1}$ converges to $\vect x_{\infty}$.
We prove the convergence in the Hermitian case for simplicity.
In the general case of~$\mat A \in \complex^{n \times n}$,
it cannot be expected that~$\mat X_k$ converges essentially to $\mat V$,
because the columns of $\mat X_k$ are orthogonal but eigenvectors may not be orthogonal.
In this case, the columns of $\mat X_k$ converge not to the eigenvectors
but to the so-called Schur vectors of~$\mat A$;
see~\cite{MR3396212} for more information.
\begin{theorem}
    [Convergence of the simultaneous iteration~\moreinfo]
    \label{theorem:convergence_subspace_iteration}
    Assume that $\mat A \in \complex^{n \times n}$ is Hermitian,
    that~$\mat X_0 \in \complex^{n \times p}$ has linearly independent columns,
    and finally that the subspace spanned by the column of~$\mat X_0$ satisfies
    \begin{equation}
        \label{eq:assumption_subspace_iteration}
        \col(\mat X_0) \cap \Span \{ \vect v_{p+1}, \dotsc, \vect v_n \} = \varnothing.
    \end{equation}
    If it holds that
    \begin{equation}
        \label{eq:order_eigenvalues}
        \lambda_1 > \lambda_2 > \dotsb > \lambda_p > \lambda_{p+1} \geq \lambda_{p+2} \geq \dotsc \geq \lambda_n,
    \end{equation}
    then $\mat X_k$ converges \emph{essentially} to $\mat V_1 := \begin{pmatrix} \vect v_1 & \hdots & \vect v_p \end{pmatrix}$.
\end{theorem}
\begin{proof}
    Let $\mat B = \mat V^{-1} \mat X_0 \in \complex^{n \times p}$, so that $\mat X_0 = \mat V \mat B$,
    and note that $\mat A^k \mat X_0 = \mat V \mat D^k \mat B$.
    We denote by~$\mat B_1 \in \complex^{p \times p}$ and $\mat B_2 \in \complex^{(n-p) \times p}$ the upper $p \times p$ and lower $(n-p) \times p$ blocks of $\mat B$,
    respectively.
    The matrix~$\mat B_1$ is nonsingular,
    otherwise the assumption~\eqref{eq:assumption_subspace_iteration} would not hold.
    Indeed, if there was a nonzero vector $\vect z \in \complex^p$ such that $\mat B_1 \vect z = 0$,
    then
    \[
        \mat X_0
        \vect z
        = \mat V
        \begin{pmatrix}
            \mat B_1 \\
            \mat B_2
        \end{pmatrix}
        \vect z
        =
        \begin{pmatrix}
            \mat V_1
            & \mat V_2
        \end{pmatrix}
        \begin{pmatrix}
            \vect 0 \\
            \mat B_2 \vect z
        \end{pmatrix}
        = \mat V_2 \mat B_2 \vect z.
    \]
    implying that $\mat X_0 \vect z \in \col(\mat X_0)$ is a linear combination of the vectors in
    \(
        \mat V_2 =
        \begin{pmatrix}
            \vect v_{p+1} & \hdots \vect v_n
        \end{pmatrix},
    \)
    which contradicts the assumption.
    We also denote by $\mat D_1$ and $\mat D_2$ the $p \times p$ upper-left and the~$(n-p) \times (n-p)$ lower-right blocks of $\mat D$, respectively.
    From the expression of $\mat A^k \mat X_0$,
    we have
    \begin{align}
        \notag
        \mat A^k \mat X_0
        &= \begin{pmatrix} \mat V_1 & \mat V_2 \end{pmatrix}
        \begin{pmatrix}
            \mat D_1^k & \\
                       & \mat D_2^k
        \end{pmatrix}
        \begin{pmatrix}
            \mat B_1 \\ \mat B_2
        \end{pmatrix}
        =
        \mat V_1 \mat D_1^k \mat B_1 + \mat V_2 \mat D_2^k \mat B_2, \\
        \label{eq:factorization}
        &= \left(\mat V_1  + \mat V_2 \mat D_2^{k} \mat B_2 \mat B_1^{-1}\mat D_1^{-k}\right) \mat D_1^k \mat B_1.
    \end{align}
    The second term in the bracket on the right-hand side converges to zero in the limit as $k \to \infty$ by~\eqref{eq:order_eigenvalues}.
    Let $\widetilde{\mat Q_k} \widetilde {\mat R_k}$~denote the reduced $\mat Q \mat R$ decomposition of the bracketed term.
    By~\cref{lemma:continuity_qr},
    which we proved for the standard $\mat Q \mat R$ decomposition but also holds for the reduced one,
    we deduce from $\widetilde{\mat Q_k} \widetilde{\mat R_k} \to \mat V_1$ that $\widetilde{\mat Q_k} \to \mat V_1$.
    Rearranging~\eqref{eq:factorization},
    we have
    \[
        \mat A^k \mat X_0 = \widetilde{\mat Q_k} (\widetilde{\mat R_k} \mat D_1^k \mat B_1).
    \]
    Since the matrix between brackets is a $p \times p$ square invertible matrix,
    this equation implies that $\col(\mat A^k \mat X_0) = \col (\widetilde{\mat Q_k})$.
    Denoting by $\mat Q_k \mat R_k$ the $\mat Q \mat R$ decomposition of~$\mat A_k \mat X_0$,
    we therefore have~$\col(\mat Q_k) = \col(\widetilde {\mat Q_k})$,
    and so the projectors on these subspaces are equal.
    We recall that, for a set of orthonormal vectors $\vect r_1, \dotsc, \vect r_p$
    gathered in a matrix $\mat R = \begin{pmatrix} \vect r_1 & \hdots & \vect r_p \end{pmatrix}$,
    the projector on $\col(\mat R) = \Span \{\vect r_1, \dotsc, \vect r_p \} \subset \complex^n$
    is the square $n \times n$ matrix
    \[
        \mat R \mat R^*
        = \vect r_1 \vect r_1^* + \dotsb + \vect r_p \vect r_p^*.
    \]
    Consequently,
    the equality of the projectors implies
    \(
        \mat Q_k \mat Q_k^* = \widetilde{\mat Q_k} \widetilde {\mat Q_k}^*.
    \)
    Now, we want to establish the essential convergence of $\mat Q_k$ to $\mat V_1$.
    To this end, we reason by induction,
    relying on the fact that the first $k$ columns of $\mat X_0$ undergo a simultaneous iteration independent of the other columns.
    For example, the first column simply undergoes a power iteration, and so it converges essentially to~$\vect v_1$.
    Assume now that the columns 1 to $p-1$ of $\mat Q_k$ converge essentially to $\vect v_1, \dotsc, \vect v_{p-1}$.
    Then the $p$-th column of $\mat Q_k$ at iteration $k$,
    which we denote by~$\vect q_p^{(k)}$,
    satisfies
    \begin{align*}
        \vect q_p^{(k)} {\vect q_p^{(k)}}^* &= \mat Q_k \mat Q_k^* - \vect q_1^{(k)} {\vect q_1^{(k)}}^* - \dotsb - \vect q_{p-1}^{(k)} {\vect q_{p-1}^{(k)}}^*
        = \widetilde{\mat Q}_k \widetilde{\mat Q}_k^* - \vect q_1^{(k)} {\vect q_1^{(k)}}^* - \dotsb - \vect q_{p-1}^{(k)} {\vect q_{p-1}^{(k)}}^* \\
        &\xrightarrow[k \to \infty]{} \mat V_1 \mat V_1^* - \vect v_1 \vect v_1^* - \dotsb - \vect v_{p-1} \vect v_{p-1}^* = \vect v_p \vect v_p^{*}.
    \end{align*}
    Therefore, noting that $\abs{a} = \sqrt{a \overline a}$ for every $a \in \complex$,
    we deduce
    \[
        \abs{\vect v_p^* \vect q_p^{(k)}} = \sqrt{\vect v_p^* \vect q_p^{(k)} {\vect q_p^{(k)}}^* \vect v_p} \xrightarrow[k \to \infty]{} \sqrt{\vect v_p^* \vect v_p \vect v_p^* \vect v_p} = 1,
    \]
    which shows that $\angle(\vect q_p^{(k)}, \vect v_p)$ converges to 0.
    Finally, observing that
    \[
        \norm*{\e^{- i \theta_k}\vect q_p^{(k)} - \vect v_p}^2 = 2 - 2 \abs{\vect v_p^* \vect q_p^{(k)}} \xrightarrow[k \to \infty]{} 0, \qquad
        \theta_k = \frac{\vect v_p^* \vect q_p^{(k)}}{\abs{\vect v_p^* \vect q_p^{(k)}}},
    \]
    we conclude that $\vect q_p^{(k)}$ converges essentially to $\vect v_p$.
\end{proof}

In addition to this convergence result,
it is possible to show that the error satisfies
\[
    \angle \Bigl(\col(\mat X_k), \col(\mat V_1)\Bigr)
    = \mathcal O \left( \abs*{\frac{\lambda_{p+1}}{\lambda_p}}^k \right).
\]
Here,
the angle between two subspaces $\mathcal A$ and $\mathcal B$ of $\complex^n$ is defined as
\[
    \angle (\mathcal A, \mathcal B) = \max_{\vect a \in \mathcal A \backslash \{\vect 0\}} \biggl( \min_{\vect b \in \mathcal B \backslash \{\vect 0\}} \angle(\vect a, \vect b) \biggr).
\]

\subsection{The $\mat Q \mat R$ algorithm}
The $\mat Q \mat R$ algorithm,
which is based on the $\mat Q \mat R$ decomposition,
is one of the most famous algorithms for calculating \emph{all} the eigenpairs of a matrix.
We first present the algorithm and then relate it to the simultaneous iteration in~\cref{sub:simultaneous_iteration}.
The method is presented in~\cref{algo:qr}.
\begin{algorithm}
\caption{QR algorithm}%
\label{algo:qr}%
\begin{algorithmic}
\State $\mat X_0 = \mat A$
\For{$i \in \{1, 2, \dotsc\}$}
    \State $\mat Q_k \mat R_k = \mat X_{k-1}$ (QR decomposition)
    \State $\mat X_{k} = \mat R_k \mat Q_k$
\EndFor
\end{algorithmic}
\end{algorithm}

Successive iterates of the QR algorithm are related by the equation
\begin{equation}
    \label{eq:qr_iteration_equation}
    \mat X_{k} = \mat Q_k^{-1}  \mat X_{k-1} \mat Q_k
    = \mat Q_k^* \mat X_{k-1} \mat Q_k
    = \dots = (\mat Q_1 \dots \mat Q_k)^* \mat X_{0} (\mat Q_1 \dots \mat Q_k)
\end{equation}
Therefore, all the iterates are related by a unitary similarity transformation,
and so they all have the same eigenvalues as $\mat X_0 = \mat A$.
Rearranging~\eqref{eq:qr_iteration_equation},
we have
\[
    (\mat Q_1 \dots \mat Q_k) \mat X_{k} = \mat A (\mat Q_1 \dots \mat Q_k),
\]
and so, introducing $\widetilde {\mat Q}_k = \mat Q_1 \dots \mat Q_k$
and noting that $\mat X_k = \mat Q_{k+1} \mat R_{k+1}$ by the algorithm,
we deduce
\[
    \widetilde {\mat Q}_{k+1} \mat R_{k+1} = \mat A \widetilde {\mat Q}_k.
\]
This reveals that
the matrix sequence $(\widetilde{\mat Q}_{k})_{k\geq 1}$ undergoes a simultaneous iteration and so,
assuming that $\mat A$ is Hermitian with $n$~distinct nonzero eigenvalues,
we deduce that $\widetilde{\mat Q}_k \to \mat V$ essentially in the limit as $k \to \infty$,
by~\cref{theorem:convergence_subspace_iteration}.
As a consequence, by~\eqref{eq:qr_iteration_equation},
it holds that~$\mat X_k \to \mat V^* \mat X_0 \mat V = \mat D$;
in other words, the matrix $\mat X_k$ converges to a diagonal matrix with the eigenvalues of~$\mat A$ on the diagonal.

\section{Projection methods}
\label{sec:projection_methods}
In this section,
we begin by presenting a general method for constructing an approximation of the eigenvectors of~$\mat A$ in a given subspace~$\mathcal U$ of~$\complex^{n}$.
We then discuss a particular choice for the subspace $\mathcal U$
as a Krylov subspace, which is very useful in practice.

Assume that $\{\vect u_1, \dotsc, \vect u_p\}$ is an orthonormal basis of $\mathcal U$.
Then for any vector $\vect v \in \complex^{n}$,
the vector of $\mathcal U$ that is closest to $\vect v$ in the Euclidean distance is given by the orthogonal projection
\[
    \mat P_{\mathcal U} \vect v := \mat U \mat U^* \vect v = (\vect u_1 \vect u_1^* + \dotsb + \vect u_p \vect u_p^*) \vect v.
\]
In practice,
the eigenvectors of~$\mat A$ are unknown,
and so it is impossible to calculate approximations using this formula.
The Rayleigh--Ritz method,
which we present hereafter,
is an alternative and practical method for constructing approximations of the eigenvectors and eigenvalues.
In general, the subspace $\mathcal U$ does not contain any eigenvector of $\mat A$,
and so the problem
\begin{equation}
    \label{eq:eigenproblem_subspace}
    \mat A \vect v = \lambda \vect v, \qquad \vect v \in \mathcal U
\end{equation}
does not admit a solution.
Let us denote by $\mat U$ the matrix with columns~$\vect u_1, \dotsc, \vect u_p$.
Since any vector $\vect v \in \mathcal U$ is equal to $\mat U \vect z$ for some vector $\vect z \in \complex^p$,
equation~\eqref{eq:eigenproblem_subspace} is equivalent to the problem
\[
    \mat A \mat U \vect z = \lambda \mat U \vect z,
\]
which is a system of $n$ equations with $p < n$ unknowns.
The Rayleigh--Ritz method is based on the idea that,
in order to obtain a problem with as many unknowns as there are equations,
we can multiply this equation by~$\mat U^*$,
which leads to the problem
\begin{equation}
    \label{eq:rayleigh_ritz}
    \mat B \vect z := (\mat U^* \mat A \mat U) \vect z = \lambda \vect z.
\end{equation}
This is standard eigenvalue problem for the matrix~$\mat U^* \mat A \mat U \in \complex^{p \times p}$,
which is much easier to solve than the original problem if $p \ll n$.
Equivalently, equation~\eqref{eq:rayleigh_ritz} may be formulated as follows: find $\vect v \in \mathcal U$
such that
\begin{equation}
    \label{eq:equivalent_rayleigh_ritz}
    \vect u^* (\mat A \vect v - \lambda \vect v), \qquad \forall \vect u \in \mathcal U.
\end{equation}
The solutions to~\eqref{eq:rayleigh_ritz} and~\eqref{eq:equivalent_rayleigh_ritz} are related by the equation~$\vect v = \mat U \vect z$.
Of course, the eigenvalues of~$\mat B$ in problem~\eqref{eq:rayleigh_ritz},
which are called the Ritz values of~$\mat A$ relative to $\mathcal U$,
are in general different from those of~$\mat A$.
Once an eigenvector $\vect y$ of~$\mat B$ has been calculated,
an approximate eigenvector of~$\mat A$,
called a \emph{Ritz vector} of~$\mat A$ relative to~$\mathcal U$,
is obtained from the equation $\widehat {\vect v} = \mat U \vect y$.
The Rayleigh--Ritz algorithm is presented in full in~\cref{algo:rayleigh_ritz}.
\begin{algorithm}
\caption{Rayleigh--Ritz}%
\label{algo:rayleigh_ritz}%
\begin{algorithmic}
\State Choose $\mathcal U \subset \complex^n$
\State Construct a matrix~$\mat U$ whose columns are orthonormal and span $\mathcal U$
\State Find the eigenvalues $\widehat \lambda_i$ and eigenvectors $\vect y_i \in \complex^p$ of $\mat B:= \mat U^* \mat A \mat U$
\State Calculate the corresponding Ritz vectors $\widehat {\vect v}_i = \mat U \vect y_i \in \complex^n$.
\end{algorithmic}
\end{algorithm}

It is clear that if $\vect v_i \in \mathcal U$,
then~$\lambda_i$ is an eigenvalue of $\mat B$ in~\eqref{eq:rayleigh_ritz}.
In fact,
we can show the following more general statement.
\begin{proposition}
    \label{proposition:invariant_subpsace_ritz}
    If $\mathcal U$ is an invariant subspace of~$\mat A$,
    meaning that $\mat A \mathcal U \subset \mathcal U$,
    then each Ritz vector of~$\mat A$ relative to~$\mathcal U$ is an eigenvector of~$\mat A$.
\end{proposition}
\begin{proof}
    Let $\mat U \in \complex^{n \times p}$ and $\mat W \in \complex^{n \times (n-p)}$ be matrices whose columns form orthonormal bases of~$\mathcal U$ and~$\mathcal U^\perp$,
    respectively.
    Here~$\mathcal U^\perp$ denotes the orthogonal complement of~$\mathcal U$ with respect to the Euclidean inner product.
    Then, since~$\mat W^* \mat A \mat U = \mat 0$ by assumption,
    it holds that
    \[
        \mat Q^* \mat A \mat Q =
        \begin{pmatrix}
            \mat U^* \mat A \mat U & \mat U^* \mat A \mat W \\
            \mat W^* \mat A \mat U & \mat W^* \mat A \mat W
        \end{pmatrix}
        =
        \begin{pmatrix}
            \mat U^* \mat A \mat U & \mat U^* \mat A \mat W \\
             \mat 0 & \mat W^* \mat A \mat W
        \end{pmatrix},
        \qquad
        \mat Q = \begin{pmatrix} \mat U & \mat W \end{pmatrix}.
    \]
    If $(\vect y, \widehat \lambda)$ is an eigenvector of $\mat U^* \mat A \mat U$,
    then
    \[
        \mat Q^* \mat A \mat Q
        \begin{pmatrix}
            \vect y \\
            \vect 0
        \end{pmatrix}
        =
        \begin{pmatrix}
            (\mat U^* \mat A \mat U) \vect y \\
            \vect 0
        \end{pmatrix}
        =
        \widehat \lambda
        \begin{pmatrix}
            \vect y \\
            \vect 0
        \end{pmatrix}
        =:
        \widehat \lambda \vect x,
    \]
    and so $(\vect x, \widehat \lambda)$ is an eigenpair of $\mat Q^* \mat A \mat Q$.
    But then $(\mat Q \vect x, \widehat \lambda) = (\mat U \vect y, \widehat \lambda)$ is an eigenpair of~$\mat A$,
    which proves the statement.
\end{proof}

If $\mathcal U$ is close to being an invariant subspace of~$\mat A$,
then it is expected that the Ritz vectors and Ritz values of~$\mat A$ relative to~$\mathcal U$ will provide good approximations of some of the eigenpairs of~$\mat A$.
Quantifying this approximation is difficult,
so we only present without proof the following error bound.
See~\cite{MR1990645} for more information.
\begin{proposition}
    Let~$\mat A$ be a full rank Hermitian matrix and $\mathcal U$ a $p$-dimensional subspace of~$\complex^n$.
    Then there exists eigenvalues $\lambda_{i_1}, \dotsc, \lambda_{i_p}$ of~$\mat A$ which satisfy
    \[
        \forall j \in \{1, \dotsc, p\}, \qquad
        \abs{\lambda_{i_j} - \widehat \lambda_j} \leq \norm{(\mat I - \mat P_{\mathcal U}) \mat A \mat P_{\mathcal U}}_2.
    \]
\end{proposition}

In the case where~$\mat A$ is Hermitian,
it is possible to show that the Ritz values are bounded from above by the eigenvalues of~$\mat A$.
The proof of this result relies on the Courant--Fisher theorem for characterizing the eigenvalues of a Hermitian matrix,
which is recalled in~\cref{theorem:courant-fisher} in the appendix.
\begin{proposition}
    If $\mat A \in \complex^{n \times n}$ is Hermitian,
    then
    \[
        \forall i \in \{1, \dotsc, p\}, \qquad
        \widehat \lambda_i \leq \lambda_i
    \]
\end{proposition}
\begin{proof}
By the Courant--Fisher theorem,
it holds that
\[
    \widehat \lambda_i
    = \max_{\mathcal S \subset \complex^p, {\rm dim}(\mathcal S) = i} \left( \min_{\vect x \in \mathcal S \backslash\{0\}} \frac{\vect x^* \mat B \vect x}{\vect x^* \vect x} \right)
\]
Letting $\vect y = \mat U \vect x$ and then $\mathcal R = \mat U \mathcal S$,
we deduce that
\begin{align*}
    \widehat \lambda_i
    &= \max_{\mathcal S \subset \complex^p, {\rm dim}(\mathcal S) = i} \left( \min_{\vect y \in \mat U \mathcal S \backslash\{0\}} \frac{\vect y^* \mat A \vect y}{\vect y^* \vect y} \right) \\
    &= \max_{\mathcal R \subset \mathcal U, {\rm dim}(\mathcal R) = i} \left( \min_{\vect y \in \mathcal R \backslash\{0\}} \frac{\vect y^* \mat A \vect y}{\vect y^* \vect y} \right)
    \leq \max_{\mathcal R \subset \complex^n, {\rm dim}(\mathcal R) = i} \left( \min_{\vect y \in \mathcal R \backslash\{0\}} \frac{\vect y^* \mat A \vect y}{\vect y^* \vect y} \right) = \lambda_i,
\end{align*}
where we used the Courant--Fisher for the matrix~$\mat A$ in the last equality.
\end{proof}

This projection approach is sometimes combined with a simultaneous subspace iteration:
an approximation $\mat X_k$ of the $p$ first eigenvector is first calculated using~\cref{algo:subspace_iteration},
and then the matrix $\mat X_k$ is used in place of~$\mat U$ in~\cref{algo:rayleigh_ritz}.

\subsection{Projection method in a Krylov subspace}
The power iteration constructs at iteration $k$ an approximation of $\vect v_1$ in the one-dimensional subspace spanned by the vector $\mat A^k \vect x_0$,
and only the previous iteration $\vect x_{k}$ is employed to construct $\vect x^{k+1}$.
One may wonder whether,
by employing all the previous iterates rather than only the previous one,
a better approximation of~$\vect v_1$ can be constructed.
More precisely, instead of looking for an approximation in the subspace $\Span \{\mat A^k \vect x_0\}$,
would it be useful to extend the search area to the Krylov subspace
\[
    \mathcal K_{k+1}(\mat A, \vect x_0) := \Span \bigl\{ \vect x_0, \mat A \vect x_0, \dotsc, \mat A^k \vect x_0 \bigr\}?
\]
The answer to this question is positive,
and the resulting method is often much faster than the power iteration.
This is achieved by employing the Rayleigh--Ritz projection method~\cref{algo:rayleigh_ritz} with the choice $\mathcal U = \mathcal K_{k+1}(\mat A, \vect x_0)$.
Applying this method requires to calculate an orthonormal basis of the Krylov subspace and to calculate the reduced matrix $\mat U^* \mat A \mat U$.
The \emph{Arnoldi method} enables to achieve these two goals simultaneously.

\subsection{The Arnoldi iteration}
This Arnoldi iteration is based on the Gram--Schmidt process and presented in~\cref{algo:arnoldi_method}.
\begin{algorithm}
\caption{Arnoldi iteration for constructing an orthonormal basis of $\mathcal K_p(\mat A, \vect u_1)$}%
\label{algo:arnoldi_method}%
\begin{algorithmic}
\State Choose $\vect u_1$ with unit norm.
\For{$j \in \{1, \dotsc p\}$}
    \State $\vect u_{j+1} \gets \mat A \vect u_{j}$
    \For{$i \in \{1, \dotsc, j\}$}
        \State $h_{i,j} \gets \vect u_{i}^* \vect u_{j+1}$
        \State $\vect u_{j+1} \gets \vect u_{j+1} - h_{i,j} \vect u_{i}$
    \EndFor
    \State $h_{j+1,j} \gets \norm{\vect u_{j+1}}$
    \State $\vect u_{j+1} \gets \vect u_{j+1} / h_{j+1,j}$
\EndFor
\end{algorithmic}
\end{algorithm}
The iteration breaks down if $h_{j+1,j} = 0$,
which indicates that $\mathcal A \vect u_j$ belongs to the Krylov subspace $\Span \{ \vect u_1, \dotsc, \vect u_j \} = \mathcal K_j(\mat A, \vect u_1)$,
implying that $\mathcal K_{j+1}(\mat A, \vect u_1) = \mathcal K_j(\mat A, \vect u_1)$.
In this case, the subspace $\mathcal K_j(\mat A, \vect u_1)$ is an invariant subspace of~$\mat A$ because, by~\cref{exercise:krylov},
we have
\[
    \mat A \mathcal K_j(\mat A, \vect u_1)
    \subset \mathcal K_{j+1}(\mat A, \vect u_1)
    = \mathcal K_j(\mat A, \vect u_1).
\]
Therefore, applying the Rayleigh--Ritz with $\mathcal U = \Span \{ \vect u_1, \dotsc, \vect u_j \}$ yields exact eigenpairs~\cref{proposition:invariant_subpsace_ritz}.
If the iteration does not break down then,
by construction,
the vectors $\{\vect u_1, \dotsc, \vect u_p\}$ at the end of the algorithm are orthonormal.
It is also simple to show by induction that they form a basis of~$\mathcal K_p(\mat A, \vect u_1)$.
The scalar coefficients $h_{i,j}$ can be collected in a matrix square $p \times p$ matrix
\[
    \mat H
    =
    \begin{pmatrix}
        h_{1,1} & h_{1,2} & h_{1,3} & \cdots  & h_{1,p} \\
        h_{2,1} & h_{2,2} & h_{2,3} & \cdots  & h_{2,p} \\
        0       & h_{3,2} & h_{3,3} & \cdots  & h_{3,p} \\
        \vdots  & \ddots  & \ddots  & \ddots  & \vdots  \\
        0       & \cdots  & 0     & h_{p,p-1} & h_{p,p}
    \end{pmatrix}.
\]
% All the coefficients are contained in this matrix, except for $h_{p+1,p}$.
This matrix contains only zeros under the first subdiagonal;
such a matrix is called a~\emph{Hessenberg} matrix.
Inspecting the algorithm,
we notice that the $j$-th column contains the coefficients of the projection of the vector~$\mat A \vect u_j$ onto the basis $\{\vect u_1, \dotsc, \vect u_p\}$.
In other words,
\begin{align}
    \label{eq:reduced_matrix_arnoldi}
    \mat U^* \mat A \mat U = \mat H,
\end{align}
We have thus shown that the Arnoldi algorithm enables to construct both an orthonormal basis of a Krylov subspace and
the associated reduced matrix.
In fact, we have the following equation
\begin{equation}
    \label{eq:arnoldi_equation}
    \mat A \mat U = \mat U \mat H + h_{p+1,p} (\vect v_{p+1} \vect e_p^*),
    \qquad \vect e_p = \begin{pmatrix} 0 \\ \vdots \\ 1 \end{pmatrix} \in \complex^p.
\end{equation}
The Arnoldi algorithm, coupled with the Rayleigh--Ritz method,
has very good convergence properties in the limit as $p \to \infty$,
in particular for eigenvalues with a large modulus.
The following result shows that the residual $\vect r = \mat A \widehat{\vect v} - \widehat \lambda \vect v$
associated with a Ritz vector can be estimated inexpensively.
Specifically, the norm of the residual is equal to the last component of the associated eigenvector of~$\mat H$ multiplied by $h_{p+1,p}$.
\begin{proposition}
    [Formula for the residual~\moreinfo]
    Let $\vect y_i$ be an eigenvector of $\mat H$ associated with the eigenvalues~$\widehat \lambda_i$,
    and let $\widehat {\vect v}_i = \mat U \vect y_i$ denote the corresponding eigenvector.
    Then
    \[
        \mat A \widehat {\vect v}_i - \widehat \lambda \vect v_i = h_{p+1,p} (\vect y_i)_p \vect v_{p+1}.
    \]
    Consequently, it holds that
    \[
        \norm{\mat A \widehat {\vect v}_i - \widehat \lambda \vect v_i} = \abs{h_{p+1,p} (\vect y_i)_p}.
    \]
\end{proposition}
\begin{proof}
    Multiplying both sides of~\eqref{eq:arnoldi_equation} by $\vect y_i$,
    we obtain
    \[
        \mat A \mat U \vect y_i = \mat U \mat H \vect y_i + h_{p+1,p} (\vect v_{p+1} \vect e_p^*) \vect y_i.
    \]
    Using the definition of $\widehat {\vect v}_i$ and rearranging the equation,
    we have
    \[
        \mat A \widehat {\vect v_i} - \widehat \lambda_i \vect y_i  = h_{p+1,p} (\vect v_{p+1} \vect e_p^*) \vect y_i,
    \]
    which immediately gives the result.
\end{proof}

In practice, the larger the dimension~$p$ of the subspace~$\mathcal U$ employed in the Rayleigh--Ritz method,
the more memory is required for storing an orthonormal basis of~$\mathcal U$.
In addition, for large values of~$p$,
computing the reduced matrix~\eqref{eq:reduced_matrix_arnoldi} and its eigenpairs becomes computationally expensive;
the computational cost of computing the matrix~$\mat H$ scales as $\mathcal O(p^2)$.
To remedy these potential issues,
the algorithm can be restarted periodically.
For example, \cref{algo:arnoldi_power} can be employed as an alternative to the power iteration
in order to find the eigenvector associated with the eigenvalue with largest modulus.

\begin{algorithm}
\caption{Restarted Arnoldi iteration}%
\label{algo:arnoldi_power}%
\begin{algorithmic}
\State Choose $\vect u_1 \in \complex^n$ and $p \ll n$
\For{$i \in \{1, 2, \dotsc\}$}
    \State Perform $p$ iterations of the Arnoldi iteration and construct~$\mathcal U$;
    \State Calculate the Ritz vector $\widehat {\vect v}_1$ associated with the largest Ritz value relative to~$\mathcal U$;
    \State If this vector is sufficiently accurate, then stop. Otherwise, restart with $\vect u_1 = \widehat {\vect v}_1$.
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{The Lanczos iteration}
The Lanczos iteration may be viewed as a simplified version of the Arnoldi iteration in the case where the matrix~$\mat A$ is Hermitian.
Let us denote by $\{ \vect u_1, \dotsc, \vect u_p \}$ the orthonormal vectors generated by the Arnoldi iteration.
When~$\mat A$ is Hermitian, it holds that
\[
    h_{i,j} = \vect u_{i}^* (\mat A \vect u_{j}) = (\mat A \vect u_i)^* \vect u_j = \overline{h_{j,i}}.
\]
Therefore, the matrix $\mat H$ is Hermitian.
This is not surprising, since we showed that $\mat H = \mat U^* \mat A \mat U$ and the matrix $\mat A$ is Hermitian.
Since~$\mat H$ is also of Hessenberg form,
we deduce that~$\mat H$ is tridiagonal.
An inspection of~\cref{algo:arnoldi_method} shows that the subdiagonal entries of~$\mat H$ are real.
Since $\mat A$ is Hermitian, the diagonal entries $h_{i,i} = \vect u_{i}^* (\mat A \vect u_{j})$ are also real,
and so we conclude that all the entries of the matrix~$\mat H$ are in fact real.
This matrix if of the form
\[
    \mat H
    =
    \begin{pmatrix}
        \alpha_1 & \beta_2 \\
        \beta_2 & \alpha_2 & \beta_3 \\
            & \beta_3 & \ddots & \ddots \\
            & & \ddots & \ddots & \beta_p \\
            & & & \beta_p & \alpha_p
    \end{pmatrix}
\]
Adapting the Arnoldi iteration to this setting leads to~\cref{algo:lanczos_method}.
\begin{algorithm}
\caption{Lanczos iteration for constructing an orthonormal basis of $\mathcal K_p(\mat A, \vect u_1)$}%
\label{algo:lanczos_method}%
\begin{algorithmic}
\State Choose $\vect u_1$ with unit norm.
\State $\beta_1 \gets 0$, $\vect u_0 \gets \vect 0 \in \complex^n$
\For{$j \in \{1, \dotsc p\}$}
    \State $\vect u_{j+1} \gets \mat A \vect u_{j} - \beta_j \vect u_{j-1}$
    \State $\alpha_j \gets \vect u_{j}^* \vect u_{j+1}$
    \State $\vect u_{j+1} \gets \vect u_{j+1} - \alpha_j \vect u_{j}$
    \State $\beta_{j+1} \gets \norm{\vect u_{j+1}}$
    \State $\vect u_{j+1} \gets \vect u_{j+1} / \beta_{j+1}$
\EndFor
\end{algorithmic}
\end{algorithm}

\section{Exercises}
\begin{exercise}
    PageRank is an algorithm for assigning a rank to the vertices of a directed graph.
    It is used by many search engines, notably Google,
    for sorting search results.
    In this context,
    the directed graph encodes the links between pages of the World Wide Web:
    the vertices of the directed graph are webpages,
    and there is an edge going from page~$i$ to page~$j$ if page~$i$ contains a hyperlink to page~$j$.

    Let us consider a directed graph $G(V, E)$ with vertices $V = \{1, \dotsc, n\}$ and edges $E$.
    The graph can be represented by its adjacency matrix $\mat A \in \{0, 1\}^{n \times n}$,
    whose entries are given by
    \[
        a_{ij} =
        \begin{cases}
            1 & \text{if there is an edge from $i$ to $j$,} \\
            0 & \text{otherwise.}
        \end{cases}
    \]
    Let $r_i$ denote the ``value'' assigned to vertex $i$.
    The idea of PageRank, in its simplest form,
    is to assign values to the vertices
    by solving the following system of equations;
    \begin{equation}
        \label{eq:equation_page_rank}
        \forall i \in V, \qquad
        r_i
        = \sum_{j \in \mathcal N(i)} \frac{r_j}{o_j}.
        % = \sum_{j=1}^{n} \frac{a_{ji}}{o_j} r_j
    \end{equation}
    where $o_j$ is the outdegree of vertex $j$,
    i.e.\ the number of edges leaving from~$j$.
    Here the sum is over the set of nodes $\mathcal N(i)$, which denotes all the ``incoming'' neighbors of $i$,
    i.e.\ those that have an edge pointing towards node $i$.
    \begin{itemize}
        \item
            Read the Wikipedia page on \emph{PageRank} to familiarize yourself with the algorithm.

        \item
            Let~$\vect r = \begin{pmatrix} r_1 & \hdots & r_n \end{pmatrix}^\t$.
            Show using~\eqref{eq:equation_page_rank} that $\vect r$ satisfies
            \[
                \vect r =
                \mat A^\t
                \begin{pmatrix}
                    \frac{1}{o_1} & &  \\
                                  & \ddots & \\
                                  & & \frac{1}{o_n}
                \end{pmatrix}
                \vect r =:  \mat A^\t \mat O^{-1} \vect r.
            \]
            In other words, $\vect r$ is an eigenvector with eigenvalue $1$ of the matrix $\mat M = \mat A^\t \mat O^{-1}$.

        \item
            Show that $\mat M$ is a left-stochastic matrix,
            i.e.\ that each column sums to 1.

        \item
            Prove that the eigenvalues of any matrix $\mat B \in \real^{n \times n}$ coincide with those of $\mat B^\t$.
            You may use the fact that $\det(\mat B) = \det(\mat B^\t)$.

        \item
            Using the previous items,
            show that 1 is an eigenvalue and that $\rho(\mat M) = 1$.
            For the second part, find a subordinate matrix norm such that $\norm{\mat M}= 1$.

        \item
            Implement PageRank  in order to rank pages from a 2013 snapshot of English Wikipedia.
            You can use either the simplified version of the algorithm given in~\eqref{eq:equation_page_rank} or the improved version with a damping factor described on Wikipedia.
            In the former case,
            the following are both sensible stopping criteria:
            \[
                \frac{\norm{\mat M \widehat{\vect r} - \widehat{\vect r}}_1}{\norm{\widehat{\vect r}}_1} < 10^{-15}
                \qquad \text{ or } \qquad
                \frac{\norm{\mat M \widehat{\vect r} - \widehat \lambda \widehat{\vect r}}_1}{\norm{\vect r}_1} < 10^{-15},
                \qquad \widehat \lambda = \frac{\widehat{\vect r}^\t \mat M \widehat{\vect r}}{\widehat{\vect r}^\t \widehat{\vect r}},
            \]
            where $\widehat{\vect v}$ is an approximation of the eigenvector corresponding to the dominant eigenvalue.
            A dataset is available on the course website to complete this part.
            This dataset contains a subset of the data publicly available \hyperlink{https://snap.stanford.edu/data/enwiki-2013.html}{here},
            and was generated from the full dataset by retaining only the 5\% best rated articles.
            After decompressing the archive,
            you can load the dataset into Julia by using the following commands:

            \begin{minted}{julia}
    import CSV
    import DataFrames

    # Data (nodes and edges)
    nodes = CSV.read("names.csv", DataFrames.DataFrame)
    edges = CSV.read("edges.csv", DataFrames.DataFrame)

    # Convert data to matrices
    nodes = Matrix(nodes)
    edges = Matrix(edges)
            \end{minted}

            After you have assigned a rank to all the pages,
            print the 10 pages with the highest ranks.
            My code returns the following entries:
            \begin{multicols}{3}
                \begin{enumerate}
                    \item United States
                    \item United Kingdom
                    \item World War~II
                    \item Latin
                    \item France
                    \item Germany
                    \item English language
                    \item China
                    \item Canada
                    \item India
                \end{enumerate}
            \end{multicols}
        \item
            \textbf{Extra credit:}
            Write a function \julia{search(keyword)} that can be employed for searching the database.
            Here is an example of what it could return:

    \begin{minted}{julia}
    julia> search("New York")
    481-element Vector{String}:
     "New York City"
     "New York"
     "The New York Times"
     "New York Stock Exchange"
     "New York University"
     
    \end{minted}
    \end{itemize}
\end{exercise}

\begin{exercise}
    \label{exercise:krylov}
    Show the following properties of the Krylov subspace $\mathcal K_p(\mat A, \vect x)$.
    \begin{itemize}
        \item
            \(
                \mathcal K_p(\mat A, \vect x) \subset \mathcal K_{p+1}(\mat A, \vect x).
            \)

        \item
            \(
                \mat A \mathcal K_p(\mat A, \vect x) \subset \mathcal K_{p+1}(\mat A, \vect x).
            \)

        \item
            The Krylov subspace $\mathcal K_p(\mat A, \vect x)$ is invariant under rescaling:
            for all $\alpha \in \complex$,
            \[
                \mathcal K_p(\mat A, \vect x) = \mathcal K_p(\alpha \mat A, \vect x) = \mathcal K_p(\mat A, \alpha \vect x).
            \]

        \item
            The Krylov subspace $\mathcal K_p(\mat A, \vect x)$ is invariant under shift of the matrix~$\mat A$:
            for all $\alpha \in \complex$,
            \[
                \mathcal K_p(\mat A, \vect x) = \mathcal K_p(\mat A - \alpha \mat I, \vect x).
            \]

        \item
            Similarity transformation: If $\mat T \in \complex^{n \times n}$ is nonsingular,
            then
            \[
                \mathcal K_p(\mat T^{-1} \mat A \mat T, \mat T^{-1} \vect x) =
                \mat T^{-1} \mathcal K_p(\mat A, \vect x).
            \]
    \end{itemize}
\end{exercise}

\begin{exercise}
    The minimal polynomial of a matrix $\mat A \in \complex^{n \times n}$ is the monic polynomial~$p$ of lowest degree such that $p(\mat A) = 0$.
    Prove that, if~$\mat A$ is Hermitian with $m \leq n$ distinct eigenvalues,
    then the minimal polynomial is given by
    \[
        p(t) = \prod_{i=1}^m (t - \lambda_i).
    \]
\end{exercise}

\begin{exercise}
    The minimal polynomial for a general matrix $\mat A \in \complex^{n \times n}$ is given by
    \[
        p(t) = \prod_{i=1}^m (t - \lambda_i)^{s_i}.
    \]
    where $s_i$ is the size of the largest Jordan block associated with the eigenvalue~$\lambda_i$ in the normal Jordan form of~$\mat A$.
    Verify that $p(\mat A) = 0$.
\end{exercise}

\begin{exercise}
    Let $d$ denote the degree of the minimal polynomial of~$\mat A$.
    Show that
    \[
        \forall p \geq d, \qquad
        \mathcal K_{p+1}(\mat A, \vect x) =
        \mathcal K_{p}(\mat A, \vect x).
    \]
    Deduce that, for $p \geq n$,
    the subspace $\mathcal K_{p}(\mat A, \vect x)$ is an invariant subspace of~$\mat A$.
\end{exercise}

\begin{exercise}
    Let $\mat A \in \complex^{n \times n}$.
    Show that $\mathcal K_{n}(\mat A, \vect x)$ is the smallest invariant subspace of~$\mat A$ that contains $\vect x$.
\end{exercise}

\begin{compexercise}
    Consider the matrix
    \[
        \mat M =
        \begin{pmatrix}
            0 & 1 & 2 & 0 \\
            1 & 0 & 1 & 0 \\
            2 & 1 & 0 & 2 \\
            0 & 0 & 2 & 0
        \end{pmatrix}
    \]
    \begin{itemize}
        \item Find the dominant eigenvalue of~$\mat M$ by using the power iteration.
        \item Find the eigenvalue of~$\mat M$ closest to 1 by using the inverse iteration.
        \item Find the other two eigenvalues of~$\mat M$ by using a method of your choice.
    \end{itemize}
\end{compexercise}

\begin{exercise}
    [A posteriori error bound]
    \label{exercise:a_posteriori_1}
    Assume that $\mat A \in \complex^{n \times n}$ is Hermitian,
    and that $\widehat {\vect v}$ is a normalized approximation of an eigenvector which satisfies
    \[
        \norm{\widehat {\vect z}} :=  \norm{\mat A \widehat {\vect v} - \widehat \lambda \widehat {\vect v}} = \delta,
        \qquad \widehat \lambda = \frac{\widehat {\vect v}^* \mat A \widehat {\vect v}}{\widehat {\vect v}^* \widehat {\vect v}}.
    \]
    Prove that there is an eigenvalue~$\lambda$ of~$\mat A$ such that
    \[
        \abs{\widehat \lambda - \lambda} \leq \delta.
    \]

    \noindent \textbf{Hint:} Consider first the case where $\mat A$ is diagonal.
\end{exercise}

\begin{exercise}
    [Bauer--Fike theorem]
    \label{exercise:a_posteriori_2}
    Assume that $\mat A \in \complex^{n \times n}$ is diagonalizable: $\mat A \mat V = \mat V \mat D$.
    Show that, if $\widehat {\vect v}$ is a normalized approximation of an eigenvector which satisfies
    \[
        \norm{\vect r} :=  \norm{\mat A \widehat {\vect v} - \widehat \lambda \widehat {\vect v}} = \delta
    \]
    for some $\widehat \lambda \in \complex$,
    then there is an eigenvalue~$\lambda$ of~$\mat A$ such that
    \[
        \abs{\widehat \lambda - \lambda} \leq \kappa_2(\mat V) \delta.
    \]

    \noindent{\textbf{Hint}:}
    Rewrite
    \[
        \norm{\widehat {\vect v}} = \norm{(\mat A - \widehat \lambda \mat I)^{-1}  {\vect r}}
        = \norm{\mat V (\mat D - \widehat \lambda \mat I)^{-1}  \mat V^{-1} {\vect r})}.
    \]
\end{exercise}

\begin{exercise}
    In \cref{exercise:a_posteriori_1} and \cref{exercise:a_posteriori_2},
    we saw examples~\emph{a posteriori} error estimates
    which guarantee the existence of an eigenvalue of~$\mat A$ within a certain distance of the approximation~$\widehat \lambda$.
    In this exercise,
    our aim is to provide an answer to the following different question:
    given an approximate eigenpair $(\widehat {\vect v}, \widehat \lambda)$,
    what is the smallest perturbation~$\mat E$ that we need to apply to $\mat A$ in order to guarantee that
    $(\widehat {\vect v}, \widehat \lambda)$ is an exact eigenpair, i.e.\ that
    \[
        (\mat A + \mat E) \widehat {\vect v} = \widehat \lambda \widehat {\vect v}?
    \]
    Assume that $\widehat {\vect v}$ is normalized and
    let $\mathcal E = \Bigl\{\mat E \in \complex^{n \times n}: (\mat A + \mat E) \widehat {\vect v} = \widehat \lambda \widehat {\vect v} \Bigr\}$.
    Prove that
    \begin{equation}
        \label{eq:kahan_parlett_jiang}
        \min_{\mat E \in \mathcal E} \norm{\mat E}_2 = \norm{\vect r}_2 := \norm{\mat A \widehat {\vect v} - \widehat \lambda \widehat {\vect v}}.
    \end{equation}
    To this end,
    you may proceed as follows:
    \begin{itemize}
        \item
            Show first that any $\mat E \in \mathcal E$ satisfies $\mat E \widehat {\vect v} = - \vect r$.

        \item
            Deduce from the first item that
            \[
                \inf_{\mat E \in \mathcal E} \norm{\mat E}_2 \geq \norm{\vect r}_2.
            \]

        \item
            Find a rank one matrix $\mat E_*$ such that
            \(
                \norm{\mat E_*}_2 = \norm{\vect r}_2,
            \)
            and then conclude.
            Recall that any rank 1 matrix can be written in the form $\mat E_* = \vect u \vect w^*$,
            with norm $\norm{\vect u}_2 \norm{\vect w}_2$.
    \end{itemize}
    Equation~\eqref{eq:kahan_parlett_jiang} is a simplified version of the Kahan--Parlett--Jiang theorem
    and is an example of a \emph{backward error estimate}.
    Whereas forward error estimates quantify the distance between an approximation and the exact solution,
    backward error estimates give the size of the perturbation that must be applied to a problem so that an approximation is exact.
\end{exercise}

\begin{exercise}
    \label{exercise:essential_convergence}
    Assume that $(\vect x_k)_{k \geq 0}$ is a sequence of normalized vectors in $\complex^n$.
    Show that the following statements are equivalent:
    \begin{itemize}
        \item $(\vect x_k)_{k \geq 0}$ converges essentially to $\vect x_{\infty}$ in the limit as $k \to \infty$.
        \item The angle $\angle(\vect x_k, \vect x_{\infty})$ converges to zero in the limit as $k \to \infty$.
        \item The projector $\mat P_{\vect x_k}$ converges to $\mat P_{y}$ in the limit as $k \to \infty$.
    \end{itemize}
\end{exercise}

\begin{exercise}
    Assume that $\mat A \in \complex^{n \times n}$ is skew-Hermitian.
    Derive a Lanczos-like algorithm for constructing an orthonormal basis of $\mathcal K_p(\mat A, \vect x)$
    and calculating the reduced matrix
    \[
        \mat U^* \mat A \mat U,
    \]
    where $\mat U \in \complex^{n \times p}$ contains the vectors of the basis as columns.
    Implement your algorithm with~$p = 20$ in order to approximate the dominant eigenvalue of the matrix~$\mat A$ constructed by the following piece of code:
    \begin{minted}{julia}
    n = 5000
    A = rand(n, n) + im * randn(n, n)
    A = A - A'  # A is now skew-Hermitian
    \end{minted}
\end{exercise}

\begin{exercise}
    Assume that $\{\vect u_1, \dotsc, \vect u_p \}$ and $\{\vect w_1, \dotsc, \vect w_n\}$
    are orthonormal bases of the same subspace $\mathcal U \subset \complex^n$.
    Show that the projectors $\mat U \mat U^*$ and $\mat W \mat W^*$ are equal.
\end{exercise}

\begin{exercise}
    Assume that $\mat A \in \complex^{n \times n}$ is a Hermitian matrix with distinct eigenvalues,
    and suppose that we know the dominant eigenpair $(\vect v_1, \lambda_1)$,
    with $\vect v_1$ normalized.
    Let
    \[
        \mat B = \mat A - \lambda_1 \vect v_1 \vect v_1^*.
    \]
    If we apply the power iteration to this matrix,
    what convergence can we expect?
\end{exercise}

\begin{exercise}
    Assume that $\widehat{\vect v_1}$ and $\widehat{\vect v_2}$ are two Ritz vectors of a Hermitian matrix~$\mat A$ relative to a subspace~$\mathcal U \subset \complex^n$.
    Show that, if the associated Ritz values are distinct,
    then $\widehat{\vect v_1}^* \widehat{\vect v_2} = 0$.
\end{exercise}

\section{Discussion and bibliography}

The content of this chapter is inspired mainly from~\cite{VanDooren} and also from~\cite{MR3396212}.
The latter volume contains a detailed coverage of the standard methods for eigenvalue problems.
Some of the exercises are taken from~\cite{Vuik},
and others are inspired from~\cite{MR3396212}.
