\chapter{Numerical computation of eigenvalues}%
\label{cha:numerical_computation_of_eigenvalues}

Calculating the eigenvalues and eigenvectors of a matrix is a task often encountered in scientific and engineering applications.
Eigenvalue problems naturally arise in quantum physics,
solid mechanics, structural engineering and molecular dynamics,
to name just a few applications.
The aim of this chapter is to present an overview of the standard methods for calculating eigenvalues and eigenvectors numerically.
We focus predominantly on the case of a Hermitian matrix $\mat A \in \complex^{n \times n}$,
which is technically simpler and arises in many applications.
The reader is invited to go through the background material in~\cref{sec:diagonalization} before reading this chapter.
The rest of this chapter is organized as follows
\begin{itemize}
    \item
        In \cref{sec:general_remarks},
        we make general remarks concerning concerning the calculation of eigenvalues.
\end{itemize}

\section{Numerical methods for eigenvalue problems: general remarks}
\label{sec:general_remarks}

As mentioned in~\cref{sec:diagonalization},
a complex number $\lambda \in \complex$ is an eigenvalue of $\mat A \in \complex^{n \times n}$
if and only if $\lambda$ is a root of the characteristic polynomial of $\mat A$:
\[
    p_A
    \colon \complex \to \complex
    \colon \lambda \mapsto \det (\mat A - \lambda \mat I).
\]
One may,
therefore,
calculate the eigenvalues of $\mat A$ by calculating the roots of the polynomial~$p_A$ using,
for example, one of the methods presented in~\cref{cha:solution_of_nonlinear_systems}.
While feasible for small matrices,
this approach is not viable for large matrices,
because the number of floating point operations required for calculating calculating the coefficients of the characteristic polynomial scales as~$n!$.

In view of the prohibitive computational cost required for calculating the characteristic polynomial,
other methods are required for solving large eigenvalue problems numerically.
All the methods that we study in this chapter are iterative in nature.
While some of them are aimed at calculating all the eigenpairs of the matrix~$\mat A$,
other methods enable to calculate only a small number of eigenpairs at a lower computational cost,
which is often desirable.
Indeed, calculating all the eigenvalues of a large matrix is computationally expensive;
on a personal computer, the following Julia code takes well over a second to terminate:
\begin{minted}{julia}
    import LinearAlgebra
    LinearAlgebra.eigen(rand(2000, 2000))
\end{minted}

In many applications,
the matrix $\mat A$ is sparse,
and in this case it is important that algorithms for eigenvalue problems do not destroy the sparsity structure.
Note that the eigenvectors of a sparse matrix are generally not sparse.

To conclude this section,
we introduce some notation used throughout this chapter.
For a diagonalizable matrix $\mat A$,
we denote the eigenvalues by $\lambda_1, \dotsc, \lambda_n$,
with $\abs{\lambda_1} \geq \abs{\lambda_2} \geq \dotsc \geq \abs{\lambda_n}$.
The associated eigenvectors are denoted by $\vect v_1, \dotsc, \vect v_n$.
Therefore, it holds that
\[
    \mat V^{-1} \mat A \mat V = \diag(\lambda_1, \dotsc, \lambda_n),
    \qquad \text{where } \mat V = \begin{pmatrix} \vect v_1 & \hdots & \vect v_n \end{pmatrix}.
\]

\section{Simple vector iterations}
\label{sec:simple_vector_iterations}

In this section,
we present simple iterative methods aimed at calculating just one eigenvector of the matrix~$\mat A$,
which we assume to be diagonalizable for simplicity.

\subsection{The power iteration}
The power iteration is the simplest method for calculating the eigenpair associated with the eigenvalue of~$\mat A$ with largest modulus.
Since the eigenvectors of $\mat A$ span $\complex^n$,
any vector $\vect x_0$ may be decomposed as
\begin{align}
    \label{eq:eigen_decomposition_x0}
    \vect x_0 = \alpha_1 \vect v_1 + \dotsb + \alpha_n \vect v_n.
\end{align}
The idea of the power iteration is to repeatedly left-multiply this vector by the matrix $\mat A$,
in order to amplify the coefficient of $\vect v_0$ relative to the other ones.
Indeed, notice that
\[
    \mat A^k \vect x_0 = \lambda_1^k \alpha_1 \vect v_1 + \dotsb + \lambda_n^k \alpha_n \vect v_n.
\]
If $\lambda_1$ is strictly larger in modulus than the other eigenvalues,
and if $\alpha_1 \neq 0$,
then for large $k$ the vector $\mat A^k \vect x_0$ is approximately aligned,
in a sense made precise below,
with the eigenvector~$\vect v_1$.
In order to avoid overflow errors at the numerical level,
the iterates are normalized at each iteration.
The power iteration is presented in~\cref{algo:power_iteration}.
\begin{algorithm}
\caption{Power iteration}%
\label{algo:power_iteration}%
\begin{algorithmic}
\State $\vect x \gets \vect x_0$
\For{$i \in \{1, 2, \dotsc\}$}
    \State $\vect x \gets \mat A \vect x$
    \State $\vect x \gets \vect x / \norm{\vect x}$
\EndFor
\end{algorithmic}
\end{algorithm}

To precisely quantify the convergence of the power method,
we introduce the notion of \emph{angle} between vectors of $\complex^n$.
\begin{align*}
    \angle(\vect x, \vect y)
    &= \arccos\left( \frac{\abs{\vect x^* \vect y}}{\sqrt{\vect x^* \vect x} \sqrt{\vect y^* \vect y}}\right) \\
    &= \arcsin\left( \frac{\norm{(\mat I - \mat P_{\vect y}) \vect x}} {\norm{\vect x}} \right),
    \qquad \mat P_{\vect y} := \frac{\vect y \vect y^*}{\vect y^* \vect y}.
\end{align*}
This definition generalizes the familiar notion of angle for vectors in $\real^2$ or $\real^3$.
We can then prove the following convergence result.
\begin{proposition}
    [Convergence of the power iteration]
    \label{proposition:convergence_of_the_power_iteration}
    Suppose that $\mat A$ is diagonalizable and that $\abs{\lambda_1} > \abs{\lambda_2}$.
    Then, for every initial guess with $\alpha_1 \neq 0$,
    the sequence $(\vect x_k)_{k \geq 0}$ generated by the power iteration satisfies
    \[
        \lim_{k \to \infty} \angle(\vect x_k, \vect v_1) = 0.
    \]
\end{proposition}
\begin{proof}
    By construction,
    it holds that
    \begin{equation}
        \label{eq:power_iteration_iterate}
        \vect x_k
        = \frac{\lambda_1^k \alpha_1 \vect v_1 + \dotsb + \lambda_n^k \alpha_n \vect v_n}{\norm{\lambda_1^k \alpha_1 \vect v_1 + \dotsb + \lambda_n^k \alpha_n \vect v_n}}
        = \e^{i \theta_k} \frac{\vect v_1 + \frac{\lambda_2^k \alpha_2}{\lambda_1^k \alpha_1} \vect v_2 +  \dotsb + \frac{\lambda_n^k \alpha_2}{\lambda_n^k \alpha_1} \vect v_n }{\norm*{\vect v_1 + \frac{\lambda_2^k \alpha_2}{\lambda_1^k \alpha_1} \vect v_2 +  \dotsb + \frac{\lambda_n^k \alpha_n}{\lambda_1^k \alpha_1} \vect v_n}},
        \qquad \e^{i \theta_k} := \frac{\lambda_1^k \alpha_1}{\abs{\lambda_1^k \alpha_1}}.
    \end{equation}
    Clearly, it holds hat $\vect x_k \to \e^{i \theta_k} \vect v_1/ \norm{\vect v_1}$.
    Using the definition of the angle between two vectors in $\complex^n$,
    and the continuity of the $\complex^n$ Euclidean inner product and of the $\arccos$ function,
    we calculate
    \[
        \angle(\vect x_k, \vect v_1)
        = \arccos\left( \frac{\abs{\vect x_k^* \vect v_1}}{\sqrt{\vect x_k^* \vect x_k} \sqrt{\vect v_1^* \vect v_1}}\right)
        = \arccos\left( \frac{\abs{\vect x_k^* \vect v_1}}{\sqrt{\vect v_1^* \vect v_1}} \right)
        \xrightarrow[k \to \infty]{} \arccos(1) = 0,
    \]
    which concludes the proof.
\end{proof}
An inspection of the proof shows that the dominant term in the error,
asymptotically in the limit as $k \to \infty$,
is the one with coefficient $\frac{\lambda_2^k \alpha_2}{\lambda_1^k \alpha_1}$.
Therefore, we deduce that
\[
    \angle(\vect x_k, \vect v_1) = \mathcal O \left( \abs*{\frac{\lambda_2}{\lambda_1}}^k \right).
\]
The convergence is slow if $\abs{\lambda_2/\lambda_1}$ is close to one,
and fast if $\abs{\lambda_2} \ll \abs{\lambda_1}$.
Once an approximation of the eigenvector $\vect v_1$ has been calculated,
the corresponding eigenvalue $\lambda_1$ can be estimated from the \emph{Rayleigh quotient:}
\[
    \rho_{\mat A}\colon \mat \complex^n_* \to \complex\colon \vect x \mapsto \frac{\vect x^* \mat A \vect x}{\vect x^* \vect x}.
\]
For any eigenvector~$\vect v$ of $\mat A$,
the corresponding eigenvalue is equal to~$\rho_{\mat A}(\vect v)$.
In order to study the error on the eigenvalue~$\lambda_1$ for the power iteration,
we assume for simplicity that $\mat A$ is Hermitian
and that the eigenvectors $\vect v_1, \dotsc, \vect v_n$ are orthonormal.
Substituting in~\eqref{eq:power_iteration_iterate},
we obtain
\[
    \rho_{\mat A}(\vect x_k)
    = \frac{\lambda_1 + \abs*{ \frac{\lambda_2^k \alpha_2}{\lambda_1^k \alpha_1} }^2 \lambda_2 +  \dotsb + \abs*{ \frac{\lambda_n^k \alpha_n}{\lambda_1^k \alpha_1} }^2 \lambda_n}
    {1 + \abs*{ \frac{\lambda_2^k \alpha_2}{\lambda_1^k \alpha_1} }^2 +  \dotsb +  \abs*{ \frac{\lambda_n^k \alpha_n}{\lambda_1^k \alpha_1} }^2}.
\]
Therefore,
we deduce
\[
    \abs{\rho_{\mat A}(\vect x_k) - \lambda_1}
    \leq  \abs*{ \frac{\lambda_2^k \alpha_2}{\lambda_1^k \alpha_1} }^2 \abs{\lambda_2 - \lambda_1} +  \dotsb + \abs*{ \frac{\lambda_n^k \alpha_n}{\lambda_1^k \alpha_1} }^2 \abs{\lambda_n- \lambda_1}
    = \mathcal O\left(\abs*{\frac{\lambda_2}{\lambda_1}}^{2k}\right).
\]

\subsection{Inverse iteration}
The power iteration is simple but enables to calculate only the dominant eigenvalue of the matrix~$\mat A$,
i.e.\ the eigenvalue of largest modulus.
Additionally, its convergence is slow when $\abs{\lambda_2} \approx \abs{\lambda_1}$.
The inverse iteration enables a more efficient calculation of not only the dominant eigenvalue
but also the other eigenvalues of~$\mat A$.
It is based on applying the power iteration to the matrix $(\mat A - \mu \mat I)^{-1}$,
where $\mu \in \complex$ is a shift.
The eigenvalues of the matrix $(\mat A - \mu \mat I)^{-1}$ are given by $(\lambda_1 - \mu)^{-1}, \dotsc, (\lambda_n - \mu)^{-1}$,
with associated eigenvectors $\vect v_1, \dotsc, \vect v_n$.
If $\abs{\lambda_i - \mu} < \abs{\lambda_j - \mu}$ for all $j \neq i$,
then the dominant eigenvalue of the matrix $(\mat A - \mu \mat I)^{-1}$ is $(\lambda_i - \mu)^{-1}$,
and so the power iteration applied to this matrix produces an approximation of the eigenvector $\vect v_i$.
In other words, by choosing the shift~$\mu$ sufficiently close to $\lambda_i$,
an approximation of~$\vect v_i$ may be obtained.
The inverse iteration is presented in~\cref{algo:inverse_iteration}.
In practice, the inverse $(\mat A - \mu \mat I)^{-1}$ need not be calculated,
and it is often preferable to solve a linear system at each iteration.
\begin{algorithm}
\caption{Inverse iteration}%
\label{algo:inverse_iteration}%
\begin{algorithmic}
\State $\vect x \gets \vect x_0$
\For{$i \in \{1, 2, \dotsc\}$}
    \State Solve $(\mat A - \mu \mat I) \vect y = \vect x$
    \State $\vect x \gets \vect y / \norm{\vect y}$
\EndFor
\State $\lambda \gets \vect x^* \mat A \vect x / \vect x^* \vect x$
\State \Return $\vect x, \lambda$
\end{algorithmic}
\end{algorithm}

An application of~\cref{proposition:convergence_of_the_power_iteration} immediately gives the following convergence result for the inverse iteration.
\begin{proposition}
    [Convergence of the inverse iteration]
    Assume that $\mat A \in \complex^n$ is diagonalizable
    and that there exist $J$ and $K$ such that
    \[
        0 < \abs{\lambda_J - \mu} < \abs{\lambda K - \mu} \leq \abs{\lambda_i - \mu} \qquad \forall j \neq J.
    \]
    Assume also that $\alpha_J \neq 0$,
    where $\alpha_J$ is the coefficient of $\vect x_J$ in the expansion of $\vect x_0$ given in~\eqref{eq:eigen_decomposition_x0}.
    Then the iterates of the inverse iteration method satisfy
    \[
        \lim_{k \to \infty} \angle(\vect x_k, \vect v_J) = 0.
    \]
    More precisely,
    \[
        \angle(\vect x_k, \vect v_J) = \mathcal O \left( \abs*{\frac{\lambda_J - \mu}{\lambda_K - \mu}}^k \right).
    \]
\end{proposition}
Notice that the closer $\lambda_J$ is to the targeted eigenvalue,
the faster the inverse iteration converges.

\subsection{Rayleigh quotient iteration}
Since the inverse iteration is fast when $\mu$ is close to an eigenvalue $\lambda_i$,
it is natural to wonder whether fast convergence can be achieved by progressively updating $\mu$ as the simulation progresses.
Specifically, the current approximation of the eigenvalue may be employed in place of~$\mu$.
This leads to the iteration of the Rayleigh quotient,
presented in
\begin{algorithm}
\caption{Inverse iteration}%
\label{algo:rayleigh_quotient}%
\begin{algorithmic}
\State $\vect x \gets \vect x_0$
\For{$i \in \{1, 2, \dotsc\}$}
    \State $\mu \gets \vect x^* \mat A \vect x / \vect x^* \vect x$
    \State Solve $(\mat A - \mu \mat I) \vect y = \vect x$
    \State $\vect x \gets \vect y / \norm{\vect y}$
\EndFor
\State $\lambda \gets \vect x^* \mat A \vect x / \vect x^* \vect x$
\State \Return $\vect x, \lambda$
\end{algorithmic}
\end{algorithm}

It is possible to show, see~\cite{MR3396212} and the references therein,
that, when~$\mat A$ is Hermitian,
the Rayleigh quotient iteration converges to an eigenvector for almost every initial guess~$\vect x_0$.
Furthermore, if convergence to an eigenvector occurs,
then $\mu$ converges cubically to the corresponding eigenvalue.

\section{Subspace iteration}
The subspace iteration resembles the power iteration but it is more general:
not just one but several vectors are updated at each iteration.
Let $\mat X_0 = \begin{pmatrix} \vect x_1 & \dotsc & \vect x_p \end{pmatrix}$ denote an initial set of linearly independent vectors.
The subspace iteration is presented in~\cref{algo:subspace_iteration}.
\begin{algorithm}
\caption{Subspace iteration}%
\label{algo:subspace_iteration}%
\begin{algorithmic}
\State $\mat X \gets \mat X_0$
\For{$i \in \{1, 2, \dotsc\}$}
    \State $\mat X \gets \mat A \mat X$
    \State $\mat X \gets \mat X \mat Z_i$
\EndFor
\end{algorithmic}
\end{algorithm}
In this algorithm, the matrix $\mat Z_i \in \complex^{p \times p}$ is a full rank matrix that plays the role of a normalization;
it ensures that the columns of $\mat X$ are orthonormal at the end of each iteration.
More precisely, the right-multiplication with the matrix~$\mat Z$ performs a Gram--Schmidt orthonormalization,
which is strongly related to the $\mat Q \mat R$ decomposition.
We recall that the Gram--Schmidt process enables to incrementally construct,
starting form an ordered set of $p$ vectors $\{\vect x_1, \dotsc, \vect x_p \}$ in $\complex^n$,
a new set of vectors $\{\vect q_1, \dotsc, \vect q_p \}$
which are \emph{orthonormal} and span the same subset of $\complex^n$ as the original vectors.
It is simple to see,
by inspection of the Gram--Schmidt process,
that each of the original vectors may be expressed as a linear combination of the new vectors
\begin{equation}
    \label{eq:gram_schmidt}
    \vect x_i = \alpha^i_{1} \vect q_{1} + \dotsb + \alpha^i_{i} \vect q_i.
\end{equation}
Gathering the original and new vectors in matrices $\mat X = \begin{pmatrix} \vect x_1 & \hdots & \vect x_p \end{pmatrix}$
and $\mat Q = \begin{pmatrix} \vect q_1 & \hdots & \vect q_p \end{pmatrix}$,
and rewriting~\eqref{eq:gram_schmidt} in matrix form,
we obtain
\[
    \mat X = \mat Q
    \begin{pmatrix}
        \alpha^1_1 & \alpha^2_1 & \hdots & \alpha^p_1 \\
                   & \alpha^2_2 & \hdots & \alpha^p_2 \\
                   &            & \ddots & \vdots \\
                   & & & \alpha^p_p
        \alpha
    \end{pmatrix}.
\]
The rightmost matrix is square and upper triangular,
and this decomposition of the matrix $\mat X$ is known as a \emph{reduced $\mat Q \mat R$ decomposition}
or simply \emph{$\mat Q \mat R$ decomposition} if $p = n$,
in which case $\mat X$ is a square matrix and $\mat Q$ is a unitary matrix.
This decomposition is unique if we require that the diagonal coefficients of~$\mat R$ are real and positive.
In practice, the matrix $\mat Z$ in~\eqref{algo:subspace_iteration} is not explicitly calculated.
Instead, the reduced $\mat Q \mat R$ decomposition of $\mat X$ is computed and we set $\mat X \leftarrow \mat Q$.

Observe that the normalization step does not influence the subspace spanned by the columns of $\mat X$.
Therefore, this subspace after~$k$ iterations
coincides with that spanned by the columns of the matrix $\mat A^k \mat X_0$.
The aim of normalizing the vectors at each step is to avoid overflow errors but,
mathematically, it would be equivalent to perform the Gram--Schmidt process only once as a final step,
after the \julia{for} loop.
In order to show the convergence of the subspace iteration,
we begin by proving the following preparatory lemma.
\begin{lemma}
    [Continuity of the QR decomposition]
    \label{lemma:continuity_qr}
    Assume that $\mat Q_k \mat R_k \to \mat Q \mat R$,
    where $\mat Q$ is orthogonal and $\mat R$ is upper triangular with positive real entries on the diagonal,
    then $\mat Q_k \to \mat Q$.
\end{lemma}
\begin{proof}
    We reason by contradiction and assume
    there is $\varepsilon > 0$ and a subsequence $(\mat Q_{k_n})_{n\geq 0}$ such that $\norm{\mat Q_{k_n} - \mat Q} \geq \varepsilon$.
    Since the set of unitary matrices form a compact subset of $\mat C^{n \times n}$,
    there exists a further subsequence $(\mat Q_{k_{n_m}})_{m\geq 0}$ that converges to a limit $\mat Q_{\infty}$
    which is also a unitary matrix and at least $\varepsilon$ away in norm from $\mat Q$.
    But then
    \[
        \mat R_{k_{n_m}}
        = \mat Q_{k_{n_m}}^{-1} (\mat Q_{k_{n_m}} \mat R_{k_{n_m}})
        = \mat Q_{k_{n_m}}^* ( \mat Q_{k_{n_m}} \mat R_{k_{n_m}})
        \xrightarrow[m \to \infty]{} \mat Q_{\infty}^* \mat Q \mat R =: \mat R_{\infty}.
    \]
    Since $\mat R_k$ is upper triangular with positive diagonal elements for all $k$,
    clearly $\mat R_{\infty}$ is also upper triangular with positive diagonal elements.
    But then $\mat Q_{\infty} \mat R_{\infty} = \mat Q \mat R$,
    and by uniqueness of the decomposition we deduce that $\mat Q = \mat Q_{\infty}$,
    which is a contradiction.
\end{proof}

\begin{theorem}
    [Convergence of the subspace iteration]
    \label{theorem:convergence_subspace_iteration}
    Assume that $\mat A = \mat V \mat D \mat V^{-1}$ is diagonalizable and that
    \begin{equation}
        \label{eq:assumption_subspace_iteration}
        \col(\mat X_0) \cap \Span \{ \vect v_{p+1}, \dotsc, \vect v_n \} = \varnothing.
    \end{equation}
    Assume also that
    \begin{equation}
        \label{eq:order_eigenvalues}
        \lambda_1 > \lambda_2 > \dotsb > \lambda_p > \lambda_{p+1} \geq \lambda_{p+2} \geq \dotsc \geq \lambda_n.
    \end{equation}
    Then $\mat X_k$ converges essentially to $\mat V_1 := \begin{pmatrix} \vect v_1 & \hdots & \vect v_p \end{pmatrix}$.
\end{theorem}
\begin{proof}
    Let $\mat B = \mat V^{-1} \mat X_0 \in \complex^{n \times p}$, so that $\mat X_0 = \mat V \mat B$,
    and note that $\mat A^k \mat X_0 = \mat V \mat D^k \mat B$.
    We denote by~$\mat B_1 \in \complex^{p \times p}$ and $\mat B_2 \in \complex^{(n-p) \times p}$ the upper $p \times p$ and lower $(n-p) \times p$ blocks of $\mat B$,
    respectively.
    The matrix~$\mat B_1$ is nonsingular,
    otherwise the assumption~\eqref{eq:assumption_subspace_iteration} would not hold.
    Indeed, if there was a nonzero vector $\vect z \in \complex^p$ such that $\mat B_1 \vect z = 0$,
    then
    \[
        \mat X_0
        \vect z
        = \mat V
        \begin{pmatrix}
            \mat B_1 \\
            \mat B_2
        \end{pmatrix}
        \vect z
        =
        \begin{pmatrix}
            \mat V_1
            & \mat V_2
        \end{pmatrix}
        \begin{pmatrix}
            \vect 0 \\
            \mat B_2 \vect z
        \end{pmatrix}
        = \mat V_2 \mat B_2 \vect z.
    \]
    implying that $\mat X_0 \vect z$ is a linear combination of the vectors in $\mat V_2 =
    \begin{pmatrix}
        \vect v_{p+1} & \hdots \vect v_n,
    \end{pmatrix}$
    which contradicts the assumption.
    From the expression of $\mat A^k \mat X_0$,
    we have
    \begin{align}
        \notag
        \mat A^k \mat X_0
        &= \begin{pmatrix} \mat V_1 & \mat V_2 \end{pmatrix}
        \begin{pmatrix}
            \mat D_1^k & \\
                       & \mat D_2^k
        \end{pmatrix}
        \begin{pmatrix}
            \mat B_1 \\ \mat B_2
        \end{pmatrix}
        =
        \mat V_1 \mat D_1^k \mat B_1 + \mat V_2 \mat D_2^k \mat B_2, \\
        \label{eq:factorization}
        &= \left(\mat V_1  + \mat V_2 \mat D_2^{k} \mat B_2 \mat B_1^{-1}\mat D_1^{-k}\right) \mat D_1^k \mat B_1.
    \end{align}
    Notice that the second term in the bracket on the right-hand side converges to zero in the limit as $k \to \infty$ by~\eqref{eq:order_eigenvalues}.
    Let $\widetilde{\mat Q_k} \widetilde {\mat R_k}$~denote the reduced $\mat Q \mat R$ decomposition of the bracketed term.
    By~\cref{lemma:continuity_qr},
    which we proved for the standard $\mat Q \mat R$ decomposition but also holds for the reduced one,
    we deduce from $\widetilde{\mat Q_k} \widetilde{\mat R_k} \to \mat V_1$ that $\widetilde{\mat Q_k} \to \mat V_1$.
    Rearranging~\eqref{eq:factorization},
    we have
    \[
        \mat A^k \mat X_0 = \widetilde{\mat Q_k} (\widetilde{\mat R_k} \mat D_1^k \mat B_1).
    \]
    Since the matrix between brackets is a $p \times p$ square invertible matrix,
    this equation implies that $\col(\mat A^k \mat X_0) = \col (\widetilde{\mat Q_k})$.
    Denoting by $\mat Q_k \mat R_k$ the $\mat Q \mat R$ decomposition of~$\mat A_k \mat X_0$,
    we therefore have~$\col(\mat Q_k) = \col(\widehat {\mat Q_k})$,
    and so the projectors on these subspaces are equal.
    We recall that, for a set of orthonormal vectors $\vect r_1, \dotsc, \vect r_p$,
    gathered in a matrix $\mat R = \begin{pmatrix} \vect r_1 & \hdots & \vect r_p \end{pmatrix}$,
    the projector on $\col(\mat R) = \Span \{\vect r_1, \dotsc, \vect r_p \} \subset \complex^n$
    is the square $n \times n$ matrix
    \[
        \mat R \mat R^*
        = \vect r_1 \vect r_1^* + \dotsb + \vect r_p \vect r_p^*.
    \]
    Consequently,
    the equality of the projectors implies
    \(
        \mat Q_k \mat Q_k^* = \widehat{\mat Q_k} \widehat {\mat Q_k}^*.
    \)
    Now, we want to establish the convergence of $\mat Q_k$ to $\mat V_1$.
    To this end, we reason by induction,
    relying on the fact that the first $k$ columns of $\mat X_0$ undergo a subspace iteration independent of the later columns.
    For example, the first column simply undergoes a power iteration, and so it converges to~$\vect v_1$.
    Assume now that the columns 1 to $p-1$ of $\mat Q_k$ converge to $\vect v_1, \dotsc, \vect v_{p-1}$.
    Then the $p$-th column $\vect q_p$ at iteration $k$ obeys
    \[
        \vect q_p^{(k)} \vect {q_p^{(k)}}^* = \mat Q_k \mat Q_k^* - \vect q_1^{(k)} {\vect q_1^{(k)}}^* - \dotsb - \vect q_{p-1}^{(k)} \vect {q_{p-1}^{(k)}}^*
        = \mat Q_k \mat Q_k^* - \vect q_1 \vect q_1^* - \dotsb - \vect q_{p-1} \vect q_{p-1}^*
        \xrightarrow[k \to \infty]{} \vect v_p \vect v_p^{*}.
    \]
\end{proof}

\section{Exercises}

\begin{exercise}
    PageRank is an algorithm for assigning a rank to the vertices in a directed graph.
    It is used by many search engines, notably Google,
    for sorting search results.
    In this context,
    the directed graph encodes the links between pages of the World Wide Web:
    the vertices of the directed graph are webpages,
    and there is an edge going from page~$i$ to page~$j$ if page~$i$ contains a hyperlink to page~$j$.

    Let us consider a directed graph $G(V, E)$ with vertices $V = \{1, \dotsc, n\}$ and edges $E$.
    The graph can be represented by its adjacency matrix $\mat A \in \{0, 1\}^{n \times n}$,
    whose entries are given by
    \[
        a_{ij} =
        \begin{cases}
            1 & \text{if there is an edge from $i$ to $j$,} \\
            0 & \text{otherwise.}
        \end{cases}
    \]
    Let $r_i$ denote the ``value'' assigned to vertex $i$.
    The idea of PageRank in its simplest form is to assign values to the vertices
    by solving the following system of equations;
    \begin{equation}
        \label{eq:equation_page_rank}
        \forall i \in V, \qquad
        r_i
        = \sum_{j \in \text{neighbors}} \frac{r_j}{o_j}.
        % = \sum_{j=1}^{n} \frac{a_{ji}}{o_j} r_j
    \end{equation}
    where $o_j$ is the outdegree of vertex $j$,
    i.e.\ the number of edges leaving from~$j$,
    and the sum is over all the neighbors of $i$.
    \begin{itemize}
        \item
            Read the Wikipedia page on \emph{PageRank} to familiarize yourself with the algorithm.

        \item
            Let~$\vect r = \begin{pmatrix} r_1 & \hdots & r_n \end{pmatrix}^\t$.
            Show using~\eqref{eq:equation_page_rank} that $\vect r$ satisfies
            \[
                \vect r =
                \mat A^\t
                \begin{pmatrix}
                    \frac{1}{o_1} & & & \\
                                  & \ddots & \\
                                  & & \frac{1}{o_n}
                \end{pmatrix}
                \vect r =:  \mat A^\t \mat O^{-1} \vect r.
            \]
            In other words, $\vect r$ is an eigenvector with eigenvalue $1$ of the matrix $\mat A^\t \mat O^{-1}$.

        \item
            Show that $\mat O^{-1} \mat A$ is a left-stochastic matrix,
            i.e.\ that each column sums to 1.

        \item
            Prove that the eigenvalues of any matrix $\mat M \in \real^{n \times n}$ coincide with those of $\mat M^\t$.
            You may use the fact that $\det(\mat M) = \det(\mat M^\t)$.

        \item
            Show that $\rho(\mat A^\t \mat O^{-1}) = 1$ and that the eigenvalue with largest modulus is 1.
            For the first part, find a subordinate matrix norm such that $\norm{\mat O^{-1} \mat A} = 1$.

        \item
            Implement PageRank in order to rank pages from a 2013 snapshot of English Wikipedia.
            A dataset is available on the course website to this end.
            This dataset contains a subset of the data publicly available \hyperlink{https://snap.stanford.edu/data/enwiki-2013.html}{here},
            and was generated from the full dataset by retaining only the 5\% best rated articles.
            After decompressing the archive,
            you can load the dataset in Julia by using the following commands:

            \begin{minted}{julia}
    import CSV
    import DataFrames

    # Data (nodes and edges)
    nodes = CSV.read("names.csv", DataFrames.DataFrame)
    edges = CSV.read("edges.csv", DataFrames.DataFrame)

    # Convert data to matrices
    nodes = Matrix(nodes)
    edges = Matrix(edges)
            \end{minted}

            After you have assigned a rank to all the pages,
            print the 10 pages with the highest ranks.
            My code returns the following entries: ``United States", ``United Kingdom", ``World War~II", ``Latin", ``France", ``Germany", ``English language", ``China", ``Canada", ``India".

        \item
            \textbf{Extra credit:}
            Write a function \julia{search(keyword)} that can be employed for searching the database.
            Here is an example of what it could return:

    \begin{minted}{julia}
    julia> search("New York")
    481-element Vector{String}:
     "New York City"
     "New York"
     "The New York Times"
     "New York Stock Exchange"
     "New York University"
     â€¦
    \end{minted}
    \end{itemize}
\end{exercise}
