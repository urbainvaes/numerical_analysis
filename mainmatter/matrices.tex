\setlength{\OuterFrameSep}{0pt}

\chapter{Background material}%
\label{cha:vectors_and_matrices}
\minitoc

In this chapter,
we collect basic results that are useful for this course.

\section{Inner products and norms}%
\label{sec:inner_product_and_norm}

We begin by recalling the definitions of the fundamental concepts of \emph{norm} and \emph{inner product}.
For generality,
we consider the case of a \emph{complex} vector space,
i.e.\ a vector space for which the scalar field is $\complex$.
\begin{definition}
A norm on a complex vector space $\mathcal X$ is a function $\norm{\placeholder}: \mathcal X \to \real$ satisfying the following axioms:
\begin{itemize}
    \item
        \textbf{Positivity}:
        \(
            \forall \vect x \in \mathcal X \backslash \{\vect 0\}, \quad
            \norm{\vect x} > 0.
        \)

    \item
        \textbf{Homogeneity}:
        \(
            \forall (c, \vect x) \in \complex \times \mathcal X, \quad
            \norm{c \vect x} = \abs{c} \, \norm{\vect x} .
        \)

    \item
        \textbf{Triangular inequality}:
        \(
            \forall (\vect x, \vect y) \in \mathcal X \times \mathcal X, \quad
            \norm{\vect x + \vect y} \leq \norm{\vect x} + \norm{\vect y}.
        \)
\end{itemize}
\end{definition}
For example,
the Euclidean norm on~$\complex^n$ is given by
\[
    \norm{\vect x} = \sqrt{x_1 + \dotsc + x_n}.
\]

\begin{definition}
    An inner product on a \emph{complex} vector space $\mathcal X$ is a function
    \[
        \ip{\placeholder,\placeholder}: \mathcal X \times \mathcal X \to \complex
    \]
    satisfying the following axioms:
    \begin{itemize}
        \item
            \textbf{Conjugate symmetry}:
            Here $\overline \placeholder$ denotes the complex conjugate.
            \[
                \forall (\vect x, \vect y) \in \mathcal X \times \mathcal X, \qquad
                \ip{\vect x, \vect y} = \overline{\ip{\vect y, \vect x}}.
            \]
        \item
            \textbf{Linearity}:
            For all $(\alpha, \beta) \in \complex^2$ and all $(\vect x, \vect y, \vect z) \in \mathcal X^3$,
            it holds that
            \[
                \ip{\alpha \vect x + \beta \vect y, \vect z}
                = \alpha \ip{\vect x, \vect z} + \beta \ip{\vect y, \vect z}.
            \]

        \item
            \textbf{Positive-definiteness}:
            \[
                \forall \vect x \in \mathcal X \backslash \{0\}, \qquad
                \ip{\vect x, \vect x} > 0.
            \]
    \end{itemize}
\end{definition}
For example, the familiar Euclidean inner product on $\complex^n$ is given by
\[
    \ip{\vect x, \vect y} := \sum_{i=1}^{n} x_i \overline y_i.
\]
A vector space with an inner product is called an \emph{inner product space}.
Any inner product on~$\mathcal X$ induces a norm via the formula
\begin{equation}
    \label{eq:induced_norm}%
    \norm{\vect x} = \sqrt{\ip{\vect x, \vect x}}.
\end{equation}
The Cauchy--Schwarz inequality enables to bound inner products using norms.
It is also useful for showing that the functional defined in~\eqref{eq:induced_norm} satisfies the triangle inequality,
which is the goal of \cref{exercise:triangle_inequality}.
\begin{proposition}
    [Cauchy--Schwarz inequality]
    \label{proposition:cauchy_shwartz}
    Let $\mathcal X$ be an inner product space.
    Then
    \begin{equation}
        \label{eq:cauchy_shwartz}
        \forall  (\vect x, \vect y) \in \mathcal X \times \mathcal X, \qquad
        \abs[big]{\ip{\vect x, \vect y}} \leq \norm{\vect x} \norm{\vect y}.
    \end{equation}
\end{proposition}
\begin{proof}
    The statement is obvious if $\vect y = \vect 0$,
    so we assume in the rest of the proof that $\vect y \neq \vect 0$.
    Let us define $p\colon \real \ni t \mapsto \norm{\vect x + t \vect y}^2$.
    Using the bilinearity of the inner product,
    we have
    \[
        p(t) = \norm{\vect x}^2 + 2 t \ip{\vect x, \vect y} + t^2 \norm{\vect y}^2.
    \]
    This shows that $p$ is a convex second-order polynomial with a minimum at $t_* = - \ip{\vect x, \vect y} / \norm{\vect y}^2$.
    Substituting this value in the expression of $p$,
    we obtain
    \[
        p(t_*) = \norm{\vect x}^2 - 2 \frac{\abs[big]{\ip{\vect x, \vect y}}^2}{\norm{\vect y}^2} +  \frac{\abs[big]{\ip{\vect x, \vect y}}^2}{\norm{\vect y}^2}
        = \norm{\vect x}^2 - \frac{\abs[big]{\ip{\vect x, \vect y}}^2}{\norm{\vect y}^2}.
    \]
    Since $p(t_*) \geq 0$ by definition of $p$,
    we obtain~\eqref{eq:cauchy_shwartz}.
\end{proof}

Several norms can be defined on the same vector space $\mathcal X$.
Two norms $\norm{\placeholder}_{\alpha}$ and $\norm{\placeholder}_{\beta}$ on $\mathcal X$ are said to be equivalent if
there exist positive real numbers~$c_{\ell}$ and $c_u$ such that
\begin{equation}
    \label{eq:norm_equilavence}
    \forall \vect x \in \mathcal X,
    \qquad c_{\ell} \norm{\vect x}_{\alpha}
    \leq \norm{\vect x}_{\beta}
    \leq c_u \norm{\vect x}_{\alpha}.
\end{equation}
As the terminology indicates, norm equivalence is an \emph{equivalence relation}.
When working with norms on finite-dimensional vector spaces,
it is important to keep in mind the following result.
The proof is provided for information purposes only.
\begin{proposition}
    Assume that $\mathcal X$ is a finite-dimensional vector space.
    Then all the norms defined on~$\mathcal X$ are pairwise equivalent.
\end{proposition}
\begin{proof}
    Let $(\vect e_1, \dotsc, \vect e_n)$ be a basis of $\mathcal X$.
    Any $\vect x \in \mathcal X$ admits a unique representation in this basis as $\vect x = \lambda_1 \vect e_1 + \dotsb + \lambda_n \vect e_n$.
    We will show that any norm $\norm{\placeholder}$ on~$\mathcal X$ is equivalent to the norm $\norm{\placeholder}_*$ given by
    \begin{equation}
        \label{eq:norm_star}
        \norm{\vect x}_* = \abs{\lambda_1} + \dotsc + \abs{\lambda_n}.
    \end{equation}
    By the triangle inequality,
    it holds that
    \begin{align}
        \nonumber
        \norm{\vect x} \leq \abs{\lambda_1} \norm{\vect e_1} + \dotsb + \abs{\lambda_n} \norm{\vect e_n}
        &\leq \Bigl(\abs{\lambda_1} + \dotsb + \abs{\lambda_n}\Bigr) \,
        \max \Bigl\{\norm{\vect e_1}, \dotsc, \norm{\vect e_n}\Bigr\} \\
        \label{eq:norm_equivalence_one}
        &= \norm{\vect x}_* \max \Bigl\{\norm{\vect e_1}, \dotsc, \norm{\vect e_n}\Bigr\}.
    \end{align}
    It remains to show that
    there exists a positive constant $\ell$ such that
    \begin{equation}
        \label{eq:bound2}
        \forall \vect x \in \mathcal X, \qquad
        \norm{\vect x}
        \geq \ell \Bigl( \abs{\lambda_1} + \dotsb + \abs{\lambda_n} \Bigr).
    \end{equation}
    To this end, we reason by contradiction.
    If this inequality were not true,
    then there would exist a sequence $(\vect x^{(i)})_{i \in \nat}$ such
    that~$\norm{\vect x^{(i)}} \to 0$ as $i \to \infty$ and $\norm{\vect x^{(i)}}_* = 1$ for all~$i \in \nat$.
    Since~$\lambda_1^{(i)} \in [-1, 1]$ for all~$i \in \nat$,
    we can extract a subsequence, still denoted by~$(\vect x^{(i)})_{i \in \nat}$ for simplicity,
    such that the corresponding coefficient $\lambda_1^{(i)}$ satisfies $\lambda_1^{(i)} \to \lambda_1^* \in [-1, 1]$,
    by compactness of the interval~$[-1, 1]$.
    Repeating this procedure for $\lambda_2, \lambda_3, \dots$,
    taking a new subsequence every time,
    we obtain a subsequence $(\vect x^{(i)})_{i \in \nat}$ such that $\lambda^{(i)}_j \to \lambda_j^*$ in the limit as~$i \to \infty$, for all $j \in \{1, \dotsc, n\}$.
    Therefore, it holds that $\vect x^{(i)} \to \vect x^* := \lambda_1^* \vect e_1 + \dotsb \lambda_n^* \vect e_n$ in the~$\norm{\placeholder}_*$ norm,
    and thus also in the~$\norm{\placeholder}$ norm by~\eqref{eq:norm_equivalence_one}.
    Since $\vect x^{(i)} \to \vect 0$ in the latter norm by assumption,
    we deduce that $\vect x^* = \vect 0$.
    But the vectors $\vect e_1, \dotsc, \vect e_n$ are linearly independent,
    and so this implies that $\lambda_1^* = \dots = \lambda_n^* = 0$,
    which is a contradiction because we also have that
    \[
        \abs{\lambda_1^*} + \dotsb + \abs{\lambda_n^*}
        = \lim_{i \to \infty} \bigl\lvert \lambda_1^{(i)} \bigr\rvert + \dotsb + \bigl\lvert \lambda_n^{(i)} \bigr\rvert = 1.
    \]
    This concludes the proof of~\eqref{eq:bound2}.
\end{proof}

\begin{exercise}
    Show that $\norm{\placeholder}_*\colon \mathcal X \to \real$ defined in~\eqref{eq:norm_star} is indeed a norm.
\end{exercise}

\begin{exercise}
    \label{exercise:triangle_inequality}
    Using~\cref{proposition:cauchy_shwartz},
    show that the function $\norm{\placeholder}$ defined by~\eqref{eq:induced_norm} satisfies the triangle inequality.
\end{exercise}

\section{Completeness}
Assume that $\mathcal X$ is a vector space with a norm~$\norm{\placeholder}$.
Together, $\mathcal X$ and $\norm{\placeholder}$ form a \emph{normed vector space}.
A sequence $(\vect x_n)_{n \geq 0}$ in~$\mathcal X$ is convergent in this space
if there exists $\vect x_* \in \mathcal X$ such that
\[
    \norm{\vect x_n - \vect x_*} \to 0 \quad \text{ in the limit $n \to \infty$. }
\]
In this case,
we write $\lim_{n \to \infty} \vect x_n = \vect x_*$ or $\vect x_n \to \vect x_*$.
\begin{definition}[Cauchy sequence]
    A sequence $(\vect x_n)_{n \geq 0}$ in~$\mathcal X$ is said to be Cauchy if for every $\varepsilon > 0$,
    there exists $N \in \nat$ such that
    \[
        \forall m,n \geq N, \qquad
        \norm{\vect x_n - \vect x_m} \leq \varepsilon.
    \]
    The normed vector space $(\mathcal X, \norm{\placeholder})$ is called complete if every Cauchy sequences is convergent.
\end{definition}
Every convergent sequence is Cauchy,
but the converse is not always true.

\begin{example}
    Consider the case where $\mathcal X = C([-1, 1])$,
    the space of continuous functions from~$[-1, 1]$ to~$\real$,
    endowed with the norm
    \[
        \norm{f} = \int_{-1}^{1} \abs{f(x)} \, \d x.
    \]
    The sequence of functions $(f_n)_{n\geq 0}$ in~$\mathcal X$ given by
    \begin{equation}
        \label{eq:example_non_complete}
        f_n(x) = x^{\frac{1}{2n+1}}
    \end{equation}
    is Cauchy but not convergent.
    Indeed, assume for contradiction that there exists~$f_* \in \mathcal X$ such that
    \begin{equation}
        \label{eq:example_non_complete_convergence}
        \norm{f_n - f_*} \xrightarrow[n \to \infty]{} 0.
    \end{equation}
    Then $f_*$ must necessarily coincide with sign function:
    \[
        {\rm sgn}(x)
        :=\begin{cases}
            -1 & \text{if } x < 0, \\
            0 & \text{if } x = 0, \\
        1 & \text{if } x > 0. \end{cases}
    \]
    However, this function is discontinuous,
    which contradicts the statement that~$f_* \in \mathcal X$.
\end{example}

In this course,
all the vector spaces encountered are complete.
For example,
\begin{itemize}
    \item $\real^n$ with any vector norm is complete.
    \item $\complex^{m\times n}$ with any matrix norm is complete.
\end{itemize}
In order to show that a sequence is convergent in a complete normed vector space,
it is sufficient to show that the sequence is Cauchy.
This approach is used in~\cref{lemma:linear_inverse_neumann} and~\cref{theorem:banach_fixed_point}.

\begin{exercise}
    Prove that every convergent sequence is Cauchy.
\end{exercise}

\section{Contraction mappings and the Banach fixed point theorem}
Let $(\mathcal X, \norm{\placeholder})$ denote a normed vector space.
A map $\phi\colon \mathcal X \to \mathcal X$ is called a contraction mapping if
there is a constant $L \in (0, 1)$ such that
\[
    \forall (x, y) \in \mathcal X \times \mathcal X, \qquad
    \norm{\phi(x) - \phi(y)} \leq L \norm{x - y}.
\]
The importance of contraction mappings in this course stems from the following theorem.

\begin{theorem}
    [Banach fixed point theorem]
    \label{theorem:banach_fixed_point}
    Let $(\mathcal X, \norm{\placeholder})$ be a complete normed space,
    and let~$\phi\colon \mathcal X \to \mathcal X$ be a contraction mapping.
    Then $\phi$ has a unique fixed point in~$\mathcal X$.
\end{theorem}
\begin{proof}
    We prove first existence and then uniqueness.

    \noindent\textbf{Existence.}
    Take $x_0 \in \mathcal X$,
    and define the sequence $(x_k)_{k\in \nat}$ inductively by
    \begin{equation}
        \label{eq:inductive_sequence}
        x_{k+1} = \phi(x_k).
    \end{equation}
    It holds that
    \[
        \norm[big]{x_{k+1} - x_{k}}
        = \norm[big]{\phi(x_{k}) - \phi(x_{k-1})}
        \leq L\norm{x_{k} - x_{k-1}}
        \leq \dots \leq L^{k} \norm{x_{1} - x_0}.
    \]
    Therefore, for any $n \geq m$,
    we have by the triangle inequality
    \begin{align*}
        \norm{x_{n} - x_{m}}
        &\leq \norm{x_{n} - x_{n-1}} + \dotsb + \norm{x_{m+1} - x_{m}} \\
        &\leq (L^{n-1} + \dotsb + L^{m}) \norm{x_{1} - x_0}
        \leq L^{m} (1 + L + \dotsb) \norm{x_{1} - x_0}
        = \frac{L^{m}}{1-L} \norm{x_{1} - x_0}.
    \end{align*}
    It follows that the sequence $(x_k)_{k\geq0}$ is Cauchy in $\mathcal X$,
    implying by completeness that $x_k \to x_*$ in the limit as $k \to \infty$,
    for some limit $x_* \in \mathcal X$.
    Being a contraction, the mapping $\phi$ is continuous,
    and so taking the limit~$k \to \infty$ in~\eqref{eq:inductive_sequence}, we obtain that
    \[
        x_* = \lim_{k \to \infty} x_{k+1}
        = \lim_{k \to \infty} \phi(x_k)
        = \phi \left( \lim_{k \to \infty} x_k \right)
        = \phi(x_*).
    \]
    In other words, $x_*$ is a fixed point of~$\phi$.

    \vspace{.2cm}
    \noindent\textbf{Uniqueness.}
    Assume that $y_* \in \mathcal X$ is another fixed point.
    Then,
    \begin{equation*}
        \norm[big]{y_* - x_{*}}
        = \norm[big]{\phi(y_*) - \phi(x_{*})}
        \leq L \norm{y_* - x_*},
    \end{equation*}
    which implies that $y_* = x_*$ since $L < 1$.
\end{proof}
\begin{remark}
    % Notice that the vector space structure is not employed in the proof of~\cref{theorem:banach_fixed_point}.
    The Banach fixed point theorem holds also in complete metric spaces.
\end{remark}

\section{Vector norms}%
\label{sub:vector_norms}

In the vector space $\complex^n$,
the most commonly used norms are particular cases of the $p$-norm, also called H\"older norm.

\begin{definition}
    \label{definition:pnorm_vector}
    Given $p \in [1, \infty]$,
    the $p$-norm of a vector $\vect x \in \complex^n$ is defined by
    \[
        \norm{\vect x}_p :=
        \begin{cases}
            \left( \sum_{i=1}^{n} \abs{x^i}^p \right)^{\frac{1}{p}} & \text{if $p < \infty$}, \\
            \max \Bigl\{ \abs{x_1}, \dotsc, \abs{x_n} \Bigr\} & \text{if $p = \infty$}.
        \end{cases}
    \]
\end{definition}
The values of $p$ most commonly encountered in applications are $1$, $2$ and $\infty$.
The $1$-norm is sometimes called the \emph{taxicab} or \emph{Manhattan} norm,
and the $2$-norm is usually called the \emph{Euclidean norm}.
The explicit expressions of these norms are
\[
    \norm{\vect x}_1 = \sum_{i=1}^{n} \abs{x_i},
    \qquad
    \norm{\vect x}_2 = \sqrt{\sum_{i=1}^{n} \abs{x_i}^2}.
\]
Notice that the infinity norm $\norm{\placeholder}_{\infty}$ may be defined as the limit of the $p$-norm as $p \to \infty$:
\[
    \norm{\vect x}_{\infty}
    := \lim_{p \to \infty} \norm{\vect x}_p.
\]

In the rest of this chapter,
the notations $\ip{\placeholder,\placeholder}$ and $\norm{\placeholder}$ without subscript always refer to the Euclidean inner product~\eqref{eq:induced_norm} and induced norm,
unless specified otherwise.

\section{Matrix norms}%
\label{sec:matrix_norms}

Given two norms $\norm{\placeholder}_{\alpha}$ and $\norm{\placeholder}_{\beta}$ on $\complex^m$ and $\complex^n$, respectively,
we define the \emph{operator norm} induced by $\norm{\placeholder}_{\alpha}$ and $\norm{\placeholder}_{\beta}$ of the matrix $\mat A$ as
\begin{equation}
    \label{eq:subordinate_norm}
    \norm{\mat A}_{\alpha,\beta} = \sup \bigl\{ \norm{\mat A \vect x}_{\alpha} : \vect x \in \complex^n, \norm{\vect x}_{\beta} \leq 1 \bigr\}.
\end{equation}
The term \emph{operator norm} is motivated by the fact that,
to any matrix $\mat A \in \complex^{m \times n}$, there naturally corresponds the linear operator from $\complex^n$ to $\complex^m$ with action~$\vect x \mapsto \mat A \vect x$.
% Equation~\eqref{eq:subordinate_norm} comes from the general definition of the norm of a bounded linear operator between normed spaces.
Matrix norms of the type~\eqref{eq:subordinate_norm} are also called \emph{subordinate} matrix norms.
An immediate corollary of the definition~\eqref{eq:subordinate_norm} is that,
for all $\vect x \in \complex^n$,
\begin{equation}
    \label{eq:submultiplicative_mat_vec}%
    \norm{\mat A \vect x}_{\alpha}
    = \norm{\mat A \widehat{\vect x}}_{\alpha} \norm{\vect x}_{\beta}
    \leq \sup \bigl\{ \norm{\mat A \vect y}_{\alpha}: \norm{\vect y}_{\beta} \leq 1 \bigr\} \norm{\vect x}_{\beta}
    = \norm{\mat A}_{\alpha,\beta} \norm{\vect x}_{\beta},
    \qquad \widehat {\vect x} = \frac{\vect x}{\norm{\vect x}_{\beta}}.
\end{equation}

\begin{exercise}
    Show that equation~\eqref{eq:subordinate_norm} defines a norm on $\complex^{m \times n}$.
\end{exercise}
\begin{solution}
    We need to verify that~\eqref{eq:subordinate_norm} satisfies the properties of positivity and homogeneity,
    together with the triangle inequality.
    \begin{itemize}
        \item
            Checking \textbf{positivity} is simple and left as an exercise.
            % \textbf{positivity}
            % . If $A \neq 0$,
            % then there is a column of $A$, say the $j$-th one, that is nonzero.
            % Let~$\vect y$ be the vector with all entries equal to zero except the $j$-th one which is equal to 1,
            % and let~$\vect x = \vect y / \norm{\vect y}_{\beta}$ so that $\norm{\vect x}_{\beta} = 1$.
            % It is clear that $A \vect x \neq \vect 0$,
            % so $\norm{A \vect x}_{\alpha} > 0$ by positivity of the norm $\norm{\placeholder}_{\alpha}$,
            % implying that $\norm{A}_{\alpha,\beta} > 0$.

        \item
            \textbf{Homogeneity} follows trivially from the definition~\eqref{eq:subordinate_norm}
            and the homogeneity of $\norm{\placeholder}_{\alpha}$.

        \item
            \textbf{Triangle inequality.}
            Let $\mat A$ and $\mat B$ be two elements of $\complex^{m \times n}$.
            Employing the triangle inequality for the norm $\norm{\placeholder}_{\alpha}$,
            we have
            \begin{align*}
                \forall \vect x \in \complex^{n} ~ \text{with} ~ \norm{\vect x}_{\beta}\leq 1, \qquad
                \norm{(\mat A + \mat B) \vect x}_{\alpha}
                &= \norm{\mat A \vect x + \mat B \vect x}_{\alpha} \\
                &\leq \norm{\mat A \vect x}_{\alpha} + \norm{\mat B \vect x}_{\alpha}
                \leq \norm{\mat A}_{\alpha,\beta} + \norm{\mat B}_{\alpha,\beta}.
            \end{align*}
            Taking the supremum as in~\eqref{eq:subordinate_norm},
            we obtain $\norm{\mat A + \mat B}_{\alpha,\beta} \leq \norm{\mat A}_{\alpha,\beta} + \norm{\mat B}_{\alpha,\beta}$.
    \end{itemize}
    Since the three properties are satisfied, $\norm{\placeholder}_{\alpha,\beta}$ is indeed a norm.
\end{solution}

The matrix $p$-norm is defined as the operator norm~\eqref{eq:subordinate_norm} in the particular case
where $\norm{\placeholder}_{\alpha}$ and $\norm{\placeholder}_{\beta}$ are both H\"older norms with the same value of $p$.
\begin{definition}
Given $p \in [1, \infty]$,
the $p$-norm of a matrix $\mat A \in \complex^{m\times n}$ is given by
\begin{equation}
    \label{eq:matrix_p_norm}
    \norm{\mat A}_{p} := \sup \bigl\{ \norm{\mat A \vect x}_{p} : \vect x \in \complex^n, \norm{\vect x}_{p} \leq 1 \bigr\}.
\end{equation}
\end{definition}
Not all matrix norms are induced by vector norms.
For example, the Frobenius norm,
which is widely used in applications,
is not induced by a vector norm.
It is, however, induced by an inner product on~$\complex^{m \times n}$.
\begin{definition}
    The Frobenius norm of $\mat A \in \complex^{m\times n}$ is given by
    \begin{equation}
        \label{eq:frobenius_norm}
        \norm{\mat A}_{\rm F} = \left( \sum_{i=1}^{m} \sum_{j=1}^{n} \abs{a_{ij}}^2 \right)^{\frac{1}{2}}.
    \end{equation}
\end{definition}

A matrix norm $\norm{\placeholder}$ is said to be submultiplicative if,
for any two matrices $\mat A \in \complex^{m \times n}$ and $\mat B \in \complex^{n \times \ell}$,
it holds that
\[
    \norm{\mat A \mat B} \leq \norm{\mat A} \norm{\mat B}.
\]
All subordinate matrix norms,
for example the $p$-norms, are submultiplicative,
and so is the Frobenius norm.

\begin{exercise}
    Write down the inner product on $\complex^{m \times n}$ corresponding to~\eqref{eq:frobenius_norm}.
\end{exercise}
\begin{exercise}
    Show that the matrix $p$-norm is submultiplicative.
\end{exercise}

\newpage
\section{Diagonalization and spectral theorem}%
\label{sec:diagonalization}

\begin{definition}
    \label{definition:diagonalizable}
    A square matrix $\mat A \in \complex^{n \times n}$ is said to be diagonalizable if there exists an invertible matrix $\mat P \in \complex^{n \times n}$
    and a diagonal matrix $\mat D \in \complex^{n \times n}$ such that
    \begin{equation}
        \label{eq:eigen_decomposition}
        \mat A \mat P = \mat P \mat D.
    \end{equation}
    In this case,
    the diagonal elements of $\mat D$ are called the eigenvalues of~$\mat A$,
    and the columns of $\mat P$ are called the eigenvectors of~$\mat A$.
\end{definition}
Denoting by $\vect e_i$ the $i$-th column of $\mat P$ and by $\lambda_i$ the $i$-th diagonal element of $\mat D$,
we have by~\eqref{eq:eigen_decomposition} that $\mat A \vect e_i = \lambda_i \vect e_i$ or,
equivalently, $(\mat A - \lambda_i \mat I_n) \vect e_i = \vect 0$.
Here $\mat I_n$ is the~$\complex^{n \times n}$ identity matrix.
Therefore, a complex number $\lambda$ is an eigenvalue of $\mat A$ if and only if $\det(\mat A - \lambda \mat I_n) = 0$.
In other words, the eigenvalues of $\mat A$ are the roots of $\det(\mat A - \lambda \mat I_n)$,
which is called the \emph{characteristic polynomial}.

\subsection*{Symmetric matrices and spectral theorem}%
The transpose of a matrix $\mat A \in \complex^{m \times n}$ is denoted by $\mat A^\t \in \complex^{n \times m}$
and defined as the matrix with entries~$a^\t_{ij} = a_{ji}$.
The conjugate transpose of $\mat A$ is the matrix obtained by taking the transpose and taking the complex conjugate of all the entries.
A real matrix equal to its transpose is necessarily square and called \emph{symmetric},
and a complex matrix equal to its conjugate transpose is called \emph{Hermitian}.
Hermitian matrices, of which real symmetric matrices are a subset,
enjoy many nice properties,
the main one being that they are diagonalizable with a matrix $\mat Q$ that is unitary,
i.e. such that $\mat Q^{-1} = \mat Q^*$.
This is the content of the \emph{spectral theorem},
a pillar of linear algebra with important generalizations to infinite-dimensional operators.

\begin{theorem}
    [Spectral theorem for Hermitian matrices]
    \label{theorem:spectral_theorem}
    If $\mat A \in \complex^{n \times n}$ is Hermitian,
    then there exists an unitary matrix $\mat Q \in \complex^{n \times n}$ and a diagonal matrix $\mat D \in \real^{n \times n}$ such that
    \[
        \mat A \mat Q = \mat Q \mat D.
    \]
\end{theorem}
\begin{proof}
    [Sketch of the proof]
    The result is trivial for $n = 1$.
    Reasoning by induction,
    we assume that the result is true for Hermitian matrices in $\complex^{n-1 \times n-1}$
    and prove that it then also holds for~$\mat A \in \complex^{n \times n}$.
    % \begin{itemize}
        % \item

            \vspace{.3cm}
            \textbf{Step 1. Existence of a real eigenvalue}.
            By the fundamental theorem of algebra,
            there exists at least one solution $\lambda_1 \in \complex$ to the equation $\det(\mat A - \lambda \mat I_n) = 0$,
            to which there corresponds a solution $\vect q_1 \in \complex^n$ of norm 1 to the equation $(\mat A - \lambda_1 \mat I_n) \vect q_{1} = \vect 0$.
            The eigenvalue $\lambda_1$ is necessarily real because
            \[
                \lambda_1 \ip{\vect e_1, \vect e_1}
                = \ip{\lambda_1 \vect e_1, \vect e_1}
                = \ip{\mat A \vect e_1, \vect e_1}
                = \ip{\vect e_1, \mat A\vect e_1}
                = \ip{\vect e_1, \lambda_1 \vect e_1}
                = \ip{\vect e_1, \lambda_1 \vect e_1}
                = \overline \lambda_1 \ip{\vect e_1, \vect e_1}.
            \]

        % \item
            \vspace{.3cm}
            \textbf{Step 2. Using the induction hypothesis}.
            Next,
            take an orthonormal basis $(\vect e_2, \dotsc, \vect e_n)$ of the orthogonal complement $\Span\{\vect q_1\}^\perp$
            and construct the unitary matrix
            \[
                V = \begin{pmatrix} \vect q_1 & \vect e_2 & \dots & \vect e_n \end{pmatrix},
            \]
            i.e.\ the matrix with columns $\vect q_1$, $\vect e_2$, etc.
            A calculation gives,
            \[
                \mat V^* \mat A \mat V =
                \begin{pmatrix}
                    \ip{\vect q_1, \mat A \vect q_1} & \ip{\vect q_1, \mat A \vect e_2} & \dots & \ip{\vect q_1, \mat A \vect e_n} \\
                    \ip{\vect e_2, \mat A \vect q_1} & \ip{\vect e_2, \mat A \vect e_2} & \dots & \ip{\vect e_2, \mat A \vect e_n} \\
                    \vdots & \vdots & \ddots & \vdots \\
                    \ip{\vect e_n, \mat A \vect q_1} & \ip{\vect e_n, \mat A \vect e_2} & \dots & \ip{\vect e_n, \mat A \vect e_n}
                \end{pmatrix}
                =
                \begin{pmatrix}
                    \lambda_1 & 0 & \dots & 0 \\
                    0 & \ip{\vect e_2, \mat A \vect e_2} & \dots & \ip{\vect e_2, \mat A \vect e_n} \\
                    \vdots & \vdots & \ddots & \vdots \\
                    0 & \ip{\vect e_n, \mat A \vect e_2} & \dots & \ip{\vect e_n, \mat A \vect e_n}
                \end{pmatrix}.
            \]
            Let us denote the $n-1 \times n-1$ lower right block of this matrix by $\mat V_{n-1}$.
            This is a Hermitian matrix of size $n-1$ so,
            using the induction hypothesis,
            we deduce that $\mat V_{n-1} = \mat Q_{n-1} \mat D_{n-1} \mat Q_{n-1}^*$
            for appropriate matrices $\mat Q_{n-1} \in \complex^{n-1 \times n-1}$ and $\mat D_{n-1} \in \real^{n-1 \times n-1}$
            which are unitary and diagonal, respectively.

        % \item
            \vspace{.3cm}
            \textbf{Step 3. Constructing $\mat Q$ and $\mat D$}.
            Define now
            \[
                \mat Q =
                \mat V
                \begin{pmatrix}
                    1 & \vect 0^\t \\
                    \vect 0 & \mat Q_{n-1}
                \end{pmatrix}.
            \]
            It is not difficult to verify that $\mat Q$ is a unitary matrix,
            and we have
            \[
                \mat Q^* \mat A \mat Q =
                \begin{pmatrix}
                    1 & \vect 0^\t \\
                    \vect 0 & \mat Q_{n-1}^*
                \end{pmatrix}
                \mat V^* \mat A \mat V
                \begin{pmatrix}
                    1 & \vect 0^\t \\
                    \vect 0 & \mat Q_{n-1}
                \end{pmatrix}
                =
                \begin{pmatrix}
                    1 & \vect 0^\t \\
                    \vect 0 & \mat Q_{n-1}^*
                \end{pmatrix}
                \begin{pmatrix}
                    \lambda_1 & \vect 0^\t \\
                    \vect 0 & \mat V_{n-1}
                \end{pmatrix}
                \begin{pmatrix}
                    1 & \vect 0^\t \\
                    \vect 0 & \mat Q_{n-1}
                \end{pmatrix}.
            \]
            Developing the last expression, we obtain
            \[
                \mat Q^* \mat A \mat Q
                =
                \begin{pmatrix}
                    1 & \vect 0^\t \\
                    \vect 0 & \mat D_{n-1}
                \end{pmatrix},
            \]
            which concludes the proof.
    % \end{itemize}
\end{proof}

We deduce, as a corollary of the spectral theorem,
that if $\vect e_1$ and $\vect e_2$ are eigenvectors of a Hermitian matrix associated with different eigenvalues,
then they are necessarily orthogonal for the Euclidean inner product.
Indeed, since $\mat A = \mat A^*$ and the eigenvalues are real, it holds that
\begin{align*}
    (\lambda_1 - \lambda_2) \ip{\vect e_1, \vect e_2}
    &= \ip{\lambda_1 \vect e_1, \vect e_2} - \ip{\vect e_1, \lambda_2 \vect e_2} \\
    &= \ip{\mat A \vect e_1, \vect e_2} - \ip{\vect e_1, \mat A \vect e_2}
    = \ip{\mat A \vect e_1, \vect e_2} - \ip{\mat A^* \vect e_1, \vect e_2} = 0.
\end{align*}

The largest eigenvalue of a matrix, in modulus,
is called the \emph{spectral radius} and denoted by~$\rho$.
The following result relates the 2-norm of a matrix to the spectral radius of $\mat A \mat A^*$.
\begin{proposition}
    It holds that $\norm{\mat A}_2 = \sqrt{\rho(\mat A^* \mat A)}$.
\end{proposition}
\begin{proof}
    Since $\mat A^* \mat A$ is Hermitian,
    it holds by the spectral theorem that $\mat A^* \mat A = \mat Q \mat D \mat Q^*$ for some unitary matrix $\mat Q$ and real diagonal matrix $\mat D$.
    Therefore, denoting by $(\mu_i)_{1 \leq i \leq n}$ the (positive) diagonal elements of $\mat D$
    and introducing $\vect y := \mat Q^* \vect x$,
    we have
    \begin{align}
        \nonumber
        \norm{\mat A \vect x}
        &= \sqrt{\vect x^* \mat A^* \mat A \vect x}
        = \sqrt{\vect x^* \mat Q \mat D \mat Q^* \vect x} \\
        \label{eq:matrices_link_norm_spectral_radius}
        &= \sqrt{\sum_{i=1}^{n} \mu_i y_i^2}
        \leq \sqrt{\rho(\mat A^* \mat A)} \sqrt{\sum_{i=1}^{n} y_i^2}
        =  \sqrt{\rho(\mat A^* \mat A)} \norm{\vect x},
    \end{align}
    where we used in the last equality the fact that $\vect y$ has the same norm as $\vect x$,
    because $\mat Q$ is unitary.
    It follows from~\eqref{eq:matrices_link_norm_spectral_radius} that $\norm{\mat A} \leq \sqrt{\rho(\mat A^* \mat A)}$,
    and the converse inequality also holds true since $\norm{\mat A \vect x} = \sqrt{\rho(\mat A^* \mat A)} \norm{\vect x}$
    if $\vect x$ is the eigenvector of $\mat A^* \mat A$ corresponding to an eigenvalue of modulus~$\rho(\mat A^* \mat A)$.
\end{proof}

To conclude this section,
we recall and prove the Courant--Fisher theorem.
\begin{theorem}
    [Courant--Fisher Min-Max theorem]
    \label{theorem:courant-fisher}
    The eigenvalues $\lambda_1 \geq \lambda_2 \geq \dotsb \geq \lambda_n$ of a Hermitian matrix are characterized by the relation
    \[
        \lambda_k = \max_{\mathcal S, {\rm dim}(\mathcal S) = k} \left( \min_{\vect x \in \mathcal S \backslash\{0\}} \frac{\vect x^* \mat A \vect x}{\vect x^* \vect x} \right).
    \]
\end{theorem}
\begin{proof}
    Let $\vect v_1, \dotsc, \vect v_n$ be normalized and pairwise orthogonal eigenvectors associated with the eigenvalues $\lambda_1, \dotsc, \lambda_n$,
    and let $\mathcal S_k = \Span \{ \vect v_1, \dotsc, \vect v_k \}$.
    Any $\vect x \in \mathcal S_k$ may be expressed as a linear combination~$\vect x = \alpha_1 \vect v_1 + \dotsb + \alpha_k \vect v_k$,
    and so
    \[
        \forall \vect x \in \mathcal S_k, \qquad
        \frac{\vect x^* \mat A \vect x}{\vect x^* \vect x} =
        \frac{\sum_{i=1}^{k} \lambda_i \abs{\alpha_i}^2}{\sum_{i=1}^{k} \abs{\alpha_i}^2}
        \geq \lambda_k.
    \]
    Therefore,
    it holds that
    \[
         \min_{\vect x \in \mathcal S_k \backslash\{0\}} \frac{\vect x^* \mat A \vect x}{\vect x^* \vect x}  \geq \lambda_k,
    \]
    which proves the $\geq$ direction.
    For the $\leq$ direction,
    let $\mathcal U_k = \Span \{ \vect v_{k}, \dots \vect v_n \}$.
    Using a well-known result from linear algebra,
    we calculate that, for any subspace $\mathcal S \subset \complex^n$ of dimension~$k$,
    \begin{align*}
        \dim(\mathcal S \cap \mathcal U_k)
        &= \dim(\mathcal S) + \dim(\mathcal U_k) -\dim(\mathcal S + \mathcal U_k) \\
        &\geq  k + (n - k + 1) - n = 1.
    \end{align*}
    Therefore, any $\mathcal S \subset \complex^n$ of dimension $k$
    has a nonzero intersection with $\mathcal U_k$.
    But since any vector in $\mathcal U_k$ can be expanded as $\beta_1 \vect v_k + \dotsb + \beta_n \vect v_n$,
    we have
    \[
        \forall \vect x \in \mathcal U_k, \qquad
        \frac{\vect x^* \mat A \vect x}{\vect x^* \vect x} =
        \frac{\sum_{i=k}^{n} \lambda_i \abs{\alpha_i}^2}{\sum_{i=k}^{n} \abs{\alpha_i}^2}
        \leq \lambda_k.
    \]
    This shows that
    \[
        \forall \mathcal S \subset \complex^n \text{ with } \dim(\mathcal S) = k,
        \qquad
         \min_{\vect x \in \mathcal S \backslash\{0\}} \frac{\vect x^* \mat A \vect x}{\vect x^* \vect x}  \leq \lambda_k,
    \]
    which enables to conclude the proof.
\end{proof}

\begin{exercise}
    Prove that if $\mat A \in \real^{n \times n}$ is diagonalizable as in~\eqref{eq:eigen_decomposition},
    then $\mat A^n = \mat P \mat D^n \mat P^{-1}$.
\end{exercise}

\section{Similarity transformation and Jordan normal form}%
\label{sec:similarity_transformation_and_jordan_normal_form}
In this section, we work with matrices in $\complex^{n \times n}$.
A \emph{similarity transformation} is a mapping of the type~$\complex^{n \times n} \ni \mat A \mapsto \mat P^{-1} \mat A \mat P \in \complex^{n \times n}$,
where $\mat P \in \complex^{n \times n}$ is a nonsingular matrix.
If two matrices are related by a similarity transformation,
they are called \emph{similar},
because they may be viewed as two representations of the same linear mapping in different bases.

\begin{definition}
    [Jordan block]
    A Jordan block with dimension $n$ is a matrix of the form
    \[
        \mat J_{n}(\lambda) =
        \begin{pmatrix}
            \lambda & 1  \\
          & \lambda & 1 \\
          &         & \ddots  & \ddots  \\
          &         &         & \lambda & 1  \\
          &         &         &         & \lambda
        \end{pmatrix}
    \]
    The parameter $\lambda \in \complex$ is called the eigenvalue of the Jordan block.
\end{definition}
A Jordan block is diagonalizable if and only if it is of dimension 1.
The only eigenvector of a Jordan block is $\begin{pmatrix} 1 & 0 & \hdots & 0 \end{pmatrix}^\t$.
The power of a Jordan block admits an explicit expression.
\begin{lemma}
    \label{lemma:matrices_power_jordan_block}
    It holds that
    \begin{equation}
        \label{eq:matrices_power_jordan_block}
        \mat J_n(\lambda)^k =
        \begin{pmatrix}
            \lambda^k & \binom{k}{1}\lambda^{k-1} & \binom{k}{2}\lambda^{k-2} & \cdots & \cdots & \binom{k}{n-1}\lambda^{k-n+1} \\
                      & \lambda^k & \binom{k}{1}\lambda^{k-1} & \cdots & \cdots & \binom{k}{n-2}\lambda^{k-n+2} \\
                      &  & \ddots & \ddots &  & \vdots\\
                      &  & & \ddots & \ddots & \vdots\\
                      &  & &  & \lambda^k & \binom{k}{1}\lambda^{k-1}\\
                      &  &  &  &  & \lambda^k
        \end{pmatrix}.
    \end{equation}
    % In addition, for any $\mu > \abs{\lambda}$,
    % there exists $C > 0$ such that
    % \[
    %     \forall k \in \nat, \qquad
    %     \norm{J_{n}(\lambda)^k} \leq C \mu^k.
    % \]
\end{lemma}
\begin{proof}
    The explicit expression of the Jordan block can be obtained by decomposing the block as $\mat J_{n}(\lambda) = \lambda \mat I + \mat N$
    and using the binomial formula:
    \[
        (\lambda \mat I + \mat N)^k =
        \sum_{i=1}^{k} \binom{k}{i} (\lambda \mat I)^{k-i} \mat N^{i}.
    \]
    To conclude the proof,
    we use the fact that $\mat N^{i}$ is a matrix with zeros everywhere except for $i$-th super-diagonal,
    which contains only ones.
    Moreover $\mat N^i = \mat 0_{n \times n}$ if $i \geq n$.
\end{proof}

A matrix is said to be of \emph{Jordan normal form} if it is block-diagonal with Jordan blocks on the diagonal.
In other words, a matrix~$\mat J \in \complex^{n \times n}$ is of Jordan normal form if
\[
    \mat J =
        \begin{pmatrix}
            J_{n_1}(\lambda_1)  \\
                              & J_{n_2}(\lambda_2)    \\
                              &                 & \ddots  \\
                              &                 &         & J_{n_{k-1}}(\lambda_{k-1})   &   \\
                              &                 &         &                              & J_{n_k}(\lambda_k)
    \end{pmatrix}
\]
with $n_1 + \dotsb + n_k = n$.
Note that $\lambda_1, \dotsc, \lambda_k$ are the eigenvalues of $\mat A$.
We state without proof the following important result.

\begin{proposition}
    [Jordan normal form]
    \label{proposition:matrices_jordan_normal_form}
    Any matrix $\mat A \in \complex^{n \times n}$ is \emph{similar} to a matrix in Jordan normal form.
    In other words, there exists an invertible matrix $\mat P \in \complex^{n \times n}$
    and a matrix in normal Jordan form $\mat J \in \complex^{n \times n}$ such that
    \[
        \mat A = \mat P \mat J \mat P^{-1}
    \]
\end{proposition}
% \begin{proof}
%     We prove the result by induction:
%     it is trivial in dimension one,
%     and we assume that it holds true up to dimension $n-1$.
%     By the fundamental theorem of algebra,
%     the matrix $\mat A$ has at least one eigenvalue~$\lambda$.
%     Considering $\mat A - \lambda \mat I$ instead of $\mat A$ if necessary,
%     we can assume without loss of generality that $\lambda = 0$.
%     Let $(\vect k_1, \dotsc, \vect k_{m})$ be a basis of the vector space generated by the generalized eigenvectors associated with $0$,
%     i.e.\ the vectors satisfying $\mat A^i \vect v = 0$ for some $i \geq 0$.
%     Let also~$\mat R$ denote the range of $\mat A - \lambda \mat I$,
%     i.e.\ the vector space generated by the columns of $\mat A - \lambda \mat I$,
%     and $(\vect e_{m+1}, \dotsc, \vect e_{n})$ be a basis of $\mat R$.
%     The subspace $\mat R$ is a stable subspace of $\mat A$,
%     which means that~$\mat A \vect r \in \real$ for all $\vect r \in \mat R$.
%     Therefore
%     \[
%         \mat B^{-1} (\mat A - \lambda \mat I) \mat B =
%         \begin{pmatrix}
%             \mat 0_{m \times m}
%         \end{pmatrix}
%     \]
% \end{proof}

\section{Oldenburger's theorem and Gelfand's formula}
The following result establishes a necessary and sufficient condition for the convergence of $\norm{\mat A^k}$ to 0 in terms of the spectral radius of $\mat A$,
and for any matrix norm $\norm{\placeholder}$.
\begin{proposition}
    [Oldenburger]
    \label{proposition:matrices_convergence_power_of_matrix}
    Let $\rho(\mat A)$ denote the spectral radius of $\mat A \in \complex^{n \times n}$ and $\norm{\placeholder}$ be a matrix norm.
    Then
    \begin{itemize}
        \item $\norm{\mat A^k} \to 0$ in the limit as $k \to \infty$ if and only if $\rho(\mat A) < 1$.
        \item $\norm{\mat A^k} \to \infty$ in the limit as $k \to \infty$ if and only if $\rho(\mat A) > 1$.
    \end{itemize}
\end{proposition}
\begin{proof}
    Since all matrix norms are equivalent,
    we can assume without loss of generality that~$\norm{\placeholder}$ is the 2-norm.
    We prove only the equivalence $\norm{\mat A^k} \to 0 \Leftrightarrow \rho(\mat A) < 1$.
    The other statement can be proved similarly.
    By~\cref{proposition:matrices_jordan_normal_form},
    there exists a nonsingular matrix $\mat P$ such that~$\mat A = \mat P \mat J \mat P^{-1}$,
    for a matrix $\mat J \in \complex^{n \times n}$ which is in normal Jordan form.
    Since $\rho(\mat A) = \rho(\mat J)$ and $\norm{\mat A^k} \to 0$ if and only if $\norm{\mat J^k} \to 0$,
    it is sufficient to show that $\norm{\mat J^k} \to 0 \Leftrightarrow \rho(\mat J) < 1$.
    The latter statement follows from the expression of the power of a Jordan block given in~\cref{lemma:matrices_power_jordan_block}.
\end{proof}

With this result,
we can prove Gelfand's formula,
which relates the spectral radius to the asymptotic growth of $\norm{\mat A^k}$,
and is used in \cref{cha:solution_of_linear_systems}.
\begin{proposition}
    [Gelfand's formula]
    \label{proposition:matrices_gelfands}
    Let $\mat A \in \complex^{n \times n}$.
    It holds for any norm that
    \[
        \lim_{k \to \infty} \norm{\mat A^k}^{\frac{1}{k}} = \rho(\mat A)
    \]
\end{proposition}
\begin{proof}
    Let $0 < \varepsilon < \rho(\mat A)$
    and define $\mat A^+ = \frac{\mat A}{\rho(\mat A) + \varepsilon}$ and $\mat A^- = \frac{\mat A}{\rho(\mat A) - \varepsilon}$.
    It holds by construction that~$\rho(\mat A^+) < 1$ and $\rho(\mat A^-) > 1$.
    Using~\cref{proposition:matrices_convergence_power_of_matrix}, we deduce that
    \[
        \lim_{k \to \infty} \norm{(\mat A^+)^k} = 0, \qquad \lim_{k \to \infty} \norm{(\mat A^-)^k} = \infty.
    \]
    Therefore, it holds that
    \[
        \limsup_{k \to \infty} \left\lVert (\mat A^+)^k \right\rVert^{\frac{1}{k}} \leq 1,
        \qquad
        \liminf_{k \to \infty} \left\lVert (\mat A^-)^k \right\rVert^{\frac{1}{k}} \geq 1.
    \]
    Substituting the expressions of $\mat A^+$ and $\mat A^-$,
    we deduce that
    \[
        \limsup_{k \to \infty} \Bigl\lVert \mat A^k \Bigr\rVert^{\frac{1}{k}} \leq \rho(\mat A) + \varepsilon,
        \qquad
        \liminf_{k \to \infty} \bigl\lVert \mat A^k \bigr\rVert^{\frac{1}{k}} \geq \rho(\mat A) - \varepsilon.
    \]
    Since $\varepsilon$ was arbitrary, we obtain that
    \[
        \rho(\mat A)
        \leq \liminf_{k \to \infty} \bigl\lVert \mat A^k \bigr\rVert^{\frac{1}{k}}
        \leq \limsup_{k \to \infty} \bigl\lVert \mat A^k \bigr\rVert^{\frac{1}{k}}
        \leq \rho(\mat A),
    \]
    which implies the statement.
\end{proof}
