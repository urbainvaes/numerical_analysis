\chapter{Numerical ordinary differential equations}
\label{cha:odep}
\minitoc

\section*{Introduction}
This chapter concerns the numerical solution of ordinary differential equations (ODEs) of the following form:
\begin{equation}
    \label{eq:ode}
    \left \{
    \begin{aligned}
        & \dot {\vect x}(t) = \vect f\bigl(t, \vect x(t)\bigr), \\
        & \vect x(t_0) = \vect x_0.
    \end{aligned}
    \right.
\end{equation}
Here $\vect f\colon \real \times \real^n \to \real$ and $\vect x_0$ is the initial condition.
Equations of this type
are the building blocks of a plethora mathematical models in science and engineering.
They have applications in celestial dynamics, molecular simulation and fluid mechanics, to mention just a few.
Ordinary differential equations also arise after discretization of time-dependent partial differential equations,
which are also ubiquitous in science.
More often than not,
it is not possible to find an explicit solution of~\eqref{eq:ode},
and so one has to resort to numerical simulation.
The rest of chapter is organized as follows:
\begin{itemize}
    \item
        In \cref{sec:existence},
        we define precisely the notions of local and global solutions for the continuous-time problem~\eqref{eq:ode},
        and we recall fundamental results concerning the existence and uniqueness of a solution.

    \item
        In~\cref{sec:one-step_methods},
        we analyze the so-called one-step numerical methods to solve~\eqref{eq:ode}.
\end{itemize}

\section{Analysis of the continuous problem}
A differentiable function $\vect x\colon I \to \real^n$,
where $I$ denotes an interval of $\real$ containing $t_0$,
is a solution of~\eqref{eq:ode} to $\vect x(t_0) = \vect x_0$ and the equation~\eqref{eq:ode} is satisfied for all~$t \in I$.
The solution is called global if~$I = \real$,
and local otherwise.

\paragraph{Integral formulation.}
If $\vect x$ is a solution to~\eqref{eq:ode},
then it holds that
\begin{equation}
    \label{eq:ode_integral}
    \forall  t \in I, \qquad
    \vect x(t) = \vect x_0 + \int_{t_0}^{t} \vect f\bigl(s, \vect x(s)\bigr) \, \d s.
\end{equation}
The converse statement is not true in general,
because a solution to~\eqref{eq:ode_integral} need not necessarily be differentiable.
However, if the integral formulation~\eqref{eq:ode_integral} holds,
then necessarily $\vect x$ is absolutely continuous
and~\eqref{eq:ode} is satisfied for almost every~$t$.
Additionally, if~\eqref{eq:ode_integral} is satisfied and the function~$\vect f$ is continuous,
then the function~$s \mapsto \vect f\bigl(s, \vect x(s)\bigr)$ is continuous,
and so~\eqref{eq:ode} is satisfied for all $t \in I$ by the fundamental theorem of analysis.

\label{sec:existence}
\begin{theorem}
    [Existence of a solution]
    \label{theorem:existence}
    Let $\vect x_0 \in \real^n$ and let $\Omega_{\mathcal T, \mathcal R}$ denote the set
    \[
        \bigl\{(t, \vect x) \in \real \times \real^n: \abs{t-t_0} \leq \mathcal T \text{ and } \vecnorm{\vect x - \vect x_0} \leq \mathcal R \bigr\},
    \]
    Assume that the following conditions are satisfied:
    \begin{itemize}
        \item
            The function $\vect f$ is uniformly bounded on~$\Omega_{\mathcal T, \mathcal R}$:
            \begin{equation}
                \label{eq:boundedness_ode}
                \forall (t, \vect x) \in \Omega_{\mathcal T, \mathcal R}, \qquad
                \vecnorm{\vect f(t, \vect x)} \leq M.
            \end{equation}

        \item
            The function $\vect f$ satisfies the following Lipschitz condition:
            there is $L > 0$ such that
            \begin{equation}
                \label{eq:local_lipschitz_ode}
                \forall \bigl((t, \vect x_1), (t, \vect x_2)\bigr) \in \Omega_{\mathcal T, \mathcal R} \times \Omega_{\mathcal T, \mathcal R}, \qquad
                \vecnorm{ \vect f(t, \vect x_1) - \vect f(t, \vect x_2) } \leq L \vecnorm{\vect x_1 - \vect x_2}.
            \end{equation}
    \end{itemize}
    Then there exists $T \in (0, \mathcal T]$ depending on $\mathcal R$, $M$ and $L$ such that
    the differential equation~\eqref{eq:ode_integral} has a local solution $\vect x\colon [t_0 - T, t_0 + T] \to \real^n$.
\end{theorem}
\begin{proof}
    Fix $T \in (0, \mathcal T]$ and let $I = [t_0 - T, t_0 + T]$.
    Let also $\mathcal X$ denote the following subset of continuous functions defined from~$I$ to $\real^n$:
    \[
    \mathcal X := \left\{ \vect x\in C\left(I, \real^n\right) : \sup_{t \in I} \norm[\big]{\vect x(t) - \vect x_0} \leq \mathcal R \right\}
    \]
    The set~$\mathcal X$ endowed with supremum metric is a closed subset of~$C(I, \real^n)$.
    Since $\mathcal X$ is a closed subset of a complete metric space,
    it is itself complete.
    Let $\Phi \colon \mathcal X \to C(I, \real^n)$ denote the mapping
    \[
        \Phi(\vect x) \colon  t \mapsto  \vect x_0 + \int_{0}^{t} \vect f\bigl(s, \vect x(s)\bigr) \, \d s.
    \]
    The right-hand side, being the integral of a bounded function,
    is indeed a continuous function.
    We will show that, for~$T$ sufficiently small,
    \begin{itemize}
        \item the mapping $\Phi$ maps $\mathcal X$ into $\mathcal X$;
        \item the mapping $\Phi$ is a contraction.
    \end{itemize}
    From~\eqref{eq:boundedness_ode},
    it follows that
    \[
        \forall \vect x \in \mathcal X, \qquad
        \forall t \in I, \qquad
        \vecnorm*{\Phi(\vect x)(t) - \vect x_0} = \vecnorm*{ \int_{t_0}^{t} \vect f\bigl(s, \vect x(s)\bigr) \, \d s }
        \leq M T.
    \]
    On the other hand, from the Lipschitz condition~\eqref{eq:local_lipschitz_ode},
    it holds that
    \[
        \forall (\vect x, \vect v) \in \mathcal X \times \mathcal X, \qquad
        \norm{\Phi(\vect x) - \Phi(\vect v)} \leq 2 L T.
    \]
    Therefore, it suffices to take $T \leq \min \left\{ \mathcal T, \frac{\mathcal R}{M}, \frac{2}{L} \right\}$ to ensure that the above conditions are satisfied.
    For a value of~$T$ in this range,
    the Banach fixed point theorem, \cref{theorem:banach_fixed_point},
    gives the existence of a unique fixed point~$\vect x_* \in \mathcal X$ of~$\Phi$.
    Since a fixed point of~$\Phi$ is a solution to~\eqref{eq:ode_integral},
    the statement is proved.
\end{proof}
It may seem at first glance that uniqueness of the solution to~\eqref{eq:ode_integral} follows from the uniqueness of the fixed point guaranteed by~\cref{theorem:banach_fixed_point}.
However, this theorem implies uniqueness \emph{only in the set~$\mathcal X$},
a property known as~\emph{conditional uniqueness}.
In order to prove that the solution is unique over the full space $C([t_0-T, t_0+T], \real^n)$,
additional arguments are required.
A simple approach is to rely on Gr\"onwall's lemma.
\begin{lemma}
    [Gr\"onwall's lemma, simplified integral form]
    Suppose that $u \colon [t_0-T, t_0+T] \to \real_{\geq 0}$ is continuous, nonnegative, and satisfies
    \begin{equation}
        \label{eq:gronwall_condition}
        \forall t \in [t_0, t_0+T], \qquad
        u(t) \leq \alpha + \int_{t_0}^{t} \beta(s) \, u(s) \, \d s,
    \end{equation}
    where $\alpha \geq 0$ and $\beta\colon [t_0, t_0 + T] \to \real_{\geq 0}$ is continuous and nonnegative.
    Then
    \begin{equation}
        \label{eq:gronwall}
        \forall t \in [t_0, t_0+T], \qquad
        u(t) \leq \alpha \exp \left( \int_{t_0}^{t} \beta(s) \, \d s \right).
    \end{equation}
\end{lemma}
\begin{proof}
    Assume first that~$\alpha > 0$,
    so that the logarithm in~\eqref{eq:intermediate_gronwall} is well-defined.
    By the fundamental theorem of calculus and~\eqref{eq:gronwall_condition},
    it holds that
    \[
        \frac{\d}{\d t} \left( \alpha + \int_{t_0}^{t} \beta(s) \, u(s) \, \d s \right)
        \leq \beta(t) \left( \alpha + \int_{t_0}^{t} \beta(s) \, u(s) \, \d s \right)
    \]
    Therefore we have
    \begin{equation}
        \label{eq:intermediate_gronwall}
        \frac{\d}{\d t} \log \left( \alpha + \int_{t_0}^{t} \beta(s) \, u(s) \, \d s \right) \leq \beta(t),
    \end{equation}
    and after integrating and exponentiating,
    we obtain
    \[
        \alpha + \int_{t_0}^{t} \beta(s) \, u(s) \, \d s
        \leq \alpha \exp \left( \int_{t_0}^{t} \beta(s) \, \d s \right)
    \]
    The statement then follows by using~\eqref{eq:gronwall_condition} again.
    If~\eqref{eq:gronwall_condition} is satisfied for~$\alpha = 0$,
    then this condition is also satisfied for all $\alpha > 0$.
    Therefore~\eqref{eq:gronwall} also holds for all~$\alpha > 0$,
    and taking the limit $\alpha \to 0$ in this equation,
    we obtain the statement.
\end{proof}
Note that the estimate~\eqref{eq:gronwall} is sharp,
since the function $v \colon [t_0, t_0 + T]$ given by
\[
    v(t) = \alpha \exp \left( \int_{t_0}^{t} \beta(s) \, \d s \right)
\]
satisfies~\eqref{eq:gronwall_condition} with inequality.
We are now ready to prove uniqueness.
\begin{theorem}
    [Uniqueness of the solution]
    \label{theorem:uniqueness}
    Let $\vect x_0 \in \real^n$ and let
    \[
        \Omega_{\mathcal T, \mathcal R}
        \bigl\{(t, \vect x) \in \real \times \real^n: \abs{t - t_0} \leq \mathcal T \text{ and } \vecnorm{\vect x - \vect x_0} \leq \mathcal R \bigr\},
    \]
    Assume that for all $\mathcal T \in \real_{>0}$ and $\mathcal R \in \real_{>0}$,
    there is $L_{\mathcal T,  \mathcal R}$ such that
    \begin{equation}
        \label{eq:local_lipschitz_uniqueness}
        \forall \bigl((t, \vect x_1), (t, \vect x_2)\bigr) \in \Omega_{\mathcal T, \mathcal R} \times \Omega_{\mathcal T, \mathcal R}, \qquad
        \vecnorm{ \vect f(t, \vect x_1) - \vect f(t, \vect x_2) } \leq L_{\mathcal T, \mathcal R} \vecnorm{\vect x_1 - \vect x_2}.
    \end{equation}
    Then if $\vect x_1$ and $\vect x_2$ in $C\bigl([t_0 - T, t_0 + T], \real^n\bigr)$ are local solutions to~\eqref{eq:ode_integral},
    it holds that $\vect x_1 = \vect x_2$.
\end{theorem}
\begin{proof}
    Let~$I = [t_0 - T, t_0 + T]$ and
    \[
        R := \max \left\{ \sup_{t \in I} \norm{\vect x_1(t) - \vect x_0},   \sup_{t \in I} \norm{\vect x_1(t) - \vect x_0} \right\} < \infty,
    \]
    If $\vect x_1$ and $\vect x_2$ are solutions,
    then
    \[
        \forall t \in [t_0 - T, t_0 + T], \qquad
        \vect x_1(t) - \vect x_2(t) = \int_{t_0}^{t} \Bigl( f\bigl(s, \vect x_1(s)\bigr) - f\bigl(s, \vect x_2(s)\bigr) \Bigr)  \, \d s.
    \]
    Taking the norm and using~\eqref{eq:local_lipschitz_uniqueness},
    we obtain
    \[
        \forall t \in [t_0, t_0 + T], \qquad
        \vecnorm{\vect x_1(t) - \vect x_2(t)} \leq L_{T,R} \int_{t_0}^{t} \vecnorm{\vect x_1(s) - \vect x_2(s)} \, \d s
    \]
    Using Gr\"onwall's lemma,
    we deduce that~$\vect x_1(t) = \vect x_2(t)$ for all $t \in [t_0, t_0 + T]$.
    A similar argument can be employed to show that $\vect x_1 = \vect x_2$ on $[t_0 - T, t_0]$.
\end{proof}

\begin{corollary}
    [Maximal solutions]
    \label{corollary:maximal_solutions}
    Assume that $\vect f$ is continuous in~$t$ and satisfies the local Lipschitz condition~\eqref{eq:local_lipschitz_uniqueness}.
    Then there exists $0 \leq T_- < T_+ \leq \infty$ such that $t_0 \in (T_-, T_+)$ and 
    the following properties are satisfied.
    \begin{itemize}
        \item
            there exists a solution $\vect x_* \colon (T_-, T_+) \to \real^n$ to~\eqref{eq:ode_integral};

        \item
            if $\vect x\colon I \to \real^n$ is a local solution of~\eqref{eq:ode_integral},
            then $I \subset (T_-, T_+)$ and $\vect x(t) = \vect x_*(t)$ for all $t \in I$.

        \item
            If $T_+$ is finite, then $\lim_{t \to T_+} \vecnorm[\big]{\vect x(t)} = \infty$,
            and if $T_-$ is finite,
            then $\lim_{t \to T_-} \vecnorm[\big]{\vect x(t)} = \infty$.
    \end{itemize}
    The solution $\vect x_*$ is called the maximal solution of~\eqref{eq:ode_integral}.
\end{corollary}
\begin{proof}
    Let $\mathcal I$ denote the union of all the open intervals~$I$ such that there exists a solution in~$C(I, \real^n)$ to~\eqref{eq:ode_integral}.
    The open set $\mathcal I$ is connected and, by~\cref{theorem:existence},
    it contains a neighborhood of~$t_0$.
    Therefore $\mathcal I$ is of the form $(T_-, T_+)$,
    where $0 \leq T_- <  t_0 < T_+ \leq \infty$.
    In view of~\cref{theorem:uniqueness},
    all the local solutions can be patched together in order to obtain a solution~$\vect x_*\colon (T_-, T_+) \to \real$.
    It remains to prove the third item.
    To this end, suppose for contradiction that $T_+$ was finite and that
    there was $(t_n)_{n \in \nat}$ such that $t_n \to T_+$ and~
    \[
        K := \sup_{n \in \nat} \vecnorm{\vect x_*(t_n)} < \infty.
    \]
    Since $\vect f$ is continuous,
    there is $M$ such that $\lvert f(t, \vect x) \rvert$ is uniformly bounded from above by~$M$ for all $(t, \vect x) \in [T_- - 1, T_+ + 1] \times B_{K+1}(\vect 0)$.
    Furthermore, by the assumption~\eqref{eq:local_lipschitz_uniqueness},
    there is $L$ such that for all $t \in [T_- - 1, T_+ + 1]$,
    the following Lipschitz condition holds:
    \[
        \forall (\vect x_1, \vect x_2) \in  \times B_{K+1}(\vect 0) \times B_{K+1}(\vect 0), \qquad
        \vecnorm{ \vect f(t, \vect x_1) - \vect f(t, \vect x_2) } \leq L \vecnorm{\vect x_1 - \vect x_2}.
    \]
    Consequently, \cref{theorem:existence} with $\mathcal T = \mathcal R = 1$ implies for all~$n$ the existence of a solution to
    \[
        \left \{
        \begin{aligned}
            & \dot {\vect x}(t) = \vect f\bigl(t, \vect x(t)\bigr), \\
            & \vect x(t_n) = \vect x_*(t_n).
        \end{aligned}
        \right.
    \]
    over the time interval $[t_n - T, t_n + T]$, where $T > 0$ depends only on, $M$ and $L$.
    But then, for~$n$ sufficiently large,
    this solution extends beyond $T_+$,
    which is a contradiction.
    An analogous reasoning can be employed for~$T_-$.
\end{proof}
\begin{example}
    Consider the ODE
    \[
        \left \{
        \begin{aligned}
            & \dot x = x^2, \\
            & x(0) = 1.
        \end{aligned}
        \right.
    \]
    The maximal solution is $x_* \colon (-\infty, 1) \to \real$ given by
    \[
        \vect x_*(t) = \sqrt{\frac{1}{1-t}}.
    \]
\end{example}

In certain settings,
it is possible to prove the maximal solution to~\eqref{eq:ode_integral} is globally defined for any initial condition.
We discuss a few important examples.
\begin{itemize}
    \item
        The first case is when $\vect f\colon \real \times \real^n$ is globally Lipschitz in its second argument,
        with a Lipschitz constant that depends continuously on the first argument.

    \item
        The second case,
        generalizing the first, 
        is when the growth of~$\vect f(t, \placeholder)$ is at most affine:
        \[
            \forall (t, \vect x) \in \real \times \real^n, \qquad
            \vecnorm{ \vect f(t, \vect x) } \leq C(t) + L(t) \vecnorm{ \vect x },
        \]
        with continuous constants~$C(t)$ and $L(t)$.
        Note that 

    \item
        The third case is when $\vect f$ is independent of~$t$ and there is a function~$W \in C^1(\real^n)$ such that~$W(\vect x) \to \infty$ in the limit as $\vecnorm{\vect x} \to \infty$
        and
        \[
            \forall \vect x \in \real^n, \qquad
            \nabla W(\vect x) \cdot \vect f(\vect x) \leq c < \infty
        \]
        Such a function is called a \emph{Lyapunov function}.
\end{itemize}
The strategy of proof for global existence usually relies on an argument by contradiction.
Consider for example the third setting.
Since the assumptions of~\cref{corollary:maximal_solutions} are satisfied,
there exists a maximal solution $\vect x_*\colon (T_-, T_+) \to \infty$.
Assume for contradiction that~$T_+$ is finite.
Then the third item in~\cref{corollary:maximal_solutions} implies that $\lim_{t \to T_+} \vecnorm{\vect x_*(t)} \to \infty$,
and so~$W\bigl(\vect x_*(t)\bigr)$ blows up as~$t$ approaches $T^+$.
On the other hand, we have
\[
    \frac{\d}{\d t} W\bigl(\vect x_*(t)\bigr) = \nabla W\bigl(\vect x_*(t)\bigr) \cdot \vect f\bigl(\vect x_*(t)\bigr) \leq c.
\]
Therefore $\lim_{t \to T_+} W\bigl(\vect x_*(t)\bigr) \leq W\bigl(\vect x_*(0)\bigr) + \lvert c \rvert (T_+ - t_0)$,
which is a contradiction.

% \paragraph{Lyapunov stability.}
% Before we move to the study of numerical schemes for SDEs,
% we introduce one last important concept: that of \emph{Lyapunov stability}.
% To precisely define this concept,
% Suppose that the assumptions of~\cref{corollary:maximal_solutions} are satisfied and
% consider the differential equation~\eqref{eq:ode} over an interval~$[-T, T]$,
% with $T_- < -T < T < T_+$, and the following perturbation thereof:
% \begin{equation}
%     \label{eq:ode_perturbed}
%     \left \{
%     \begin{aligned}
%         & \dot {\vect x_{\delta}}(t) = \vect f\bigl(t, \vect x(t)\bigr) + \vect \delta\bigl(t, \vect x(t)\bigr), \\
%         & \vect x_{\delta}(t_0) = \vect x_0 + \vect \delta_0.
%     \end{aligned}
%     \right.
% \end{equation}
% Roughly speaking, Lyapunov stability is a property of the differential equation which
% guarantees that the solution to the perturbed equation~\eqref{eq:ode_perturbed} remains close to that of the unperturbed equation~\eqref{eq:ode}
% when the perturbation is small.
% \begin{definition}
%     [Stability in the sense of Lyapunov]
%     Equation~\eqref{eq:ode} is stable in the sense of Lyapunov if there is $C = C(T)$ such that
%     for all~$\varepsilon > 0$ and all $\vect \delta \colon \real \times \real^n \to \real$ continuous and bounded and all $\vect \delta_0$ with 
%     \[
%         \sup_{(t, \vect x) \in [-T, T] \times \real^d} \vecnorm{ \vect \delta(t, \vect x)} \leq \varepsilon
%         \quad \text{ and } \quad
%         \vecnorm{\vect \delta_0} \leq \varepsilon
%     \]
% \end{definition}

\section{One-step methods}
\label{sec:one-step_methods}
From now on, we assume for simplicity that $t_0 = 0$,
and we consider the initial value problem~\eqref{eq:ode} over the interval $[0, T]$.
Most numerical methods for ODEs construct an approximation of the solution at discrete points:
\[
    \vect x_n \approx \vect x(t_n), \qquad n = 0, 1, 2, \dotsc.
\]
The discretization points $(t_n)_{n \in \nat}$ are commonly equidistant,
i.e.~$t_n = n \Delta$ where $\Delta$ is the \emph{discretization step}.
We begin in~\cref{sub:explicit_euler} and~\cref{sub:implicit_euler} by studying the simplest one-step methods,
and then in~\cref{sub:one_step_general} we discuss more general methods.

\subsection{Forward Euler method}
\label{sub:explicit_euler}
Assume that~\eqref{eq:ode} has a unique solution~$\vect x(t)$ over the interval $[0, T]$.
If $\vect x(t)$ is twice continuously differentiable,
then by Taylor's formula,
we have
\begin{equation}
    \label{eq:taylor_forward}
    \vect x(t + \Delta) = \vect x(t) + \Delta \vect f(t, \vect x) + \frac{\Delta^2}{2} \vect x''(\xi),
    \qquad \xi \in (t, t+\Delta).
\end{equation}
This motivates a method known as the \emph{forward} or \emph{explicit} Euler method:
\[
    \vect x_{n+1} = \vect x_{n} + \Delta \vect f(t_n, \vect x_n),
\]
with the same initial condition as for the continuous equation~\eqref{eq:ode}.

\begin{theorem}
    [Convergence of the forward Euler method]
    \label{theorem:forward_euler}
    Assume that there is $L \in \real_{>0}$ such that
    \begin{equation}
        \label{eq:global_lipschitz}
        \forall (t, \vect x, \vect y) \in \real \times \real^n \times \real^n, \qquad
        \vecnorm{\vect f(t, \vect x) - \vect f(t, \vect y)}
        \leq L \vecnorm{\vect x - \vect y}.
    \end{equation}
    Suppose in addition that there exists a unique, twice continuously differentiable and globally defined solution to~\eqref{eq:ode},
    and let
    \[
        M = \sup_{t \in [0,T]} \vecnorm{\vect x''(t)}
    \]
    Then the following error estimate holds:
    \begin{equation}
        \label{eq:error_bound_forward_euler}
        \forall n \in \left\{0, 1, \dotsc, \floor*{\frac{T}{\Delta}} \right\},
        \qquad
        \vecnorm{\vect x(t_n) - \vect x_n}
        \leq
        \frac{\Delta M}{2} \left( \frac{\e^{L t_n} - 1}{L} \right).
    \end{equation}
\end{theorem}
\begin{proof}
    Notice that
    \begin{align*}
        \label{eq:error_propagation}
        \vect x(t_{n}) - \vect x_{n}
        &= \Bigl( \vect x(t_{n-1}) + \Delta \, \vect f\bigl(t_{n-1}, \vect x(t_{n-1})\bigr) + \frac{\Delta^2}{2} \vect x''(\xi) \Bigr)
        - \Bigl( \vect x_n + \Delta \, f\bigl(t_{n-1}, \vect x_{n-1}\bigr) \Bigr) \\
        &= \bigl(\vect x(t_{n-1}) - \vect x_{n-1}\bigr) + \Delta \Bigl( f\bigl(t_{n-1}, \vect x(t_{n-1})\bigr) - f\bigl(t_{n-1}, \vect x_{n-1}\bigr) \Bigr) + \frac{\Delta^2}{2} \vect x''(\xi_n),
    \end{align*}
    for some $\xi_n \in (t_{n-1}, t_n)$.
    Let $\vect e_n = \vect x(t_n) - \vect x_n$ and $\vect \varepsilon_n = \frac{\Delta^2}{2} \vect x''(\xi_n)$.
    The first term is the error at iteration~$n$,
    and the second may be bounded from~\eqref{eq:global_lipschitz},
    which gives
    \begin{align*}
        \vecnorm{\vect e_{n}}
        &\leq (1 + \Delta L) \vecnorm{\vect e_{n-1}}
        + \vecnorm{\vect \varepsilon_n}.
    \end{align*}
    The structure of this equation is important,
    as it appears in the analysis of all one-step methods for ODEs.
    The first term is an amplification of the error at the previous iteration,
    and the second term is an upper bound on the additional error introduced at step~$n$.
    Applying this inequality to the previous time steps, we obtain
    \begin{align*}
        \vecnorm{\vect e_{n}}
        &\leq (1 + \Delta L) \Bigl( (1 + \Delta L) \vecnorm{\vect e_{n-2}} + \vecnorm{\vect \varepsilon_{n-1}} \Bigr) + \vecnorm{\vect \varepsilon_{n}} \\
        &\leq \dotsc
        \leq (1 + \Delta L)^{n} \vecnorm{\vect e_{0}} + \sum_{i=1}^{n} (1 + \Delta L)^{n-i} \vecnorm{\vect \varepsilon_{i}}.
    \end{align*}
    Since $\vecnorm{\vect \varepsilon_i} \leq \Delta^2M/2$,
    we have by using the formula for geometric series that
    \[
        \vecnorm{\vect e_{n}}
        \leq (1 + \Delta L)^{n} \vecnorm{\vect e_{0}} + \frac{(1+\Delta L)^{n} - 1}{\Delta L} \left( \frac{\Delta^2 M}{2} \right).
    \]
    The first term is zero because $\vecnorm{\vect e_{0}} = 0$.
    Using the bound $(1+\Delta L)^n \leq \bigl(\exp(\Delta L)\bigr)^n = \e^{L t_n}$ in the second term and rearranging,
    we finally obtain the statement~\eqref{eq:error_bound_forward_euler}.
\end{proof}

\subsection{Backward Euler method}
\label{sub:implicit_euler}
If we apply the Taylor expansion~\eqref{eq:taylor_forward} backward around~$t + \Delta$ instead of forward around~$t$,
we obtain
\[
    \label{eq:taylor_forward}
    \vect x(t) = \vect x(t + \Delta) - \Delta \vect f(t + \Delta, \vect x) + \frac{\Delta^2}{2} \vect x''(\xi),
    \qquad \xi \in (t, t+\Delta).
\]
This motivates the so-called \emph{backward} or \emph{implicit} Euler method:
\begin{equation}
    \label{eq:backward_euler}
    \vect x_{n+1} = \vect x_{n} + \Delta \vect f(t_n, \vect x_{n+1}).
\end{equation}
Observe that the right-hand side depends on $\vect x_{n+1}$.
Therefore, given $t_n$ and $\vect x_n$,
this is a nonlinear equation for the unknown $\vect x_{n+1}$,
which can be solved using any of the methods studied in~\cref{cha:solution_of_nonlinear_systems}.
Finding a solution to~\eqref{eq:backward_euler} amounts to finding a fixed point of the function
\[
    \vect y \mapsto \vect F(\vect y) := \vect x_{n} + \Delta \vect f(t_n, \vect y)
\]
A priori, the existence and uniqueness of such a fixed point is not guaranteed.
We proved in~\cref{theorem:exponential_convergence_fixed_point} that a sufficient condition for these to hold is
that~$\vect F$ is globally Lipschitz with a constant strictly less than 1,
which holds if and only if the function~$\vect y \mapsto \vect f(t_n, \vect y)$ is globally Lipschitz with a constant strictly less than $1/\Delta$.
If the condition~\eqref{eq:global_lipschitz} holds, for example,
then the backward Euler method~\eqref{eq:backward_euler} is guaranteed to be well defined for $\Delta < \frac{1}{L}$.
\Cref{theorem:exponential_convergence_fixed_point} also ensures that, if~$\vect F$ is locally Lipschitz with a constant less than~$1$,
then the fixed by using the iteration
\begin{equation}
    \label{eq:fixed_point_ode}
    \vect y_{k+1} = \vect F(\vect y_k).
\end{equation}
and there is exponential convergence~$\vect y_k \to \vect x_{n+1}$ in the limit as $k \to \infty$.
A natural starting point for~\eqref{eq:fixed_point_ode} is~$\vect y_0 = \vect x_n$.
Using a reasoning similar to that employed for proving~\cref{theorem:forward_euler},
we can prove the following result.
\begin{theorem}
    [Convergence of the backward Euler method]
    Under the same assumptions as in~\cref{theorem:forward_euler} and if $\Delta < \frac{1}{L}$,
    then the following error estimate holds:
    \begin{equation}
        \label{eq:estimate_backward_euler}
        \forall n \in \left\{0, 1, \dotsc, \floor*{\frac{T}{\Delta}} \right\},
        \qquad
        \vecnorm{\vect x(t_n) - \vect x_n}
        \leq
        \frac{\Delta M}{2} \left( \frac{\left( \frac{1}{1 - \Delta L} \right)^n - 1}{L} \right).
    \end{equation}
\end{theorem}
\begin{proof}
    The proof is left as an exercise.
    Note that, if $\Delta < \frac{1}{2L}$,
    then
    \begin{align*}
         \frac{1}{1 - \Delta L}
         &= 1 + \Delta L + (\Delta L)^2 + (\Delta L)^3 + (\Delta L)^4 \dotsc \\
         &\leq 1 + \Delta L + (\Delta L)^2 + \frac{1}{2} (\Delta L)^2 + \frac{1}{4} (\Delta L)^2 + \dotsc \\
         &\leq 1 + \Delta L + 2 (\Delta L)^2 \leq \exp\bigl(\Delta L + (\Delta L)^2\bigr),
     \end{align*}
     and so the error estimate~\eqref{eq:estimate_backward_euler} gives
     \[
        \vecnorm{\vect x(t_n) - \vect x_n}
        \leq
        \frac{\Delta M}{2} \left( \frac{\exp(L t_n + \Delta L^2 t_n) - 1}{L} \right),
     \]
     which makes it clear that the right-hand side of~\eqref{eq:estimate_backward_euler} is close to that of~\eqref{eq:error_bound_forward_euler} when $\Delta \ll 1$.
\end{proof}

\subsection{General one-step methods}
\label{sub:one_step_general}
In general, one-step methods to solve differential equations are those that can be written abstractly as
\[
    \vect x_{n+1} = \vect x_n + \Delta \vect \Phi_{\Delta}(t_n, \vect x_n).
\]
where $\vect \Phi_{\Delta}\colon \real \times \real^n \to \real^n$ is such that
\[
    \Phi_{\Delta}(t, \vect x)
    \approx \frac{1}{\Delta} \int_{t}^{t+\Delta} \vect f\bigl(s, \vect x^{t, \vect x}(s)\bigr) \, \d s
    = \frac{\vect x^{t, \vect x}(t + \Delta) - \vect x}{\Delta}.
\]
Here $\vect x^{t, \vect x}$ denotes the solution to the differential equation~\eqref{eq:ode} with initial condition $\vect x(s) = \vect x$.
