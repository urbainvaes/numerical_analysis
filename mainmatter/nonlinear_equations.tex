\chapter{Solution of nonlinear systems}
\label{cha:solution_of_nonlinear_systems}

This chapter concerns the numerical solution of nonlinear equations of the general form
\begin{equation}
    \label{eq:nonlinear_equation}
    f(\vect x) = 0, \qquad f\colon \real^n \to \real^n.
\end{equation}
A solution to this equation is called a \emph{zero} of the function $f$.
Except in particular cases (for example linear systems),
there does not exist a numerical method for solving~\eqref{eq:nonlinear_equation} in a finite number of operations,
so iterative methods are required.

In contrast with the previous chapter,
it may not be the case that~\eqref{eq:nonlinear_equation} admits one and only one solution.
For example, the equation $1 + x^2 = 0$ does not have a (real) solution,
and the equation $\cos(x) = 0$ has infinitely many.
Therefore, convergence results usually contain assumptions on the function~$f$ that guarantee the existence and uniqueness of a solution in~$\real^n$ or a subset of~$\real^n$.

For an iterative method generating approximations $(\vect x_k)_{k \geq 0}$ of a root $\vect x_*$,
we define the error as $\vect e_k = \vect x_k - \vect x_*$.
If the sequence $(\vect x_k)_{k \geq 0}$ converges to $\vect x_*$ in the limit as $k \to \infty$
and if
\begin{equation}
    \label{eq:rate_of_convergence}
    \lim_{k \to \infty} \frac{\norm{\vect e_{k+1}}}{\norm{\vect e_k}^q} = r,
\end{equation}
then we say that $(\vect x_k)_{k \geq 0}$ converges with \emph{order of convergence} $q$ and
\emph{rate of convergence} $r$.
In addition, we say that the convergence is linear $q = 1$,
superlinear if $q > 1$ and quadratic if $q = 2$.

\section{The bisection method}
As an introduction to numerical methods for solving nonlinear equations,
we present the bisection method.
This method applies only in the case of a real-valued function $f\colon \real \to \real$,
and relies on the knowledge of two points $a < b$ such that $f(a)$ and $f(b)$ have different signs.
By the intermediate value theorem,
there necessarily exists $x_* \in (a, b)$ such that $f(x_*) = 0$.
The idea of the bisection method it to successively divide the interval in two equal parts,
and to retain, based on the sign of $f$ at the midpoint $x_{1/2}$,
the one that necessarily contains a root.
If $f(x_{1/2}) f(a) \geq 0$, then $f(x_{1/2}) f(b) \leq 0$ and so there necessarily exists a root of $f$ in the interval $[x_{1/2}, b)$ by the intermediate value theorem.
In contrast, if $f(x_{1/2}) f(a) < 0$, then there necessarily is a root in the interval $(a, x_{1/2})$.
The algorithm is presented in \cref{algo:bisection}.
\begin{algorithm}
\caption{Bisection method}%
\label{algo:bisection}%
\begin{algorithmic}
\State Assume that $f(a) f(b) < 0$ with $a < b$.
\State Pick $\varepsilon > 0$.
\State $x \gets a/2 + b/2$
\While{$|b - a| \geq \varepsilon$}
    \If{$f(x) f(a) \geq 0$}
        \State $a \gets x$
    \Else
        \State $b \gets x$
    \EndIf
    \State $x \gets a/2 + b/2$
\EndWhile
\end{algorithmic}
\end{algorithm}

The following result establishes the convergence of the method.
\begin{proposition}
    \label{proposition:convergence_bisection}
    Assume that $f\colon \real \to \real$ is a continuous function.
    Let $[a_j, b_j]$ denote the interval obtained after $j$ iterations of the bisection method,
    and let $x_j$ denote the midpoint $(a_j + b_j)/2$.
    Then there exists a root~$x_*$ of $f$ such that
    \begin{equation}
        \label{eq:error_bisection}
        \abs{x_j - x_*} \leq (b_0 - a_0) 2^{-(j+1)}.
    \end{equation}
\end{proposition}
\begin{proof}
    By construction, $f(a_j) f(b_j) \leq 0$ and $f(b) \neq 0$.
    Therefore, by the intermediate value theorem,
    there exists a root of $f$ in the interval $[a_j, b_j)$,
    implying that
    \[
        \abs{x_j - x_*} \leq \frac{b_j - a_j}{2}.
    \]
    Since $b_j - a_j = 2^{-j} (b_0 - a_0)$,
    the statement follows.
\end{proof}
Although the limit in~\eqref{eq:rate_of_convergence} may not be well-defined (for example, $x_1$ may be a root of~$f$),
the error $x_j - x_*$ is bounded in absolute value by the sequence $(\widetilde e_j)_{j \geq 0}$,
where $\widetilde e_j = (b_0 - a_0) 2^{-(j+1)}$ by \cref{proposition:convergence_bisection}.
Since the latter sequence exhibits linear convergence to 0,
the convergence of the bisection method is said to be linear,
by a slight abuse of terminology.

\section{Fixed point methods}

Let $\vect s$ denote a zero of the function $f$.
The idea of iterative methods for~\eqref{eq:nonlinear_equation} is to construct,
starting from an initial guess $\vect x_0$,
a sequence $(\vect x_k)_{k=0, 1, \dotsc}$ approaching~$\vect s$.
A number of iterative methods for solving ~\eqref{eq:nonlinear_equation} are based on an iteration of the form
\begin{equation}
    \label{eq:fixed_point}
    \vect x_{k+1} = F(\vect x_{k}),
\end{equation}
for an appropriate continuous function $F$.
Assume that $\vect x_k$ converges to some point $\vect x_* \in \real^n$ in the limit as $k \to \infty$.
Then, taking the limit $k \to \infty$ in~\eqref{eq:fixed_point},
we find that $\vect x_*$ satisfies
\[
    F(\vect x_*) = \vect x_*.
\]
Such a point~$\vect x_*$ is called a \emph{fixed point} of the function $F$.
Several definitions of the function~$F$ can be employed in order to ensure that
a fixed point of $F$ coincides with a zero of $f$.
On may, for example, define $F(\vect x) = \alpha f(\vect x) + \vect x$,
for some nonzero scalar coefficient $\alpha$.
Then $F(\vect x_*) = \vect x_*$ if and only if $f(\vect x_*) = 0$.
Later in this chapter,
we study two instances of numerical methods which can be recast in the form~\eqref{eq:fixed_point}.
Before this,
we study the convergence of the iteration~\eqref{eq:fixed_point} for a general function~$F$.

\section{Convergence of fixed point methods}
Equation~\eqref{eq:fixed_point} may be viewed as a \emph{discrete-time} dynamical system.
In order to study the behavior of the system as $k \to \infty$,
it is important to understand the concept of stability of a fixed point.
The concept of stability appears also in the field of ordinary differential equations,
which are \emph{continuous-time} dynamical systems.
Before we define this concept,
we introduce the notation
\[
    B_{\delta} (\vect x) := \bigl\{ \vect y \in \real^n : \norm{\vect y - \vect x} < \delta \bigr\}
\]
for the open ball of radius $\delta$ around $\vect x \in \real^n$.
\vspace{-.2cm}
\begin{definition}
    [Stability of fixed points]
    Let $(\vect x_k)_{k\geq0}$ denote iterates obtained from~\eqref{eq:fixed_point} when starting from an initial vector~$\vect x_0$.
    Then we say that a fixed point $\vect x_*$ is
    \begin{itemize}
        \item
            an \emph{attractor} if there exists a neighborhood $\mathcal V$ of $s$ such that
            \begin{equation}
                \label{eq:nonlinear_attractor}
                \forall \vect x_0 \in \mathcal V, \qquad
                \vect x_k \xrightarrow[k \to \infty]{} \vect x_*.
            \end{equation}
            The largest neighborhood for which this is true,
            i.e. the set of values of $\vect x_0$ such that~\eqref{eq:nonlinear_attractor} holds true,
            is called the basin of attraction of $\vect x_*$.

        \item
            stable (in the sense of Lyapunov) if for all $\varepsilon > 0$,
            there exists $\delta > 0$ such that
            \[
                \forall \vect x_0 \in B_{\delta}(\vect x_*), \qquad
                \norm{\vect x_k - \vect x_*} < \varepsilon.
            \]

        \item
            asymptotically stable if it is stable and an attractor.

        \item
            exponentially stable if there exists $C > 0$, $\alpha \in (0, 1)$, and $\delta > 0$ such that
            \[
                \forall \vect x_0 \in B_{\delta}(\vect x_*),
                \quad \forall k \in \nat, \qquad
                \norm{\vect x_k - \vect x_*} \leq C \alpha^k \norm{\vect x_0 - \vect x_*}.
            \]

        \item
            globally exponentially stable if there exists $C > 0$, $\alpha \in (0, 1)$ such that
            \[
                \forall \vect x_0 \in \real^n,
                \quad \forall k \in \nat, \qquad
                \norm{\vect x_k - \vect x_*} \leq C \alpha^k \norm{\vect x_0 - \vect x_*}.
            \]
        \item
            Unstable if it is not stable.
    \end{itemize}
\end{definition}
Clearly, global exponential stability implies exponential stability,
which itself implies asymptotic stability and stability.
If $\vect x_*$ is globally exponentially stable,
then $\vect x_*$ is the unique fixed point of $F$;
showing this is the aim of~\cref{exercise:global_exponential_stability}.
If $\vect x_*$ is an attractor,
then the system is said to be locally convergent to~$\vect x_*$.
The larger the basin of attraction of $\vect x_*$,
the less careful we need to be when picking the initial guess~$\vect x_0$.
Global exponential stability of~\eqref{eq:fixed_point} can sometimes be shown
provided that $f$ satisfies a strong hypothesis.

\begin{definition}
    [Lipschitz continuity]
    A function $F\colon \real^n \to \real^n$ is said to be \emph{Lipschitz} continuous
    with constant $L$ if
    \[
        \forall (\vect x, \vect y) \in \real^n \times \real^n, \qquad
        \norm[big]{f(\vect y) - f(\vect x)} \leq L \norm{\vect y - \vect x}.
    \]
\end{definition}
A function $F$ that is Lipschitz continuous with a constant $L < 1$ is called a \emph{contraction}.
In this case, it is possible to prove the global exponential stability of the fixed point.
\begin{theorem}
    Assume that $F$ is a contraction.
    Then there exists a unique fixed point of~\eqref{eq:fixed_point},
    and it holds that
    \[
        \forall \vect x_0 \in \real^n,
        \quad \forall k \in \nat, \qquad
        \norm{\vect x_k - \vect x_*} \leq L^k \norm{\vect x_0 - \vect x_*}.
    \]
\end{theorem}
\begin{proof}
    It holds that
    \[
        \norm[big]{\vect x_{k+2} - \vect x_{k+1}}
        = \norm[big]{F(\vect x_{k+1}) - F(\vect x_k)}
        \leq L\norm{\vect x_{k+1} - \vect x_k}
        \leq \dots \leq L^{k+1} \norm{\vect x_{1} - \vect x_0}.
    \]
    Therefore, for any $n \geq m$,
    we have by the triangle inequality
    \begin{align*}
        \norm{\vect x_{n} - \vect x_{m}}
        &\leq \norm{\vect x_{n} - \vect x_{n-1}} + \dotsb + \norm{\vect x_{m+1} - \vect x_{m}} \\
        &\leq (L^n + \dotsb + L^m) \norm{\vect x_{1} - \vect x_0} \leq L^m (1 + L + \dotsb) \norm{\vect x_{1} - \vect x_0}
        = \frac{L^m}{1-L} \norm{\vect x_{1} - \vect x_0}.
    \end{align*}
    It follows that the sequence $(\vect x_k)_{k\geq0}$ is Cauchy in $\real^n$,
    implying by completeness that $\vect x_k \to \vect x_*$ in the limit as $k \to \infty$,
    for some limit $\vect x_* \in \real^n$.
    Being a contraction, the function $F$ is continuous,
    and so taking the limit~$k \to \infty$ in~\eqref{eq:fixed_point} we obtain that $\vect x_*$ is a fixed point of~$F$.
    Then
    \begin{equation}
        \label{eq:contraction}
        \norm[big]{\vect x_{k} - \vect x_{*}}
        = \norm[big]{F(\vect x_{k-1}) - F(\vect x_*)}
        \leq L\norm{\vect x_{k-1} - \vect x_*}
        \leq \dots \leq L^{k} \norm{\vect x_{0} - \vect x_*},
    \end{equation}
    proving the statement.
    To prove the uniqueness of the fixed point,
    assume there was another fixed point $\vect y_*$.
    Then, taking $\vect x_0 = \vect y_*$ in~\eqref{eq:contraction},
    we find
    \begin{equation*}
        \norm[big]{\vect y_* - \vect x_{*}}
        \leq \dots \leq L^{k} \norm{\vect y_* - \vect x_*},
    \end{equation*}
    which is impossible since $L < 1$.
\end{proof}

It is possible to prove a weaker, local result under a less restrictive assumptions on the function~$f$.
\begin{theorem}
    \label{theorem:local_convergence}
    Assume that $F\colon: \real^n \to \real^n$ has a fixed point $\vect x_*$ and satisfies the local Lipschitz condition
    \begin{equation}
        \label{eq:local_lipschitz}
        \forall \vect x \in B_{\delta}(\vect x_*), \qquad
        \norm[big]{F(\vect x) - F(\vect x_*)} \leq L \norm{\vect x - \vect x_*},
    \end{equation}
    with $0 \leq L < 1$ and $\delta > 0$.
    Then for all $\vect x_0 \in B_{\delta}(\vect x_*)$ it holds that
    \begin{itemize}
        \item All the iterates $(\vect x_k)_{k \in \nat}$ belong to $B_{\delta}(\vect x_*)$.
        \item The sequence $(\vect x_k)_{k \in \nat}$ converges exponentially to $\vect x_*$.
        \item There is no other fixed point of $F$ inside $B_{\delta}(\vect x_*)$.
    \end{itemize}
\end{theorem}
\begin{proof}
    See~\cref{exercise:prove_local_convergence}
\end{proof}
It is possible to guarantee that condition~\eqref{eq:local_lipschitz} holds provided that
we have sufficiently good control of the derivatives of the function~$F$.
The function $F$ is differentiable at $\vect x$ (in the sense of Fr√©chet) if
there exists a linear operator $\D F_{\vect x} \colon \real^n \to \real^n$ such that
\[
    \lim_{\vect h \to 0} \frac{\norm{F(\vect x + \vect h) - \vect F(\vect x) - \D F_{\vect x}(\vect h)}}{\norm{\vect h}}
    = 0.
\]
If $F$ is differentiable,
then the Jacobian matrix of~$F$ at $\vect x$ is defined as as
\[
    J_F(\vect x) =
    \begin{pmatrix}
        \partial_{1} h_1(\vect x) & \hdots & \partial_n h_1(\vect x) \\
        \vdots & \ddots & \vdots \\
        \partial_{1} h_n(\vect x) & \hdots & \partial_n h_n(\vect x)
    \end{pmatrix},
\]
and it holds that $\D F_{\vect x}(\vect h) = J_F(\vect x) \vect h$.
We recall that by Taylor's theorem.

\section{Exercises}

\begin{compexercise}
Implement the bisection method for finding the solution(s) to the equation
\[
    x = \cos(x).
\]
\end{compexercise}

\begin{exercise}
    Find a discrete-time dynamical system over $\real$ of the form
    \[
        x_{k+1} = F(x_{k})
    \]
    for which $0$ is an attractor but is not stable.

    \noindent \textbf{Hint:} Use a function $F$ that is discontinuous.
\end{exercise}

\begin{exercise}
    \label{exercise:global_exponential_stability}
    Show that if $\vect x_*$ is a globally exponentially stable fixed point of~$F$,
    then~$F$ does not have any other fixed point: $\vect x_*$ is the unique fixed point.
\end{exercise}

\begin{exercise}
    \label{exercise:prove_local_convergence}
    Prove \cref{theorem:local_convergence}.
\end{exercise}

\begin{center}
\begin{tikzpicture}[limb/.style={line cap=round,line width=1.5mm,line join=bevel}]
\draw[line width=2mm,rounded corners,fill=yellow] (-2,0) -- (0,-2) -- (2,0) -- (0,2) -- cycle;
\fill (1.5mm,7mm) circle (1.5mm);
\fill(0,-7.5mm) -- ++(10mm,0mm) -- ++(120:2mm)--++(100:1mm)--++(150:2mm) arc (70:170:2.5mm and 1mm);
\draw[limb] (-7.5mm,-6.5mm)--++(70:4mm)--++(85:4mm) coordinate(a)--++(-45:5mm)--(-2.5mm,-6.5mm);
\fill[rotate around={45:(a)}] ([shift={(-0.5mm,0.55mm)}]a) --++(0mm,-3mm)--++
        (7mm,-0.5mm)coordinate(b)--++(0mm,4mm)coordinate(c)--cycle;
\draw[limb] ([shift={(-0.6mm,-0.4mm)}]b) --++(-120:5mm) ([shift={(-0.5mm,-0.5mm)}]c) --++
        (-3mm,0mm)--++(-100:3mm)coordinate (d);
\draw[ultra thick] (d) -- ++(-45:1.25cm);
\end{tikzpicture}
% \textcolor{red}{Work in progress}
\end{center}
